2017-02-18 01:05:40.796194: step 0, loss = 13.06 (13.2 examples/sec; 19.380 sec/batch)
2017-02-18 01:08:36.682343: step 10, loss = 12.96 (125.7 examples/sec; 2.037 sec/batch)
2017-02-18 01:08:55.930781: step 20, loss = 12.75 (130.1 examples/sec; 1.967 sec/batch)
2017-02-18 01:09:15.196950: step 30, loss = 13.03 (138.7 examples/sec; 1.845 sec/batch)
2017-02-18 01:09:34.066047: step 40, loss = 12.84 (139.2 examples/sec; 1.839 sec/batch)
2017-02-18 01:09:53.044251: step 50, loss = 13.03 (130.0 examples/sec; 1.969 sec/batch)
2017-02-18 01:10:11.743259: step 60, loss = 12.84 (141.3 examples/sec; 1.812 sec/batch)
2017-02-18 01:10:30.955851: step 70, loss = 13.05 (126.5 examples/sec; 2.024 sec/batch)
2017-02-18 01:10:50.371113: step 80, loss = 12.71 (132.7 examples/sec; 1.929 sec/batch)
2017-02-18 01:11:09.533234: step 90, loss = 12.79 (127.0 examples/sec; 2.015 sec/batch)
2017-02-18 01:11:29.082333: step 100, loss = 12.43 (127.2 examples/sec; 2.012 sec/batch)
2017-02-18 01:11:50.137623: step 110, loss = 12.87 (140.1 examples/sec; 1.827 sec/batch)
2017-02-18 01:12:09.089789: step 120, loss = 12.60 (127.5 examples/sec; 2.007 sec/batch)
2017-02-18 01:12:28.163943: step 130, loss = 12.71 (136.2 examples/sec; 1.880 sec/batch)
2017-02-18 01:12:47.739410: step 140, loss = 12.77 (127.2 examples/sec; 2.013 sec/batch)
2017-02-18 01:13:06.966096: step 150, loss = 12.56 (138.3 examples/sec; 1.851 sec/batch)
2017-02-18 01:13:26.101889: step 160, loss = 12.37 (130.3 examples/sec; 1.964 sec/batch)
2017-02-18 01:13:45.592520: step 170, loss = 12.37 (130.3 examples/sec; 1.964 sec/batch)
2017-02-18 01:14:04.592481: step 180, loss = 12.42 (135.9 examples/sec; 1.883 sec/batch)
2017-02-18 01:14:23.768536: step 190, loss = 12.49 (128.1 examples/sec; 1.998 sec/batch)
2017-02-18 01:14:42.761077: step 200, loss = 12.66 (138.6 examples/sec; 1.847 sec/batch)
2017-02-18 01:15:04.499694: step 210, loss = 12.45 (137.0 examples/sec; 1.869 sec/batch)
2017-02-18 01:15:23.776270: step 220, loss = 12.38 (134.6 examples/sec; 1.902 sec/batch)
2017-02-18 01:15:42.830426: step 230, loss = 12.46 (129.0 examples/sec; 1.985 sec/batch)
2017-02-18 01:16:01.880621: step 240, loss = 12.52 (130.1 examples/sec; 1.967 sec/batch)
2017-02-18 01:16:20.963840: step 250, loss = 12.20 (129.0 examples/sec; 1.984 sec/batch)
2017-02-18 01:16:40.438056: step 260, loss = 12.46 (135.9 examples/sec; 1.883 sec/batch)
2017-02-18 01:16:59.776228: step 270, loss = 12.66 (127.1 examples/sec; 2.015 sec/batch)
2017-02-18 01:17:18.547767: step 280, loss = 12.32 (138.4 examples/sec; 1.850 sec/batch)
2017-02-18 01:17:37.608538: step 290, loss = 12.10 (138.0 examples/sec; 1.855 sec/batch)
2017-02-18 01:17:56.950097: step 300, loss = 12.49 (139.2 examples/sec; 1.839 sec/batch)
2017-02-18 01:18:18.948884: step 310, loss = 12.29 (124.4 examples/sec; 2.058 sec/batch)
2017-02-18 01:18:37.750276: step 320, loss = 12.27 (143.2 examples/sec; 1.787 sec/batch)
2017-02-18 01:18:57.030446: step 330, loss = 12.40 (128.9 examples/sec; 1.985 sec/batch)
2017-02-18 01:19:16.309416: step 340, loss = 12.32 (131.4 examples/sec; 1.949 sec/batch)
2017-02-18 01:19:35.771299: step 350, loss = 12.29 (126.9 examples/sec; 2.018 sec/batch)
2017-02-18 01:19:54.814286: step 360, loss = 12.02 (133.0 examples/sec; 1.925 sec/batch)
2017-02-18 01:20:14.038813: step 370, loss = 12.18 (135.8 examples/sec; 1.886 sec/batch)
2017-02-18 01:20:33.088690: step 380, loss = 12.15 (130.1 examples/sec; 1.968 sec/batch)
2017-02-18 01:20:52.384097: step 390, loss = 12.37 (138.2 examples/sec; 1.853 sec/batch)
2017-02-18 01:21:11.868344: step 400, loss = 12.29 (137.2 examples/sec; 1.866 sec/batch)
2017-02-18 01:21:33.438221: step 410, loss = 12.29 (140.4 examples/sec; 1.823 sec/batch)
2017-02-18 01:21:52.764990: step 420, loss = 12.07 (127.0 examples/sec; 2.017 sec/batch)
2017-02-18 01:22:11.953690: step 430, loss = 11.79 (128.6 examples/sec; 1.991 sec/batch)
2017-02-18 01:22:31.310510: step 440, loss = 11.88 (127.4 examples/sec; 2.009 sec/batch)
2017-02-18 01:22:50.531271: step 450, loss = 11.89 (132.8 examples/sec; 1.927 sec/batch)
2017-02-18 01:23:09.963315: step 460, loss = 12.10 (126.1 examples/sec; 2.031 sec/batch)
2017-02-18 01:23:29.407027: step 470, loss = 12.33 (131.9 examples/sec; 1.941 sec/batch)
2017-02-18 01:23:48.338817: step 480, loss = 12.08 (136.1 examples/sec; 1.882 sec/batch)
2017-02-18 01:24:07.491302: step 490, loss = 12.10 (127.2 examples/sec; 2.012 sec/batch)
2017-02-18 01:24:26.805040: step 500, loss = 12.14 (125.9 examples/sec; 2.034 sec/batch)
2017-02-18 01:24:47.837788: step 510, loss = 11.66 (138.5 examples/sec; 1.849 sec/batch)
2017-02-18 01:25:06.866513: step 520, loss = 11.85 (132.0 examples/sec; 1.939 sec/batch)
2017-02-18 01:25:26.088751: step 530, loss = 12.04 (137.4 examples/sec; 1.863 sec/batch)
2017-02-18 01:25:45.566698: step 540, loss = 11.80 (133.3 examples/sec; 1.921 sec/batch)
2017-02-18 01:26:04.947136: step 550, loss = 11.96 (132.8 examples/sec; 1.927 sec/batch)
2017-02-18 01:26:23.987553: step 560, loss = 11.43 (138.2 examples/sec; 1.853 sec/batch)
2017-02-18 01:26:43.012911: step 570, loss = 12.18 (133.0 examples/sec; 1.925 sec/batch)
2017-02-18 01:27:02.503711: step 580, loss = 11.76 (130.8 examples/sec; 1.957 sec/batch)
2017-02-18 01:27:21.499392: step 590, loss = 11.46 (129.0 examples/sec; 1.984 sec/batch)
2017-02-18 01:27:40.670151: step 600, loss = 11.71 (132.9 examples/sec; 1.926 sec/batch)
2017-02-18 01:28:02.147412: step 610, loss = 11.98 (134.2 examples/sec; 1.907 sec/batch)
2017-02-18 01:28:21.337739: step 620, loss = 11.54 (129.4 examples/sec; 1.979 sec/batch)
2017-02-18 01:28:40.578512: step 630, loss = 11.87 (129.7 examples/sec; 1.973 sec/batch)
2017-02-18 01:29:00.148596: step 640, loss = 11.92 (126.1 examples/sec; 2.030 sec/batch)
2017-02-18 01:29:19.141837: step 650, loss = 11.49 (136.2 examples/sec; 1.880 sec/batch)
2017-02-18 01:29:38.593004: step 660, loss = 11.50 (132.6 examples/sec; 1.931 sec/batch)
2017-02-18 01:29:58.099919: step 670, loss = 11.82 (133.9 examples/sec; 1.912 sec/batch)
2017-02-18 01:30:17.558137: step 680, loss = 11.30 (131.4 examples/sec; 1.948 sec/batch)
2017-02-18 01:30:36.768598: step 690, loss = 11.76 (135.7 examples/sec; 1.886 sec/batch)
2017-02-18 01:30:55.844603: step 700, loss = 11.58 (131.4 examples/sec; 1.948 sec/batch)
2017-02-18 01:31:17.562580: step 710, loss = 11.88 (130.6 examples/sec; 1.960 sec/batch)
2017-02-18 01:31:36.874524: step 720, loss = 12.09 (130.5 examples/sec; 1.961 sec/batch)
2017-02-18 01:31:56.111724: step 730, loss = 11.68 (128.7 examples/sec; 1.990 sec/batch)
2017-02-18 01:32:15.293253: step 740, loss = 11.44 (140.8 examples/sec; 1.818 sec/batch)
2017-02-18 01:32:34.416158: step 750, loss = 11.43 (131.8 examples/sec; 1.943 sec/batch)
2017-02-18 01:32:53.651105: step 760, loss = 11.44 (127.6 examples/sec; 2.006 sec/batch)
2017-02-18 01:33:13.067275: step 770, loss = 11.48 (136.2 examples/sec; 1.879 sec/batch)
2017-02-18 01:33:32.585229: step 780, loss = 11.93 (131.8 examples/sec; 1.942 sec/batch)
2017-02-18 01:33:51.552110: step 790, loss = 11.47 (127.4 examples/sec; 2.010 sec/batch)
2017-02-18 01:34:11.221652: step 800, loss = 11.56 (132.9 examples/sec; 1.926 sec/batch)
2017-02-18 01:34:32.803468: step 810, loss = 11.94 (133.0 examples/sec; 1.924 sec/batch)
2017-02-18 01:34:51.915537: step 820, loss = 11.51 (139.3 examples/sec; 1.837 sec/batch)
2017-02-18 01:35:11.716476: step 830, loss = 11.18 (134.8 examples/sec; 1.899 sec/batch)
2017-02-18 01:35:30.890627: step 840, loss = 11.17 (126.4 examples/sec; 2.026 sec/batch)
2017-02-18 01:35:50.232285: step 850, loss = 11.70 (125.9 examples/sec; 2.033 sec/batch)
2017-02-18 01:36:09.489432: step 860, loss = 11.59 (125.3 examples/sec; 2.043 sec/batch)
2017-02-18 01:36:28.779887: step 870, loss = 11.34 (136.0 examples/sec; 1.882 sec/batch)
2017-02-18 01:36:47.990735: step 880, loss = 11.39 (141.1 examples/sec; 1.814 sec/batch)
2017-02-18 01:37:07.439749: step 890, loss = 11.78 (138.0 examples/sec; 1.855 sec/batch)
2017-02-18 01:37:26.509006: step 900, loss = 11.80 (138.2 examples/sec; 1.852 sec/batch)
2017-02-18 01:37:48.629731: step 910, loss = 11.08 (129.8 examples/sec; 1.972 sec/batch)
2017-02-18 01:38:08.104898: step 920, loss = 11.40 (132.0 examples/sec; 1.939 sec/batch)
2017-02-18 01:38:27.234995: step 930, loss = 11.10 (131.0 examples/sec; 1.953 sec/batch)
2017-02-18 01:38:46.494677: step 940, loss = 11.35 (138.9 examples/sec; 1.843 sec/batch)
2017-02-18 01:39:05.379218: step 950, loss = 11.11 (132.2 examples/sec; 1.936 sec/batch)
2017-02-18 01:39:24.415070: step 960, loss = 11.32 (132.1 examples/sec; 1.938 sec/batch)
2017-02-18 01:39:43.858364: step 970, loss = 11.88 (124.7 examples/sec; 2.052 sec/batch)
2017-02-18 01:40:03.404889: step 980, loss = 11.19 (126.0 examples/sec; 2.032 sec/batch)
2017-02-18 01:40:22.558733: step 990, loss = 11.55 (131.9 examples/sec; 1.942 sec/batch)
2017-02-18 01:40:41.528504: step 1000, loss = 11.57 (138.9 examples/sec; 1.843 sec/batch)
2017-02-18 01:41:03.214652: step 1010, loss = 11.71 (140.0 examples/sec; 1.829 sec/batch)
2017-02-18 01:41:22.084319: step 1020, loss = 11.33 (139.2 examples/sec; 1.839 sec/batch)
2017-02-18 01:41:41.314369: step 1030, loss = 11.44 (131.7 examples/sec; 1.943 sec/batch)
2017-02-18 01:42:00.554825: step 1040, loss = 11.39 (129.8 examples/sec; 1.972 sec/batch)
2017-02-18 01:42:19.804242: step 1050, loss = 11.28 (137.2 examples/sec; 1.866 sec/batch)
2017-02-18 01:42:39.121090: step 1060, loss = 11.29 (139.2 examples/sec; 1.840 sec/batch)
2017-02-18 01:42:58.787766: step 1070, loss = 11.69 (135.3 examples/sec; 1.892 sec/batch)
2017-02-18 01:43:17.712047: step 1080, loss = 11.21 (137.2 examples/sec; 1.866 sec/batch)
2017-02-18 01:43:37.213820: step 1090, loss = 10.94 (126.0 examples/sec; 2.032 sec/batch)
2017-02-18 01:43:56.510935: step 1100, loss = 11.38 (126.4 examples/sec; 2.026 sec/batch)
2017-02-18 01:44:18.798963: step 1110, loss = 11.39 (135.9 examples/sec; 1.884 sec/batch)
2017-02-18 01:44:38.107111: step 1120, loss = 11.11 (139.3 examples/sec; 1.837 sec/batch)
2017-02-18 01:44:57.525458: step 1130, loss = 11.01 (127.7 examples/sec; 2.005 sec/batch)
2017-02-18 01:45:16.871699: step 1140, loss = 11.58 (129.6 examples/sec; 1.975 sec/batch)
2017-02-18 01:45:36.058576: step 1150, loss = 10.59 (137.3 examples/sec; 1.864 sec/batch)
2017-02-18 01:45:55.150886: step 1160, loss = 11.04 (131.0 examples/sec; 1.955 sec/batch)
2017-02-18 01:46:14.362822: step 1170, loss = 11.07 (133.0 examples/sec; 1.925 sec/batch)
2017-02-18 01:46:33.574705: step 1180, loss = 10.85 (127.2 examples/sec; 2.013 sec/batch)
2017-02-18 01:46:52.821943: step 1190, loss = 11.08 (138.1 examples/sec; 1.854 sec/batch)
2017-02-18 01:47:11.986480: step 1200, loss = 11.39 (135.9 examples/sec; 1.883 sec/batch)
2017-02-18 01:47:33.320412: step 1210, loss = 10.95 (138.5 examples/sec; 1.848 sec/batch)
2017-02-18 01:47:52.308491: step 1220, loss = 11.54 (136.6 examples/sec; 1.874 sec/batch)
2017-02-18 01:48:11.874910: step 1230, loss = 11.58 (138.1 examples/sec; 1.854 sec/batch)
2017-02-18 01:48:30.909172: step 1240, loss = 10.99 (129.8 examples/sec; 1.973 sec/batch)
2017-02-18 01:48:50.186337: step 1250, loss = 11.28 (131.4 examples/sec; 1.949 sec/batch)
2017-02-18 01:49:09.305179: step 1260, loss = 11.32 (133.6 examples/sec; 1.917 sec/batch)
2017-02-18 01:49:28.480376: step 1270, loss = 11.02 (135.4 examples/sec; 1.891 sec/batch)
2017-02-18 01:49:47.592492: step 1280, loss = 11.35 (132.1 examples/sec; 1.937 sec/batch)
2017-02-18 01:50:07.228797: step 1290, loss = 10.55 (124.1 examples/sec; 2.063 sec/batch)
2017-02-18 01:50:26.452793: step 1300, loss = 11.19 (136.4 examples/sec; 1.877 sec/batch)
2017-02-18 01:50:48.524797: step 1310, loss = 11.34 (137.3 examples/sec; 1.865 sec/batch)
2017-02-18 01:51:08.214659: step 1320, loss = 11.48 (131.2 examples/sec; 1.952 sec/batch)
2017-02-18 01:51:27.930388: step 1330, loss = 11.16 (131.3 examples/sec; 1.950 sec/batch)
2017-02-18 01:51:47.443510: step 1340, loss = 11.06 (137.6 examples/sec; 1.860 sec/batch)
2017-02-18 01:52:06.925456: step 1350, loss = 11.02 (126.9 examples/sec; 2.018 sec/batch)
2017-02-18 01:52:26.491901: step 1360, loss = 10.95 (125.5 examples/sec; 2.040 sec/batch)
2017-02-18 01:52:46.187115: step 1370, loss = 10.81 (131.3 examples/sec; 1.949 sec/batch)
2017-02-18 01:53:05.305125: step 1380, loss = 10.68 (136.4 examples/sec; 1.877 sec/batch)
2017-02-18 01:53:25.022592: step 1390, loss = 11.62 (130.0 examples/sec; 1.969 sec/batch)
2017-02-18 01:53:44.327727: step 1400, loss = 10.95 (126.9 examples/sec; 2.018 sec/batch)
2017-02-18 01:54:06.273850: step 1410, loss = 11.00 (138.2 examples/sec; 1.853 sec/batch)
2017-02-18 01:54:25.483652: step 1420, loss = 10.87 (137.6 examples/sec; 1.861 sec/batch)
2017-02-18 01:54:44.537894: step 1430, loss = 10.95 (136.7 examples/sec; 1.873 sec/batch)
2017-02-18 01:55:03.937755: step 1440, loss = 10.81 (130.3 examples/sec; 1.965 sec/batch)
2017-02-18 01:55:23.278889: step 1450, loss = 10.89 (133.1 examples/sec; 1.924 sec/batch)
2017-02-18 01:55:42.947642: step 1460, loss = 10.94 (124.9 examples/sec; 2.049 sec/batch)
2017-02-18 01:56:02.646382: step 1470, loss = 10.72 (132.5 examples/sec; 1.932 sec/batch)
2017-02-18 01:56:21.539844: step 1480, loss = 10.81 (138.4 examples/sec; 1.850 sec/batch)
2017-02-18 01:56:40.665391: step 1490, loss = 10.24 (137.9 examples/sec; 1.856 sec/batch)
2017-02-18 01:57:00.219924: step 1500, loss = 10.82 (126.4 examples/sec; 2.026 sec/batch)
2017-02-18 01:57:22.204715: step 1510, loss = 11.15 (137.3 examples/sec; 1.865 sec/batch)
2017-02-18 01:57:41.930276: step 1520, loss = 10.56 (127.1 examples/sec; 2.014 sec/batch)
2017-02-18 01:58:01.233919: step 1530, loss = 11.41 (138.6 examples/sec; 1.847 sec/batch)
2017-02-18 01:58:20.681580: step 1540, loss = 10.98 (129.2 examples/sec; 1.981 sec/batch)
2017-02-18 01:58:40.199618: step 1550, loss = 11.15 (129.5 examples/sec; 1.977 sec/batch)
2017-02-18 01:58:59.780390: step 1560, loss = 11.32 (139.0 examples/sec; 1.842 sec/batch)
2017-02-18 01:59:18.960797: step 1570, loss = 10.96 (138.9 examples/sec; 1.843 sec/batch)
2017-02-18 01:59:38.390454: step 1580, loss = 10.97 (131.9 examples/sec; 1.941 sec/batch)
2017-02-18 01:59:57.882018: step 1590, loss = 11.06 (132.6 examples/sec; 1.931 sec/batch)
2017-02-18 02:00:17.339217: step 1600, loss = 10.92 (126.5 examples/sec; 2.024 sec/batch)
2017-02-18 02:00:39.445842: step 1610, loss = 11.14 (125.6 examples/sec; 2.038 sec/batch)
2017-02-18 02:00:58.918100: step 1620, loss = 10.61 (138.3 examples/sec; 1.851 sec/batch)
2017-02-18 02:01:18.284632: step 1630, loss = 10.92 (126.7 examples/sec; 2.021 sec/batch)
2017-02-18 02:01:37.578126: step 1640, loss = 10.51 (137.7 examples/sec; 1.859 sec/batch)
2017-02-18 02:01:56.884237: step 1650, loss = 10.57 (135.0 examples/sec; 1.896 sec/batch)
2017-02-18 02:02:16.188313: step 1660, loss = 10.93 (125.5 examples/sec; 2.040 sec/batch)
2017-02-18 02:02:35.805114: step 1670, loss = 10.82 (132.3 examples/sec; 1.935 sec/batch)
2017-02-18 02:02:55.122914: step 1680, loss = 10.21 (130.7 examples/sec; 1.958 sec/batch)
2017-02-18 02:03:14.048422: step 1690, loss = 10.91 (138.0 examples/sec; 1.855 sec/batch)
2017-02-18 02:03:33.486398: step 1700, loss = 10.12 (136.9 examples/sec; 1.870 sec/batch)
2017-02-18 02:03:55.251532: step 1710, loss = 10.23 (137.9 examples/sec; 1.856 sec/batch)
2017-02-18 02:04:14.667358: step 1720, loss = 10.68 (132.8 examples/sec; 1.927 sec/batch)
2017-02-18 02:04:34.018441: step 1730, loss = 10.71 (128.6 examples/sec; 1.990 sec/batch)
2017-02-18 02:04:53.332644: step 1740, loss = 10.46 (137.1 examples/sec; 1.867 sec/batch)
2017-02-18 02:05:12.765678: step 1750, loss = 10.43 (126.3 examples/sec; 2.026 sec/batch)
2017-02-18 02:05:32.117501: step 1760, loss = 10.55 (133.8 examples/sec; 1.913 sec/batch)
2017-02-18 02:05:51.325986: step 1770, loss = 10.70 (139.8 examples/sec; 1.832 sec/batch)
2017-02-18 02:06:10.708830: step 1780, loss = 10.48 (130.3 examples/sec; 1.965 sec/batch)
2017-02-18 02:06:30.386965: step 1790, loss = 10.41 (132.1 examples/sec; 1.938 sec/batch)
2017-02-18 02:06:49.464832: step 1800, loss = 10.42 (132.7 examples/sec; 1.929 sec/batch)
2017-02-18 02:07:11.622926: step 1810, loss = 10.71 (122.6 examples/sec; 2.088 sec/batch)
2017-02-18 02:07:30.933370: step 1820, loss = 10.88 (129.9 examples/sec; 1.970 sec/batch)
2017-02-18 02:07:50.401395: step 1830, loss = 10.07 (129.3 examples/sec; 1.979 sec/batch)
2017-02-18 02:08:09.729444: step 1840, loss = 10.37 (136.2 examples/sec; 1.879 sec/batch)
2017-02-18 02:08:29.333710: step 1850, loss = 10.47 (140.0 examples/sec; 1.828 sec/batch)
2017-02-18 02:08:48.585093: step 1860, loss = 9.94 (130.8 examples/sec; 1.957 sec/batch)
2017-02-18 02:09:08.074240: step 1870, loss = 10.94 (126.4 examples/sec; 2.025 sec/batch)
2017-02-18 02:09:27.042067: step 1880, loss = 10.15 (136.2 examples/sec; 1.880 sec/batch)
2017-02-18 02:09:46.353172: step 1890, loss = 10.83 (137.7 examples/sec; 1.859 sec/batch)
2017-02-18 02:10:05.260040: step 1900, loss = 11.01 (137.8 examples/sec; 1.858 sec/batch)
2017-02-18 02:10:27.062748: step 1910, loss = 10.53 (137.9 examples/sec; 1.857 sec/batch)
2017-02-18 02:10:46.085635: step 1920, loss = 10.60 (138.9 examples/sec; 1.843 sec/batch)
2017-02-18 02:11:05.256607: step 1930, loss = 10.55 (137.4 examples/sec; 1.864 sec/batch)
2017-02-18 02:11:24.736622: step 1940, loss = 10.00 (129.7 examples/sec; 1.973 sec/batch)
2017-02-18 02:11:44.475933: step 1950, loss = 10.82 (129.4 examples/sec; 1.978 sec/batch)
2017-02-18 02:12:03.955342: step 1960, loss = 10.09 (139.8 examples/sec; 1.831 sec/batch)
2017-02-18 02:12:23.311915: step 1970, loss = 10.92 (133.0 examples/sec; 1.925 sec/batch)
2017-02-18 02:12:42.859278: step 1980, loss = 10.87 (129.7 examples/sec; 1.974 sec/batch)
2017-02-18 02:13:02.190208: step 1990, loss = 10.24 (131.0 examples/sec; 1.954 sec/batch)
2017-02-18 02:13:21.687705: step 2000, loss = 10.38 (135.1 examples/sec; 1.895 sec/batch)
2017-02-18 02:13:43.465663: step 2010, loss = 10.77 (130.8 examples/sec; 1.957 sec/batch)
2017-02-18 02:14:03.097515: step 2020, loss = 10.25 (127.0 examples/sec; 2.016 sec/batch)
2017-02-18 02:14:22.317837: step 2030, loss = 10.40 (138.0 examples/sec; 1.855 sec/batch)
2017-02-18 02:14:41.176441: step 2040, loss = 10.97 (134.8 examples/sec; 1.899 sec/batch)
2017-02-18 02:15:00.021635: step 2050, loss = 9.90 (139.4 examples/sec; 1.837 sec/batch)
2017-02-18 02:15:18.920780: step 2060, loss = 9.98 (125.3 examples/sec; 2.042 sec/batch)
2017-02-18 02:15:38.626427: step 2070, loss = 10.77 (127.1 examples/sec; 2.014 sec/batch)
2017-02-18 02:15:57.853436: step 2080, loss = 10.40 (137.7 examples/sec; 1.859 sec/batch)
2017-02-18 02:16:16.970343: step 2090, loss = 10.29 (130.3 examples/sec; 1.965 sec/batch)
2017-02-18 02:16:36.581102: step 2100, loss = 10.60 (138.4 examples/sec; 1.850 sec/batch)
2017-02-18 02:16:58.596918: step 2110, loss = 10.07 (131.7 examples/sec; 1.944 sec/batch)
2017-02-18 02:17:18.255678: step 2120, loss = 10.28 (134.8 examples/sec; 1.899 sec/batch)
2017-02-18 02:17:37.259332: step 2130, loss = 9.92 (135.4 examples/sec; 1.891 sec/batch)
2017-02-18 02:17:56.698395: step 2140, loss = 10.83 (127.6 examples/sec; 2.006 sec/batch)
2017-02-18 02:18:15.925193: step 2150, loss = 11.47 (133.1 examples/sec; 1.923 sec/batch)
2017-02-18 02:18:35.225282: step 2160, loss = 9.86 (137.7 examples/sec; 1.859 sec/batch)
2017-02-18 02:18:54.067141: step 2170, loss = 10.60 (137.6 examples/sec; 1.861 sec/batch)
2017-02-18 02:19:13.336218: step 2180, loss = 10.13 (134.8 examples/sec; 1.900 sec/batch)
2017-02-18 02:19:32.392288: step 2190, loss = 10.08 (137.5 examples/sec; 1.861 sec/batch)
2017-02-18 02:19:51.802908: step 2200, loss = 9.95 (139.3 examples/sec; 1.837 sec/batch)
2017-02-18 02:20:13.600973: step 2210, loss = 10.52 (133.4 examples/sec; 1.919 sec/batch)
2017-02-18 02:20:33.019108: step 2220, loss = 10.68 (130.2 examples/sec; 1.966 sec/batch)
2017-02-18 02:20:52.157149: step 2230, loss = 10.36 (129.6 examples/sec; 1.976 sec/batch)
2017-02-18 02:21:11.333556: step 2240, loss = 9.99 (139.1 examples/sec; 1.840 sec/batch)
2017-02-18 02:21:30.757731: step 2250, loss = 10.26 (130.9 examples/sec; 1.956 sec/batch)
2017-02-18 02:21:49.909572: step 2260, loss = 10.41 (136.3 examples/sec; 1.878 sec/batch)
2017-02-18 02:22:09.279659: step 2270, loss = 9.55 (126.9 examples/sec; 2.018 sec/batch)
2017-02-18 02:22:28.565793: step 2280, loss = 10.12 (132.1 examples/sec; 1.937 sec/batch)
2017-02-18 02:22:47.865921: step 2290, loss = 9.85 (126.9 examples/sec; 2.018 sec/batch)
2017-02-18 02:23:07.066129: step 2300, loss = 10.23 (129.0 examples/sec; 1.985 sec/batch)
2017-02-18 02:23:29.066293: step 2310, loss = 9.66 (138.8 examples/sec; 1.844 sec/batch)
2017-02-18 02:23:48.375567: step 2320, loss = 9.57 (136.0 examples/sec; 1.882 sec/batch)
2017-02-18 02:24:07.712683: step 2330, loss = 10.69 (135.9 examples/sec; 1.884 sec/batch)
2017-02-18 02:24:27.218824: step 2340, loss = 9.79 (127.7 examples/sec; 2.005 sec/batch)
2017-02-18 02:24:46.593967: step 2350, loss = 9.98 (133.3 examples/sec; 1.920 sec/batch)
2017-02-18 02:25:06.007299: step 2360, loss = 10.16 (127.1 examples/sec; 2.014 sec/batch)
2017-02-18 02:25:24.974358: step 2370, loss = 10.03 (136.7 examples/sec; 1.873 sec/batch)
2017-02-18 02:25:44.565810: step 2380, loss = 10.13 (130.4 examples/sec; 1.963 sec/batch)
2017-02-18 02:26:03.582814: step 2390, loss = 10.50 (131.9 examples/sec; 1.940 sec/batch)
2017-02-18 02:26:22.693790: step 2400, loss = 9.84 (130.1 examples/sec; 1.967 sec/batch)
2017-02-18 02:26:44.487520: step 2410, loss = 10.84 (133.8 examples/sec; 1.914 sec/batch)
2017-02-18 02:27:03.931429: step 2420, loss = 10.50 (129.3 examples/sec; 1.980 sec/batch)
2017-02-18 02:27:23.504235: step 2430, loss = 10.53 (138.0 examples/sec; 1.854 sec/batch)
2017-02-18 02:27:43.037196: step 2440, loss = 10.25 (132.8 examples/sec; 1.927 sec/batch)
2017-02-18 02:28:02.206200: step 2450, loss = 9.42 (126.1 examples/sec; 2.030 sec/batch)
2017-02-18 02:28:21.920543: step 2460, loss = 9.93 (124.9 examples/sec; 2.049 sec/batch)
2017-02-18 02:28:41.286785: step 2470, loss = 10.30 (137.1 examples/sec; 1.867 sec/batch)
2017-02-18 02:29:00.710841: step 2480, loss = 10.04 (125.8 examples/sec; 2.036 sec/batch)
2017-02-18 02:29:20.027623: step 2490, loss = 10.08 (140.0 examples/sec; 1.828 sec/batch)
2017-02-18 02:29:39.195508: step 2500, loss = 10.53 (139.3 examples/sec; 1.837 sec/batch)
2017-02-18 02:30:01.211767: step 2510, loss = 9.75 (130.6 examples/sec; 1.960 sec/batch)
2017-02-18 02:30:20.620505: step 2520, loss = 9.77 (138.2 examples/sec; 1.852 sec/batch)
2017-02-18 02:30:40.108985: step 2530, loss = 10.30 (139.8 examples/sec; 1.832 sec/batch)
2017-02-18 02:30:59.839868: step 2540, loss = 10.07 (135.7 examples/sec; 1.886 sec/batch)
2017-02-18 02:31:19.600007: step 2550, loss = 10.57 (138.7 examples/sec; 1.846 sec/batch)
2017-02-18 02:31:38.833536: step 2560, loss = 9.86 (127.7 examples/sec; 2.005 sec/batch)
2017-02-18 02:31:58.286171: step 2570, loss = 9.86 (132.7 examples/sec; 1.929 sec/batch)
2017-02-18 02:32:17.636951: step 2580, loss = 9.78 (138.4 examples/sec; 1.849 sec/batch)
2017-02-18 02:32:36.658598: step 2590, loss = 9.98 (134.9 examples/sec; 1.898 sec/batch)
2017-02-18 02:32:56.411617: step 2600, loss = 10.87 (131.7 examples/sec; 1.943 sec/batch)
2017-02-18 02:33:18.267001: step 2610, loss = 10.15 (132.3 examples/sec; 1.935 sec/batch)
2017-02-18 02:33:37.904713: step 2620, loss = 10.26 (126.4 examples/sec; 2.025 sec/batch)
2017-02-18 02:33:57.117929: step 2630, loss = 10.23 (136.5 examples/sec; 1.875 sec/batch)
2017-02-18 02:34:16.500319: step 2640, loss = 10.17 (125.4 examples/sec; 2.042 sec/batch)
2017-02-18 02:34:35.926264: step 2650, loss = 9.65 (138.2 examples/sec; 1.853 sec/batch)
2017-02-18 02:34:55.208465: step 2660, loss = 9.87 (136.5 examples/sec; 1.875 sec/batch)
2017-02-18 02:35:14.715283: step 2670, loss = 9.44 (123.8 examples/sec; 2.068 sec/batch)
2017-02-18 02:35:33.854413: step 2680, loss = 9.82 (131.7 examples/sec; 1.944 sec/batch)
2017-02-18 02:35:53.585291: step 2690, loss = 10.28 (137.6 examples/sec; 1.860 sec/batch)
2017-02-18 02:36:13.139285: step 2700, loss = 10.36 (125.8 examples/sec; 2.035 sec/batch)
2017-02-18 02:36:34.923611: step 2710, loss = 10.09 (135.1 examples/sec; 1.896 sec/batch)
2017-02-18 02:36:54.369018: step 2720, loss = 9.02 (126.7 examples/sec; 2.020 sec/batch)
2017-02-18 02:37:14.006946: step 2730, loss = 9.75 (125.4 examples/sec; 2.042 sec/batch)
2017-02-18 02:37:33.276004: step 2740, loss = 9.48 (137