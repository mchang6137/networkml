2017-02-03 22:59:27.988690: step 0, loss = 12.91 (3.5 examples/sec; 9.042 sec/batch)
2017-02-03 23:00:51.456426: step 10, loss = 14.02 (61.0 examples/sec; 0.524 sec/batch)
2017-02-03 23:00:56.611086: step 20, loss = 14.97 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:01:01.787375: step 30, loss = 15.21 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:01:06.927799: step 40, loss = 14.17 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:01:12.087287: step 50, loss = 13.70 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:01:17.263806: step 60, loss = 13.55 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:01:22.441730: step 70, loss = 13.13 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:01:27.765056: step 80, loss = 13.10 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:01:32.932076: step 90, loss = 13.23 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:01:38.082115: step 100, loss = 13.26 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:01:44.292795: step 110, loss = 13.02 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:01:49.527956: step 120, loss = 13.15 (62.8 examples/sec; 0.509 sec/batch)
2017-02-03 23:01:54.760847: step 130, loss = 13.13 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:01:59.947983: step 140, loss = 13.23 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:02:05.288421: step 150, loss = 13.18 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:02:10.446105: step 160, loss = 13.20 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:02:15.870680: step 170, loss = 13.12 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:02:21.192704: step 180, loss = 13.20 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:02:26.372064: step 190, loss = 12.85 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:02:31.604067: step 200, loss = 13.15 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:02:37.684438: step 210, loss = 13.11 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:02:43.004385: step 220, loss = 13.03 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:02:48.483236: step 230, loss = 12.87 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:02:53.880366: step 240, loss = 13.29 (54.1 examples/sec; 0.591 sec/batch)
2017-02-03 23:02:59.055575: step 250, loss = 13.09 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:03:04.320361: step 260, loss = 13.01 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:03:09.707180: step 270, loss = 13.21 (54.2 examples/sec; 0.590 sec/batch)
2017-02-03 23:03:14.859219: step 280, loss = 13.01 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:03:20.121312: step 290, loss = 13.15 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:03:25.373314: step 300, loss = 13.05 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:03:31.594065: step 310, loss = 13.27 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:03:36.755185: step 320, loss = 13.12 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:03:42.010278: step 330, loss = 12.95 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:03:47.241694: step 340, loss = 13.10 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:03:52.571282: step 350, loss = 13.15 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:03:57.911745: step 360, loss = 13.18 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:04:03.247720: step 370, loss = 13.04 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:04:08.427405: step 380, loss = 12.92 (61.2 examples/sec; 0.522 sec/batch)
2017-02-03 23:04:13.677210: step 390, loss = 13.10 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:04:18.893453: step 400, loss = 12.99 (63.0 examples/sec; 0.508 sec/batch)
2017-02-03 23:04:24.947567: step 410, loss = 12.77 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:04:30.265944: step 420, loss = 13.01 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:04:35.523089: step 430, loss = 13.18 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:04:40.839980: step 440, loss = 13.00 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:04:46.016930: step 450, loss = 13.21 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:04:51.434506: step 460, loss = 13.02 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:04:56.767642: step 470, loss = 12.98 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:05:02.263100: step 480, loss = 13.02 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:05:07.499531: step 490, loss = 12.88 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:05:12.676040: step 500, loss = 12.97 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:05:19.079814: step 510, loss = 13.04 (54.0 examples/sec; 0.593 sec/batch)
2017-02-03 23:05:24.243142: step 520, loss = 13.01 (62.4 examples/sec; 0.512 sec/batch)
2017-02-03 23:05:29.508260: step 530, loss = 13.05 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:05:34.682770: step 540, loss = 12.99 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:05:39.947736: step 550, loss = 13.07 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:05:45.196767: step 560, loss = 12.95 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:05:50.441706: step 570, loss = 12.94 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:05:55.701241: step 580, loss = 12.84 (60.2 examples/sec; 0.532 sec/batch)
2017-02-03 23:06:00.868097: step 590, loss = 12.91 (63.0 examples/sec; 0.508 sec/batch)
2017-02-03 23:06:06.140980: step 600, loss = 12.96 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:06:12.202871: step 610, loss = 13.04 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:06:17.525461: step 620, loss = 12.96 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:06:22.862394: step 630, loss = 13.03 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:06:28.120278: step 640, loss = 12.96 (61.8 examples/sec; 0.517 sec/batch)
2017-02-03 23:06:33.395477: step 650, loss = 12.98 (60.6 examples/sec; 0.528 sec/batch)
2017-02-03 23:06:38.648456: step 660, loss = 12.99 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:06:43.813896: step 670, loss = 12.94 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:06:49.005648: step 680, loss = 13.00 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:06:54.247401: step 690, loss = 13.01 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:06:59.434733: step 700, loss = 13.00 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:07:05.498296: step 710, loss = 13.07 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:07:10.836935: step 720, loss = 13.08 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:07:16.094370: step 730, loss = 12.83 (60.7 examples/sec; 0.527 sec/batch)
2017-02-03 23:07:21.284811: step 740, loss = 12.76 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:07:26.436629: step 750, loss = 12.91 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:07:31.694628: step 760, loss = 13.07 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:07:36.874985: step 770, loss = 12.92 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:07:42.046710: step 780, loss = 12.92 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:07:47.225018: step 790, loss = 12.88 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:07:52.407743: step 800, loss = 12.90 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:07:58.476087: step 810, loss = 12.99 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:08:03.710500: step 820, loss = 12.78 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:08:08.941923: step 830, loss = 12.99 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:08:14.188437: step 840, loss = 12.81 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:08:19.450705: step 850, loss = 12.89 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:08:24.632486: step 860, loss = 12.83 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:08:29.958829: step 870, loss = 12.95 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:08:35.110501: step 880, loss = 12.99 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:08:40.348274: step 890, loss = 12.90 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:08:45.611606: step 900, loss = 12.90 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:08:51.681066: step 910, loss = 12.78 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:08:56.937516: step 920, loss = 12.75 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:09:02.259895: step 930, loss = 12.96 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:09:07.524305: step 940, loss = 12.76 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:09:12.770018: step 950, loss = 13.03 (60.6 examples/sec; 0.528 sec/batch)
2017-02-03 23:09:18.024355: step 960, loss = 12.95 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:09:23.205370: step 970, loss = 12.86 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:09:28.375754: step 980, loss = 12.94 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:09:33.573338: step 990, loss = 12.86 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:09:38.896578: step 1000, loss = 12.80 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:09:44.993700: step 1010, loss = 12.78 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:09:50.190373: step 1020, loss = 12.84 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:09:55.441746: step 1030, loss = 12.86 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:10:00.707812: step 1040, loss = 12.80 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:10:05.982379: step 1050, loss = 12.77 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:10:11.243169: step 1060, loss = 12.90 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:10:16.528689: step 1070, loss = 12.87 (61.0 examples/sec; 0.524 sec/batch)
2017-02-03 23:10:21.885602: step 1080, loss = 12.78 (60.7 examples/sec; 0.527 sec/batch)
2017-02-03 23:10:27.215700: step 1090, loss = 12.93 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:10:32.460490: step 1100, loss = 12.65 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:10:38.531069: step 1110, loss = 12.83 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:10:43.860537: step 1120, loss = 12.84 (54.0 examples/sec; 0.592 sec/batch)
2017-02-03 23:10:49.037466: step 1130, loss = 12.84 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:10:54.201404: step 1140, loss = 12.73 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:10:59.393450: step 1150, loss = 12.81 (61.0 examples/sec; 0.524 sec/batch)
2017-02-03 23:11:04.729685: step 1160, loss = 12.77 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:11:09.990624: step 1170, loss = 12.99 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:11:15.248765: step 1180, loss = 12.86 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:11:20.497365: step 1190, loss = 12.80 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:11:25.716788: step 1200, loss = 12.91 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:11:31.784873: step 1210, loss = 12.68 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:11:37.018427: step 1220, loss = 12.73 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:11:42.197938: step 1230, loss = 12.77 (60.9 examples/sec; 0.526 sec/batch)
2017-02-03 23:11:47.528504: step 1240, loss = 12.76 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:11:52.678043: step 1250, loss = 12.91 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:11:57.859691: step 1260, loss = 12.75 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:12:03.004445: step 1270, loss = 12.56 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:12:08.466529: step 1280, loss = 13.01 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:12:13.710294: step 1290, loss = 12.80 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:12:18.880014: step 1300, loss = 12.70 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:12:25.033047: step 1310, loss = 12.78 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:12:30.274572: step 1320, loss = 12.75 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:12:35.511364: step 1330, loss = 12.75 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:12:40.676355: step 1340, loss = 12.71 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:12:46.017428: step 1350, loss = 12.79 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:12:51.183344: step 1360, loss = 12.66 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:12:56.442558: step 1370, loss = 12.74 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:13:01.848729: step 1380, loss = 12.65 (63.0 examples/sec; 0.508 sec/batch)
2017-02-03 23:13:07.001992: step 1390, loss = 12.72 (62.7 examples/sec; 0.511 sec/batch)
2017-02-03 23:13:12.331378: step 1400, loss = 12.59 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:13:18.514291: step 1410, loss = 12.80 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:13:23.694975: step 1420, loss = 12.66 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:13:29.090902: step 1430, loss = 12.65 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:13:34.355120: step 1440, loss = 12.80 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:13:39.529708: step 1450, loss = 12.58 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:13:44.853959: step 1460, loss = 12.72 (62.7 examples/sec; 0.511 sec/batch)
2017-02-03 23:13:50.186402: step 1470, loss = 12.60 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:13:55.356948: step 1480, loss = 12.74 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:14:00.677166: step 1490, loss = 12.68 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:14:05.942222: step 1500, loss = 12.82 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:14:12.336888: step 1510, loss = 12.62 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:14:17.736616: step 1520, loss = 12.88 (63.1 examples/sec; 0.507 sec/batch)
2017-02-03 23:14:23.005854: step 1530, loss = 12.73 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:14:28.204075: step 1540, loss = 12.62 (60.6 examples/sec; 0.528 sec/batch)
2017-02-03 23:14:33.389317: step 1550, loss = 12.79 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:14:38.556615: step 1560, loss = 12.91 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:14:43.981849: step 1570, loss = 12.81 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:14:49.163986: step 1580, loss = 12.71 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:14:54.328155: step 1590, loss = 12.64 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:14:59.498410: step 1600, loss = 12.79 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:15:05.641744: step 1610, loss = 12.65 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:15:10.969582: step 1620, loss = 12.64 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:15:16.246769: step 1630, loss = 12.48 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:15:21.648510: step 1640, loss = 12.83 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:15:26.832300: step 1650, loss = 12.65 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:15:32.169479: step 1660, loss = 12.75 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:15:37.380164: step 1670, loss = 12.81 (60.9 examples/sec; 0.525 sec/batch)
2017-02-03 23:15:42.708389: step 1680, loss = 12.55 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:15:47.904210: step 1690, loss = 12.78 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:15:53.082570: step 1700, loss = 12.71 (62.4 examples/sec; 0.512 sec/batch)
2017-02-03 23:15:59.537016: step 1710, loss = 12.70 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:16:04.792490: step 1720, loss = 12.52 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:16:10.040365: step 1730, loss = 12.78 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:16:15.351310: step 1740, loss = 12.66 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:16:20.605776: step 1750, loss = 12.69 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:16:25.862651: step 1760, loss = 12.61 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:16:31.113481: step 1770, loss = 12.74 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:16:36.309685: step 1780, loss = 12.54 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:16:41.617735: step 1790, loss = 12.59 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:16:46.777510: step 1800, loss = 12.59 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:16:53.008861: step 1810, loss = 12.76 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:16:58.260032: step 1820, loss = 12.64 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:17:03.430405: step 1830, loss = 12.78 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:17:08.613855: step 1840, loss = 12.63 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:17:13.864611: step 1850, loss = 12.51 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:17:19.049425: step 1860, loss = 12.80 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:17:24.390517: step 1870, loss = 12.68 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:17:29.629755: step 1880, loss = 12.49 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:17:34.879084: step 1890, loss = 12.65 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:17:40.067310: step 1900, loss = 12.41 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:17:46.142949: step 1910, loss = 12.59 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:17:51.401609: step 1920, loss = 12.63 (62.6 examples/sec; 0.512 sec/batch)
2017-02-03 23:17:56.576645: step 1930, loss = 12.45 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:18:01.745622: step 1940, loss = 12.59 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:18:06.929785: step 1950, loss = 12.42 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:18:12.096941: step 1960, loss = 12.64 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:18:17.336502: step 1970, loss = 12.59 (53.4 examples/sec; 0.599 sec/batch)
2017-02-03 23:18:22.656432: step 1980, loss = 12.66 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:18:27.845648: step 1990, loss = 12.66 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:18:33.022140: step 2000, loss = 12.64 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:18:39.189408: step 2010, loss = 12.57 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:18:44.357341: step 2020, loss = 12.57 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:18:49.704421: step 2030, loss = 12.38 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:18:54.961344: step 2040, loss = 12.64 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:19:00.246615: step 2050, loss = 12.47 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:19:05.446823: step 2060, loss = 12.57 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:19:10.666270: step 2070, loss = 12.74 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:19:15.910366: step 2080, loss = 12.72 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:19:21.086215: step 2090, loss = 12.80 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:19:26.359176: step 2100, loss = 12.58 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:19:32.561367: step 2110, loss = 12.47 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:19:37.846436: step 2120, loss = 12.62 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:19:43.053884: step 2130, loss = 12.47 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:19:48.221904: step 2140, loss = 12.79 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:19:53.468959: step 2150, loss = 12.57 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:19:58.797943: step 2160, loss = 12.46 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:20:03.969013: step 2170, loss = 12.57 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:20:09.142839: step 2180, loss = 12.68 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:20:14.492878: step 2190, loss = 12.61 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:20:19.905905: step 2200, loss = 12.64 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:20:26.063753: step 2210, loss = 12.45 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:20:31.309802: step 2220, loss = 12.60 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:20:36.556921: step 2230, loss = 12.64 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:20:41.706611: step 2240, loss = 12.45 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:20:47.024309: step 2250, loss = 12.57 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:20:52.207778: step 2260, loss = 12.60 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:20:57.490372: step 2270, loss = 12.45 (59.9 examples/sec; 0.534 sec/batch)
2017-02-03 23:21:02.661072: step 2280, loss = 12.42 (63.2 examples/sec; 0.507 sec/batch)
2017-02-03 23:21:07.849208: step 2290, loss = 12.60 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:21:13.265072: step 2300, loss = 12.35 (53.8 examples/sec; 0.594 sec/batch)
2017-02-03 23:21:19.411764: step 2310, loss = 12.37 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:21:24.662525: step 2320, loss = 12.63 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:21:29.904065: step 2330, loss = 12.44 (61.8 examples/sec; 0.517 sec/batch)
2017-02-03 23:21:35.162924: step 2340, loss = 12.19 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:21:40.384395: step 2350, loss = 12.32 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:21:45.727185: step 2360, loss = 12.49 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:21:50.886571: step 2370, loss = 12.53 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:21:56.227835: step 2380, loss = 12.31 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:22:01.405002: step 2390, loss = 12.38 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:22:06.557613: step 2400, loss = 12.47 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:22:12.830701: step 2410, loss = 12.50 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:22:18.023792: step 2420, loss = 12.52 (60.7 examples/sec; 0.527 sec/batch)
2017-02-03 23:22:23.272797: step 2430, loss = 12.39 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:22:28.419587: step 2440, loss = 12.53 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:22:33.896078: step 2450, loss = 12.33 (54.0 examples/sec; 0.593 sec/batch)
2017-02-03 23:22:39.092639: step 2460, loss = 12.49 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:22:44.257002: step 2470, loss = 12.31 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:22:49.444584: step 2480, loss = 12.37 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:22:54.703167: step 2490, loss = 12.41 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:22:59.940494: step 2500, loss = 12.26 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:23:06.051459: step 2510, loss = 12.62 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:23:11.249204: step 2520, loss = 12.48 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:23:16.649240: step 2530, loss = 12.56 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:23:22.156383: step 2540, loss = 12.41 (60.4 examples/sec; 0.530 sec/batch)
2017-02-03 23:23:27.569323: step 2550, loss = 12.61 (54.3 examples/sec; 0.589 sec/batch)
2017-02-03 23:23:33.059480: step 2560, loss = 12.39 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:23:38.234588: step 2570, loss = 12.02 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:23:43.639476: step 2580, loss = 12.57 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:23:48.985547: step 2590, loss = 12.07 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:23:54.393365: step 2600, loss = 12.28 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:24:00.487560: step 2610, loss = 13.16 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:24:05.749364: step 2620, loss = 12.11 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:24:10.999793: step 2630, loss = 12.44 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:24:16.182219: step 2640, loss = 12.23 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:24:21.455793: step 2650, loss = 12.50 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:24:26.630325: step 2660, loss = 12.19 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:24:31.969362: step 2670, loss = 12.20 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:24:37.150424: step 2680, loss = 12.33 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:24:42.320098: step 2690, loss = 12.75 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:24:47.736204: step 2700, loss = 12.33 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:24:53.797498: step 2710, loss = 12.48 (62.9 examples/sec; 0.509 sec/batch)
2017-02-03 23:24:59.056998: step 2720, loss = 12.41 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:25:04.247102: step 2730, loss = 12.47 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:25:09.640072: step 2740, loss = 12.12 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:25:14.810554: step 2750, loss = 12.31 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:25:20.140574: step 2760, loss = 12.54 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:25:25.475529: step 2770, loss = 12.53 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:25:30.729466: step 2780, loss = 12.36 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:25:35.916316: step 2790, loss = 12.23 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:25:41.104264: step 2800, loss = 12.31 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:25:47.261937: step 2810, loss = 12.43 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:25:52.504753: step 2820, loss = 12.27 (63.0 examples/sec; 0.508 sec/batch)
2017-02-03 23:25:57.916254: step 2830, loss = 12.36 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:26:03.212380: step 2840, loss = 12.41 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:26:08.684687: step 2850, loss = 12.44 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:26:14.005574: step 2860, loss = 12.46 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:26:19.234054: step 2870, loss = 12.09 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:26:24.568635: step 2880, loss = 12.16 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:26:29.828401: step 2890, loss = 12.52 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:26:35.010273: step 2900, loss = 12.50 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:26:41.252928: step 2910, loss = 12.35 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:26:46.496254: step 2920, loss = 12.28 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:26:51.745343: step 2930, loss = 12.28 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:26:57.080413: step 2940, loss = 12.24 (62.8 examples/sec; 0.509 sec/batch)
2017-02-03 23:27:02.330892: step 2950, loss = 12.46 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:27:07.661419: step 2960, loss = 12.28 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:27:12.898668: step 2970, loss = 12.24 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:27:18.242533: step 2980, loss = 12.14 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:27:23.516740: step 2990, loss = 11.85 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:27:28.782841: step 3000, loss = 12.41 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:27:35.050940: step 3010, loss = 12.42 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:27:40.304696: step 3020, loss = 12.52 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:27:45.484519: step 3030, loss = 12.22 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:27:50.743561: step 3040, loss = 12.27 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:27:55.990077: step 3050, loss = 12.31 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:28:01.258399: step 3060, loss = 12.32 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:28:06.425625: step 3070, loss = 12.57 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:28:11.762395: step 3080, loss = 12.40 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:28:16.927313: step 3090, loss = 11.91 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:28:22.173591: step 3100, loss = 12.39 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:28:28.246339: step 3110, loss = 12.16 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:28:33.588080: step 3120, loss = 12.23 (53.7 examples/sec; 0.596 sec/batch)
2017-02-03 23:28:39.005267: step 3130, loss = 12.42 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:28:44.184042: step 3140, loss = 12.19 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:28:49.607881: step 3150, loss = 12.58 (54.4 examples/sec; 0.588 sec/batch)
2017-02-03 23:28:54.842798: step 3160, loss = 12.40 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:29:00.093398: step 3170, loss = 12.33 (62.4 examples/sec; 0.512 sec/batch)
2017-02-03 23:29:05.344280: step 3180, loss = 12.25 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:29:10.615765: step 3190, loss = 12.18 (61.0 examples/sec; 0.524 sec/batch)
2017-02-03 23:29:15.941708: step 3200, loss = 12.21 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:29:22.013898: step 3210, loss = 12.20 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:29:27.174816: step 3220, loss = 12.05 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:29:32.561690: step 3230, loss = 12.25 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:29:37.892782: step 3240, loss = 12.18 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:29:43.230383: step 3250, loss = 12.29 (62.7 examples/sec; 0.511 sec/batch)
2017-02-03 23:29:48.553591: step 3260, loss = 12.31 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:29:53.742858: step 3270, loss = 12.70 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:29:58.910165: step 3280, loss = 12.20 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:30:04.326441: step 3290, loss = 12.06 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:30:09.589605: step 3300, loss = 12.25 (60.8 examples/sec; 0.526 sec/batch)
2017-02-03 23:30:15.687374: step 3310, loss = 12.37 (61.2 examples/sec; 0.522 sec/batch)
2017-02-03 23:30:20.860443: step 3320, loss = 12.35 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:30:26.046101: step 3330, loss = 12.19 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:30:31.831951: step 3340, loss = 12.21 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:30:37.164795: step 3350, loss = 12.35 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:30:42.420648: step 3360, loss = 12.41 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:30:47.784557: step 3370, loss = 12.36 (60.7 examples/sec; 0.528 sec/batch)
2017-02-03 23:30:53.039893: step 3380, loss = 12.38 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:30:58.199970: step 3390, loss = 12.31 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:31:03.386720: step 3400, loss = 12.05 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:31:09.513936: step 3410, loss = 12.24 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:31:14.917307: step 3420, loss = 12.29 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:31:20.093129: step 3430, loss = 12.23 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:31:25.335289: step 3440, loss = 12.19 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:31:30.498379: step 3450, loss = 12.05 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:31:35.748998: step 3460, loss = 12.33 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:31:41.019998: step 3470, loss = 12.27 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:31:46.309907: step 3480, loss = 12.28 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:31:51.871219: step 3490, loss = 12.12 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:31:57.148666: step 3500, loss = 12.24 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:32:03.466901: step 3510, loss = 12.00 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:32:08.716755: step 3520, loss = 11.85 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:32:14.053232: step 3530, loss = 11.98 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:32:19.232848: step 3540, loss = 12.09 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:32:24.421851: step 3550, loss = 11.95 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:32:29.780730: step 3560, loss = 12.43 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:32:34.974366: step 3570, loss = 12.23 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:32:40.222455: step 3580, loss = 12.35 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:32:45.560562: step 3590, loss = 12.03 (53.7 examples/sec; 0.596 sec/batch)
2017-02-03 23:32:50.899620: step 3600, loss = 12.19 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:32:57.166662: step 3610, loss = 12.54 (53.7 examples/sec; 0.596 sec/batch)
2017-02-03 23:33:02.449910: step 3620, loss = 12.02 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:33:07.631171: step 3630, loss = 12.18 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:33:12.899118: step 3640, loss = 12.48 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:33:18.147097: step 3650, loss = 12.29 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:33:23.498628: step 3660, loss = 12.23 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:33:28.771503: step 3670, loss = 12.21 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:33:34.016547: step 3680, loss = 12.01 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:33:39.365315: step 3690, loss = 12.33 (60.9 examples/sec; 0.526 sec/batch)
2017-02-03 23:33:44.534316: step 3700, loss = 12.04 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:33:50.662115: step 3710, loss = 12.28 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:33:55.923557: step 3720, loss = 12.06 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:34:01.269159: step 3730, loss = 12.12 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:34:06.436125: step 3740, loss = 12.18 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:34:11.683195: step 3750, loss = 12.08 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:34:17.171369: step 3760, loss = 12.00 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:34:22.425321: step 3770, loss = 12.32 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:34:27.840456: step 3780, loss = 12.16 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:34:33.106034: step 3790, loss = 12.25 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:34:38.447172: step 3800, loss = 12.06 (60.7 examples/sec; 0.527 sec/batch)
2017-02-03 23:34:44.557310: step 3810, loss = 12.08 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:34:49.818384: step 3820, loss = 12.15 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:34:55.018099: step 3830, loss = 11.72 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:35:00.269598: step 3840, loss = 12.18 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:35:05.522916: step 3850, loss = 12.01 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:35:10.777321: step 3860, loss = 11.91 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:35:16.093346: step 3870, loss = 12.02 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:35:21.358715: step 3880, loss = 11.89 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:35:26.679723: step 3890, loss = 12.45 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:35:31.949444: step 3900, loss = 12.31 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:35:38.096028: step 3910, loss = 12.03 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:35:43.329910: step 3920, loss = 12.33 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:35:48.523155: step 3930, loss = 11.97 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:35:53.692439: step 3940, loss = 12.03 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:35:58.945451: step 3950, loss = 11.98 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:36:04.123624: step 3960, loss = 11.87 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:36:09.315170: step 3970, loss = 12.28 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:36:14.654932: step 3980, loss = 12.43 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:36:20.090729: step 3990, loss = 12.05 (53.8 examples/sec; 0.594 sec/batch)
2017-02-03 23:36:25.426267: step 4000, loss = 12.38 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:36:31.508683: step 4010, loss = 11.95 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:36:36.800002: step 4020, loss = 12.00 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:36:41.988502: step 4030, loss = 12.05 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:36:47.400085: step 4040, loss = 12.04 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:36:52.907022: step 4050, loss = 12.30 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:36:58.092767: step 4060, loss = 11.93 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:37:03.337987: step 4070, loss = 12.10 (60.9 examples/sec; 0.525 sec/batch)
2017-02-03 23:37:08.509800: step 4080, loss = 12.25 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:37:13.686208: step 4090, loss = 12.23 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:37:19.023565: step 4100, loss = 12.05 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:37:25.107792: step 4110, loss = 12.13 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:37:30.269514: step 4120, loss = 12.06 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:37:35.597436: step 4130, loss = 12.02 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:37:40.815617: step 4140, loss = 12.17 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:37:46.011098: step 4150, loss = 11.74 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:37:51.336829: step 4160, loss = 12.00 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:37:56.732014: step 4170, loss = 12.37 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:38:01.896870: step 4180, loss = 12.37 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:38:07.154764: step 4190, loss = 12.03 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:38:12.352409: step 4200, loss = 12.00 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:38:18.456493: step 4210, loss = 12.15 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:38:23.743073: step 4220, loss = 11.76 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:38:28.931489: step 4230, loss = 12.24 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:38:34.177001: step 4240, loss = 12.22 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:38:39.371352: step 4250, loss = 11.87 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:38:44.615022: step 4260, loss = 11.98 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:38:49.943850: step 4270, loss = 12.05 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:38:55.367274: step 4280, loss = 11.65 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:39:00.685219: step 4290, loss = 12.05 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:39:06.047993: step 4300, loss = 11.94 (60.4 examples/sec; 0.530 sec/batch)
2017-02-03 23:39:12.162570: step 4310, loss = 12.30 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:39:17.335385: step 4320, loss = 11.87 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:39:22.539020: step 4330, loss = 11.67 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:39:27.873970: step 4340, loss = 12.04 (54.1 examples/sec; 0.591 sec/batch)
2017-02-03 23:39:33.026147: step 4350, loss = 12.73 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:39:38.184132: step 4360, loss = 12.24 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:39:43.428983: step 4370, loss = 11.87 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:39:48.691904: step 4380, loss = 11.79 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:39:54.181466: step 4390, loss = 12.07 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:39:59.354132: step 4400, loss = 12.25 (61.8 examples/sec; 0.517 sec/batch)
2017-02-03 23:40:05.424045: step 4410, loss = 12.21 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:40:10.684787: step 4420, loss = 11.82 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:40:15.953569: step 4430, loss = 12.13 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:40:21.221562: step 4440, loss = 11.80 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:40:26.476984: step 4450, loss = 11.91 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:40:31.664266: step 4460, loss = 11.43 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:40:36.927195: step 4470, loss = 11.93 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:40:42.196703: step 4480, loss = 12.14 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:40:47.767341: step 4490, loss = 11.96 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:40:53.048390: step 4500, loss = 12.01 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:40:59.260236: step 4510, loss = 11.84 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:41:04.503912: step 4520, loss = 12.03 (54.7 examples/sec; 0.585 sec/batch)
2017-02-03 23:41:09.863260: step 4530, loss = 11.79 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:41:15.030209: step 4540, loss = 12.44 (63.1 examples/sec; 0.507 sec/batch)
2017-02-03 23:41:20.365968: step 4550, loss = 12.02 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:41:25.607049: step 4560, loss = 11.79 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:41:31.029967: step 4570, loss = 11.83 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:41:36.271939: step 4580, loss = 12.19 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:41:41.530948: step 4590, loss = 12.41 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:41:46.951599: step 4600, loss = 11.90 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:41:53.105358: step 4610, loss = 11.97 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:41:58.279264: step 4620, loss = 11.47 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:42:03.543114: step 4630, loss = 12.13 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:42:08.941893: step 4640, loss = 11.82 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:42:14.270768: step 4650, loss = 12.00 (54.0 examples/sec; 0.593 sec/batch)
2017-02-03 23:42:19.679769: step 4660, loss = 11.81 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:42:24.935820: step 4670, loss = 12.00 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:42:30.209636: step 4680, loss = 12.05 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:42:35.532185: step 4690, loss = 12.17 (59.8 examples/sec; 0.536 sec/batch)
2017-02-03 23:42:47.026303: step 4710, loss = 12.24 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:42:52.324748: step 4720, loss = 11.72 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:42:57.583613: step 4730, loss = 11.36 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:43:03.075564: step 4740, loss = 11.79 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:43:08.332645: step 4750, loss = 11.54 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:43:13.693068: step 4760, loss = 11.75 (61.8 examples/sec; 0.517 sec/batch)
2017-02-03 23:43:18.952288: step 4770, loss = 11.74 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:43:24.190401: step 4780, loss = 11.95 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:43:34.707430: step 4800, loss = 11.68 (54.0 examples/sec; 0.593 sec/batch)
2017-02-03 23:43:40.927515: step 4810, loss = 11.84 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:43:46.186501: step 4820, loss = 11.77 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:43:51.439094: step 4830, loss = 11.52 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:43:56.763838: step 4840, loss = 11.66 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:44:02.008107: step 4850, loss = 11.76 (54.3 examples/sec; 0.589 sec/batch)
2017-02-03 23:44:07.340507: step 4860, loss = 12.02 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:44:12.658649: step 4870, loss = 11.71 (54.3 examples/sec; 0.589 sec/batch)
2017-02-03 23:44:17.903185: step 4880, loss = 12.03 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:44:23.085485: step 4890, loss = 11.84 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:44:28.486581: step 4900, loss = 12.23 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:44:34.883036: step 4910, loss = 11.85 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:44:40.044894: step 4920, loss = 11.92 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:44:45.295083: step 4930, loss = 12.03 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:44:50.551414: step 4940, loss = 11.69 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:44:55.812906: step 4950, loss = 11.77 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:45:01.080573: step 4960, loss = 11.61 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:45:06.425087: step 4970, loss = 11.94 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:45:11.617794: step 4980, loss = 11.99 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:45:17.040958: step 4990, loss = 12.16 (61.1 examples/sec; 0.523 sec/batch)
2017-02-03 23:45:22.301051: step 5000, loss = 12.22 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:46:46.331329: step 5010, loss = 11.78 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:46:51.488771: step 5020, loss = 11.66 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:46:56.646705: step 5030, loss = 11.67 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:47:01.809875: step 5040, loss = 12.09 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:47:06.987224: step 5050, loss = 11.64 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:47:12.164976: step 5060, loss = 11.66 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:47:17.340695: step 5070, loss = 11.89 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:47:22.581601: step 5080, loss = 12.25 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:47:27.984473: step 5090, loss = 11.84 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:47:33.225533: step 5100, loss = 11.67 (62.7 examples/sec; 0.511 sec/batch)
2017-02-03 23:47:39.281960: step 5110, loss = 12.12 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:47:44.541479: step 5120, loss = 11.36 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:47:49.712815: step 5130, loss = 12.23 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:47:54.967109: step 5140, loss = 11.60 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:48:00.219431: step 5150, loss = 11.63 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:48:05.454051: step 5160, loss = 11.84 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:48:10.870961: step 5170, loss = 11.73 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:48:16.198715: step 5180, loss = 11.55 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:48:21.609772: step 5190, loss = 11.87 (54.0 examples/sec; 0.592 sec/batch)
2017-02-03 23:48:26.788594: step 5200, loss = 11.95 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:48:32.939340: step 5210, loss = 11.99 (60.6 examples/sec; 0.528 sec/batch)
2017-02-03 23:48:38.220736: step 5220, loss = 11.62 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:48:43.442009: step 5230, loss = 12.12 (62.6 examples/sec; 0.511 sec/batch)
2017-02-03 23:48:48.920062: step 5240, loss = 11.72 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:48:54.174257: step 5250, loss = 11.50 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:48:59.436086: step 5260, loss = 11.95 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:49:04.847765: step 5270, loss = 11.63 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:49:10.031362: step 5280, loss = 11.96 (60.4 examples/sec; 0.530 sec/batch)
2017-02-03 23:49:15.277359: step 5290, loss = 11.65 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:49:20.521089: step 5300, loss = 11.57 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:49:26.748996: step 5310, loss = 11.35 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:49:31.924512: step 5320, loss = 11.75 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:49:37.318627: step 5330, loss = 11.72 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:49:42.704077: step 5340, loss = 11.81 (53.8 examples/sec; 0.594 sec/batch)
2017-02-03 23:49:48.026289: step 5350, loss = 11.71 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:49:53.268702: step 5360, loss = 11.53 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:49:58.411065: step 5370, loss = 11.75 (63.0 examples/sec; 0.508 sec/batch)
2017-02-03 23:50:03.660526: step 5380, loss = 11.84 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:50:08.903262: step 5390, loss = 12.15 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:50:14.212519: step 5400, loss = 11.73 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:50:20.324936: step 5410, loss = 11.92 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:50:25.636104: step 5420, loss = 12.09 (53.4 examples/sec; 0.599 sec/batch)
2017-02-03 23:50:30.920973: step 5430, loss = 11.56 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:50:36.155660: step 5440, loss = 11.44 (54.1 examples/sec; 0.592 sec/batch)
2017-02-03 23:50:41.431532: step 5450, loss = 12.11 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:50:46.604886: step 5460, loss = 12.02 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:50:51.941875: step 5470, loss = 11.69 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:50:57.207560: step 5480, loss = 11.73 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:51:02.453834: step 5490, loss = 12.07 (53.8 examples/sec; 0.595 sec/batch)
2017-02-03 23:51:07.792571: step 5500, loss = 11.82 (53.8 examples/sec; 0.594 sec/batch)
2017-02-03 23:51:14.039631: step 5510, loss = 12.16 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:51:19.295037: step 5520, loss = 11.68 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:51:24.541081: step 5530, loss = 11.91 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:51:29.784365: step 5540, loss = 11.81 (61.5 examples/sec; 0.521 sec/batch)
2017-02-03 23:51:35.122004: step 5550, loss = 11.74 (54.1 examples/sec; 0.591 sec/batch)
2017-02-03 23:51:40.376319: step 5560, loss = 11.59 (54.1 examples/sec; 0.592 sec/batch)
2017-02-03 23:51:45.635689: step 5570, loss = 11.63 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:51:50.881689: step 5580, loss = 11.47 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:51:56.312946: step 5590, loss = 11.86 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:52:01.551671: step 5600, loss = 12.16 (62.3 examples/sec; 0.513 sec/batch)
2017-02-03 23:52:07.719269: step 5610, loss = 11.75 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:52:13.216696: step 5620, loss = 11.83 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:52:18.354978: step 5630, loss = 11.40 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:52:23.666352: step 5640, loss = 11.84 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:52:29.068878: step 5650, loss = 11.49 (53.9 examples/sec; 0.593 sec/batch)
2017-02-03 23:52:34.309878: step 5660, loss = 11.41 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:52:39.763647: step 5670, loss = 11.33 (54.3 examples/sec; 0.590 sec/batch)
2017-02-03 23:52:45.161397: step 5680, loss = 11.48 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:52:50.487007: step 5690, loss = 11.83 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:52:55.725515: step 5700, loss = 11.33 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:53:01.812648: step 5710, loss = 11.57 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:53:07.215981: step 5720, loss = 11.70 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:53:12.383104: step 5730, loss = 11.69 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:53:17.642401: step 5740, loss = 11.48 (61.4 examples/sec; 0.521 sec/batch)
2017-02-03 23:53:23.101212: step 5750, loss = 11.94 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:53:28.423405: step 5760, loss = 11.36 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:53:33.664048: step 5770, loss = 11.58 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:53:38.926033: step 5780, loss = 11.56 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:53:44.244758: step 5790, loss = 11.65 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:53:49.500862: step 5800, loss = 11.43 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:53:55.711216: step 5810, loss = 11.31 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:54:00.863182: step 5820, loss = 11.41 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:54:06.325074: step 5830, loss = 11.60 (63.1 examples/sec; 0.507 sec/batch)
2017-02-03 23:54:11.486118: step 5840, loss = 11.29 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:54:16.879322: step 5850, loss = 11.50 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:54:22.131445: step 5860, loss = 11.62 (61.2 examples/sec; 0.523 sec/batch)
2017-02-03 23:54:27.481722: step 5870, loss = 11.01 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:54:32.650352: step 5880, loss = 10.87 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:54:37.974417: step 5890, loss = 11.40 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:54:43.470524: step 5900, loss = 11.48 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:54:49.658358: step 5910, loss = 11.98 (61.3 examples/sec; 0.522 sec/batch)
2017-02-03 23:54:54.992977: step 5920, loss = 11.78 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:55:00.162161: step 5930, loss = 11.95 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:55:05.361785: step 5940, loss = 11.89 (61.0 examples/sec; 0.524 sec/batch)
2017-02-03 23:55:10.701852: step 5950, loss = 11.65 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:55:16.032827: step 5960, loss = 11.77 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:55:21.306298: step 5970, loss = 10.95 (53.5 examples/sec; 0.598 sec/batch)
2017-02-03 23:55:26.737119: step 5980, loss = 11.58 (60.9 examples/sec; 0.526 sec/batch)
2017-02-03 23:55:32.088055: step 5990, loss = 11.50 (53.4 examples/sec; 0.599 sec/batch)
2017-02-03 23:55:37.355351: step 6000, loss = 11.82 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:55:43.410191: step 6010, loss = 11.12 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:55:48.606028: step 6020, loss = 11.90 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:55:53.912552: step 6030, loss = 11.64 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:55:59.398980: step 6040, loss = 11.19 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:56:04.950100: step 6050, loss = 11.74 (53.5 examples/sec; 0.599 sec/batch)
2017-02-03 23:56:10.123891: step 6060, loss = 11.78 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:56:15.604634: step 6070, loss = 11.51 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:56:20.953509: step 6080, loss = 11.24 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:56:26.374700: step 6090, loss = 11.38 (53.9 examples/sec; 0.594 sec/batch)
2017-02-03 23:56:31.831228: step 6100, loss = 11.78 (62.3 examples/sec; 0.514 sec/batch)
2017-02-03 23:56:38.056895: step 6110, loss = 11.77 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:56:43.302661: step 6120, loss = 11.82 (62.4 examples/sec; 0.513 sec/batch)
2017-02-03 23:56:48.659194: step 6130, loss = 11.62 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:56:54.003886: step 6140, loss = 11.60 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:56:59.327342: step 6150, loss = 11.23 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:57:04.495644: step 6160, loss = 11.09 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:57:09.749838: step 6170, loss = 11.39 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:57:15.170002: step 6180, loss = 11.57 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:57:20.362651: step 6190, loss = 11.06 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:57:25.548976: step 6200, loss = 11.31 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:57:31.758574: step 6210, loss = 11.59 (62.1 examples/sec; 0.516 sec/batch)
2017-02-03 23:57:36.919006: step 6220, loss = 11.66 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:57:42.268319: step 6230, loss = 11.48 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:57:47.519272: step 6240, loss = 11.54 (61.6 examples/sec; 0.519 sec/batch)
2017-02-03 23:57:52.841482: step 6250, loss = 11.67 (62.7 examples/sec; 0.510 sec/batch)
2017-02-03 23:57:58.003143: step 6260, loss = 11.20 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:58:03.337965: step 6270, loss = 11.86 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:58:08.765824: step 6280, loss = 11.55 (61.6 examples/sec; 0.520 sec/batch)
2017-02-03 23:58:14.031192: step 6290, loss = 11.90 (61.1 examples/sec; 0.524 sec/batch)
2017-02-03 23:58:19.289549: step 6300, loss = 11.57 (62.2 examples/sec; 0.514 sec/batch)
2017-02-03 23:58:25.407936: step 6310, loss = 11.69 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:58:30.822133: step 6320, loss = 11.36 (61.9 examples/sec; 0.517 sec/batch)
2017-02-03 23:58:36.059362: step 6330, loss = 11.39 (62.5 examples/sec; 0.512 sec/batch)
2017-02-03 23:58:41.402108: step 6340, loss = 11.45 (60.7 examples/sec; 0.527 sec/batch)
2017-02-03 23:58:46.681315: step 6350, loss = 11.56 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:58:51.872850: step 6360, loss = 11.51 (61.7 examples/sec; 0.519 sec/batch)
2017-02-03 23:58:57.114570: step 6370, loss = 11.33 (61.4 examples/sec; 0.522 sec/batch)
2017-02-03 23:59:02.435417: step 6380, loss = 11.44 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:59:07.748542: step 6390, loss = 11.38 (62.6 examples/sec; 0.512 sec/batch)
2017-02-03 23:59:13.107900: step 6400, loss = 11.65 (53.6 examples/sec; 0.597 sec/batch)
2017-02-03 23:59:19.329924: step 6410, loss = 11.10 (61.5 examples/sec; 0.520 sec/batch)
2017-02-03 23:59:24.646502: step 6420, loss = 11.48 (61.8 examples/sec; 0.518 sec/batch)
2017-02-03 23:59:29.917658: step 6430, loss = 11.32 (62.1 examples/sec; 0.515 sec/batch)
2017-02-03 23:59:35.328829: step 6440, loss = 11.34 (62.2 examples/sec; 0.515 sec/batch)
2017-02-03 23:59:40.661431: step 6450, loss = 11.44 (62.0 examples/sec; 0.516 sec/batch)
2017-02-03 23:59:45.989173: step 6460, loss = 11.40 (61.7 examples/sec; 0.518 sec/batch)
2017-02-03 23:59:51.242185: step 6470, loss = 11.80 (61.0 examples/sec; 0.525 sec/batch)
2017-02-03 23:59:56.521746: step 6480, loss = 11.33 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:00:01.851783: step 6490, loss = 11.63 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:00:07.121063: step 6500, loss = 11.06 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:00:13.422979: step 6510, loss = 11.55 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:00:18.764721: step 6520, loss = 11.18 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:00:23.938066: step 6530, loss = 12.04 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:00:29.285202: step 6540, loss = 11.72 (61.8 examples/sec; 0.517 sec/batch)
2017-02-04 00:00:34.545969: step 6550, loss = 11.27 (62.1 examples/sec; 0.516 sec/batch)
2017-02-04 00:00:39.874395: step 6560, loss = 11.58 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:00:45.109997: step 6570, loss = 11.70 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:00:50.383455: step 6580, loss = 11.59 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:00:55.801112: step 6590, loss = 11.19 (62.3 examples/sec; 0.513 sec/batch)
2017-02-04 00:01:01.037272: step 6600, loss = 11.38 (62.2 examples/sec; 0.514 sec/batch)
2017-02-04 00:01:07.512303: step 6610, loss = 11.29 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:01:12.684710: step 6620, loss = 11.25 (61.2 examples/sec; 0.523 sec/batch)
2017-02-04 00:01:17.962839: step 6630, loss = 11.12 (62.5 examples/sec; 0.512 sec/batch)
2017-02-04 00:01:23.294404: step 6640, loss = 11.33 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:01:28.747434: step 6650, loss = 11.53 (63.1 examples/sec; 0.507 sec/batch)
2017-02-04 00:01:34.298547: step 6660, loss = 11.31 (62.5 examples/sec; 0.512 sec/batch)
2017-02-04 00:01:39.698638: step 6670, loss = 11.89 (62.2 examples/sec; 0.514 sec/batch)
2017-02-04 00:01:44.998925: step 6680, loss = 11.53 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:01:50.408123: step 6690, loss = 11.45 (53.8 examples/sec; 0.594 sec/batch)
2017-02-04 00:01:55.568042: step 6700, loss = 11.41 (61.7 examples/sec; 0.518 sec/batch)
2017-02-04 00:02:01.745063: step 6710, loss = 11.17 (54.2 examples/sec; 0.591 sec/batch)
2017-02-04 00:02:07.020863: step 6720, loss = 10.95 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:02:12.276754: step 6730, loss = 11.46 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:02:17.613363: step 6740, loss = 11.42 (53.2 examples/sec; 0.601 sec/batch)
2017-02-04 00:02:23.014254: step 6750, loss = 11.17 (53.8 examples/sec; 0.595 sec/batch)
2017-02-04 00:02:28.419533: step 6760, loss = 10.85 (62.7 examples/sec; 0.510 sec/batch)
2017-02-04 00:02:33.872717: step 6770, loss = 11.60 (62.4 examples/sec; 0.512 sec/batch)
2017-02-04 00:02:39.361803: step 6780, loss = 11.35 (54.1 examples/sec; 0.592 sec/batch)
2017-02-04 00:02:44.651167: step 6790, loss = 11.21 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:02:50.068496: step 6800, loss = 10.74 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:02:56.207076: step 6810, loss = 11.37 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:03:01.532518: step 6820, loss = 11.75 (62.8 examples/sec; 0.510 sec/batch)
2017-02-04 00:03:06.696476: step 6830, loss = 11.84 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:03:12.287429: step 6840, loss = 11.49 (53.8 examples/sec; 0.595 sec/batch)
2017-02-04 00:03:17.611587: step 6850, loss = 11.17 (53.9 examples/sec; 0.594 sec/batch)
2017-02-04 00:03:22.873674: step 6860, loss = 11.60 (61.8 examples/sec; 0.517 sec/batch)
2017-02-04 00:03:28.122560: step 6870, loss = 11.73 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:03:33.366761: step 6880, loss = 11.15 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:03:38.762915: step 6890, loss = 11.60 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:03:44.240768: step 6900, loss = 11.23 (62.7 examples/sec; 0.510 sec/batch)
2017-02-04 00:03:50.341329: step 6910, loss = 11.16 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:03:55.590987: step 6920, loss = 11.25 (61.1 examples/sec; 0.524 sec/batch)
2017-02-04 00:04:00.839936: step 6930, loss = 11.18 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:04:06.313863: step 6940, loss = 11.16 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:04:11.739180: step 6950, loss = 11.79 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:04:17.235246: step 6960, loss = 11.11 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:04:22.428929: step 6970, loss = 11.53 (62.2 examples/sec; 0.515 sec/batch)
2017-02-04 00:04:27.711194: step 6980, loss = 11.42 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:04:33.109811: step 6990, loss = 11.45 (62.1 examples/sec; 0.516 sec/batch)
2017-02-04 00:04:38.436088: step 7000, loss = 11.15 (61.7 examples/sec; 0.518 sec/batch)
2017-02-04 00:04:44.632342: step 7010, loss = 11.33 (60.8 examples/sec; 0.526 sec/batch)
2017-02-04 00:04:50.287513: step 7020, loss = 11.28 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:04:55.630758: step 7030, loss = 11.28 (60.7 examples/sec; 0.527 sec/batch)
2017-02-04 00:05:00.961611: step 7040, loss = 10.68 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:05:06.407520: step 7050, loss = 11.03 (61.0 examples/sec; 0.525 sec/batch)
2017-02-04 00:05:11.737856: step 7060, loss = 11.15 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:05:16.999741: step 7070, loss = 11.89 (53.9 examples/sec; 0.594 sec/batch)
2017-02-04 00:05:22.355972: step 7080, loss = 11.22 (53.7 examples/sec; 0.595 sec/batch)
2017-02-04 00:05:27.610216: step 7090, loss = 11.76 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:05:33.006488: step 7100, loss = 10.92 (62.5 examples/sec; 0.512 sec/batch)
2017-02-04 00:05:39.148255: step 7110, loss = 11.58 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:05:44.316988: step 7120, loss = 11.75 (60.8 examples/sec; 0.527 sec/batch)
2017-02-04 00:05:49.652183: step 7130, loss = 11.16 (62.4 examples/sec; 0.512 sec/batch)
2017-02-04 00:05:55.012099: step 7140, loss = 10.83 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:06:00.169675: step 7150, loss = 11.36 (63.1 examples/sec; 0.507 sec/batch)
2017-02-04 00:06:05.731042: step 7160, loss = 11.43 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:06:11.064155: step 7170, loss = 11.58 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:06:16.334480: step 7180, loss = 10.95 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:06:21.617276: step 7190, loss = 11.37 (60.7 examples/sec; 0.527 sec/batch)
2017-02-04 00:06:26.951282: step 7200, loss = 11.14 (61.6 examples/sec; 0.519 sec/batch)
2017-02-04 00:06:33.093422: step 7210, loss = 11.46 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:06:38.436100: step 7220, loss = 11.16 (60.9 examples/sec; 0.526 sec/batch)
2017-02-04 00:06:43.758635: step 7230, loss = 10.78 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:06:49.248484: step 7240, loss = 10.83 (53.8 examples/sec; 0.595 sec/batch)
2017-02-04 00:06:54.486118: step 7250, loss = 11.28 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:06:59.667076: step 7260, loss = 11.12 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:07:04.833456: step 7270, loss = 11.40 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:07:10.019784: step 7280, loss = 11.23 (62.1 examples/sec; 0.516 sec/batch)
2017-02-04 00:07:15.262353: step 7290, loss = 11.33 (62.3 examples/sec; 0.514 sec/batch)
2017-02-04 00:07:20.543607: step 7300, loss = 11.24 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:07:26.844089: step 7310, loss = 10.75 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:07:32.178878: step 7320, loss = 11.10 (61.6 examples/sec; 0.520 sec/batch)
2017-02-04 00:07:37.488623: step 7330, loss = 11.45 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:07:42.823390: step 7340, loss = 11.87 (54.0 examples/sec; 0.593 sec/batch)
2017-02-04 00:07:48.132523: step 7350, loss = 11.00 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:07:53.470408: step 7360, loss = 11.80 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:07:58.663202: step 7370, loss = 11.01 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:08:03.927540: step 7380, loss = 10.58 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:08:09.082150: step 7390, loss = 10.98 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:08:14.355676: step 7400, loss = 11.25 (54.0 examples/sec; 0.593 sec/batch)
2017-02-04 00:08:20.527466: step 7410, loss = 11.87 (61.6 examples/sec; 0.520 sec/batch)
2017-02-04 00:08:25.762642: step 7420, loss = 11.52 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:08:31.253913: step 7430, loss = 11.47 (60.9 examples/sec; 0.526 sec/batch)
2017-02-04 00:08:36.493245: step 7440, loss = 11.16 (62.5 examples/sec; 0.512 sec/batch)
2017-02-04 00:08:41.838164: step 7450, loss = 11.56 (53.7 examples/sec; 0.595 sec/batch)
2017-02-04 00:08:47.102371: step 7460, loss = 11.73 (62.5 examples/sec; 0.512 sec/batch)
2017-02-04 00:08:52.351696: step 7470, loss = 11.57 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:08:57.543410: step 7480, loss = 11.37 (61.4 examples/sec; 0.521 sec/batch)
2017-02-04 00:09:02.944390: step 7490, loss = 10.99 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:09:08.137185: step 7500, loss = 11.21 (61.1 examples/sec; 0.523 sec/batch)
2017-02-04 00:09:14.368026: step 7510, loss = 11.67 (61.2 examples/sec; 0.523 sec/batch)
2017-02-04 00:09:19.693310: step 7520, loss = 11.26 (62.8 examples/sec; 0.510 sec/batch)
2017-02-04 00:09:25.121163: step 7530, loss = 11.23 (61.7 examples/sec; 0.519 sec/batch)
2017-02-04 00:09:30.371557: step 7540, loss = 11.40 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:09:35.770137: step 7550, loss = 11.35 (62.9 examples/sec; 0.508 sec/batch)
2017-02-04 00:09:41.043487: step 7560, loss = 11.17 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:09:46.371727: step 7570, loss = 11.24 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:09:51.547055: step 7580, loss = 11.35 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:09:56.798113: step 7590, loss = 11.54 (61.8 examples/sec; 0.518 sec/batch)
2017-02-04 00:10:02.121231: step 7600, loss = 11.00 (62.2 examples/sec; 0.515 sec/batch)
2017-02-04 00:10:08.312940: step 7610, loss = 10.91 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:10:13.664199: step 7620, loss = 11.09 (62.3 examples/sec; 0.513 sec/batch)
2017-02-04 00:10:19.162455: step 7630, loss = 11.53 (53.7 examples/sec; 0.596 sec/batch)
2017-02-04 00:10:24.592075: step 7640, loss = 11.04 (61.6 examples/sec; 0.519 sec/batch)
2017-02-04 00:10:29.922522: step 7650, loss = 10.89 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:10:35.183702: step 7660, loss = 10.67 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:10:40.420529: step 7670, loss = 11.47 (62.2 examples/sec; 0.515 sec/batch)
2017-02-04 00:10:45.769225: step 7680, loss = 11.38 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:10:51.087037: step 7690, loss = 11.67 (54.0 examples/sec; 0.592 sec/batch)
2017-02-04 00:10:56.349589: step 7700, loss = 11.38 (61.1 examples/sec; 0.524 sec/batch)
2017-02-04 00:11:02.665094: step 7710, loss = 10.53 (53.7 examples/sec; 0.595 sec/batch)
2017-02-04 00:11:07.991264: step 7720, loss = 10.99 (61.6 examples/sec; 0.519 sec/batch)
2017-02-04 00:11:13.399085: step 7730, loss = 10.83 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:11:18.705485: step 7740, loss = 11.57 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:11:23.975095: step 7750, loss = 11.14 (62.2 examples/sec; 0.514 sec/batch)
2017-02-04 00:11:29.225676: step 7760, loss = 10.97 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:11:34.562150: step 7770, loss = 10.97 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:11:39.802867: step 7780, loss = 11.51 (62.2 examples/sec; 0.514 sec/batch)
2017-02-04 00:11:44.955446: step 7790, loss = 11.54 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:11:50.292388: step 7800, loss = 12.14 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:11:56.417422: step 7810, loss = 11.63 (61.5 examples/sec; 0.520 sec/batch)
2017-02-04 00:12:01.657281: step 7820, loss = 11.20 (61.1 examples/sec; 0.524 sec/batch)
2017-02-04 00:12:06.944417: step 7830, loss = 11.33 (60.9 examples/sec; 0.526 sec/batch)
2017-02-04 00:12:12.364864: step 7840, loss = 10.89 (53.9 examples/sec; 0.594 sec/batch)
2017-02-04 00:12:17.764298: step 7850, loss = 10.85 (61.7 examples/sec; 0.518 sec/batch)
2017-02-04 00:12:23.008103: step 7860, loss = 11.22 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:12:28.330870: step 7870, loss = 11.39 (62.6 examples/sec; 0.511 sec/batch)
2017-02-04 00:12:33.593487: step 7880, loss = 11.03 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:12:38.849355: step 7890, loss = 11.50 (62.4 examples/sec; 0.513 sec/batch)
2017-02-04 00:12:44.194859: step 7900, loss = 10.55 (61.2 examples/sec; 0.523 sec/batch)
2017-02-04 00:12:50.585989: step 7910, loss = 11.66 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:12:55.848506: step 7920, loss = 11.46 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:13:01.016574: step 7930, loss = 11.05 (61.6 examples/sec; 0.520 sec/batch)
2017-02-04 00:13:06.363368: step 7940, loss = 10.80 (61.9 examples/sec; 0.517 sec/batch)
2017-02-04 00:13:11.684642: step 7950, loss = 11.56 (62.1 examples/sec; 0.515 sec/batch)
2017-02-04 00:13:17.021286: step 7960, loss = 11.05 (61.3 examples/sec; 0.522 sec/batch)
2017-02-04 00:13:22.193413: step 7970, loss = 10.63 (60.8 examples/sec; 0.527 sec/batch)
2017-02-04 00:13:27.527645: step 7980, loss = 11.63 (62.0 examples/sec; 0.516 sec/batch)
2017-02-04 00:13:33.000540: step 7990, loss = 11.18 (61.6 examples/sec; 0.520 sec/batch)
2017-02-04 00:13:38.244175: step 8000, loss = 11.25 (62.3 examples/sec; 0.514 sec/batch)