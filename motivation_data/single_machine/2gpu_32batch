2017-02-04 00:25:05.879727: step 0, loss = 13.14 (5.1 examples/sec; 6.220 sec/batch)
2017-02-04 00:26:03.349371: step 10, loss = 14.35 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:26:11.544042: step 20, loss = 15.24 (39.3 examples/sec; 0.813 sec/batch)
2017-02-04 00:26:19.665812: step 30, loss = 15.69 (39.4 examples/sec; 0.811 sec/batch)
2017-02-04 00:26:27.784396: step 40, loss = 13.66 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:26:35.886591: step 50, loss = 13.63 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 00:26:44.773286: step 60, loss = 13.76 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:26:52.995129: step 70, loss = 13.28 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 00:27:01.632403: step 80, loss = 13.35 (35.6 examples/sec; 0.898 sec/batch)
2017-02-04 00:27:10.256619: step 90, loss = 13.13 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:27:18.709349: step 100, loss = 13.17 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 00:27:28.329990: step 110, loss = 13.07 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:27:37.161068: step 120, loss = 13.11 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:27:45.769805: step 130, loss = 13.09 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:27:54.737155: step 140, loss = 13.10 (35.5 examples/sec; 0.903 sec/batch)
2017-02-04 00:28:03.350998: step 150, loss = 13.15 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:28:11.698326: step 160, loss = 13.12 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:28:20.125588: step 170, loss = 13.07 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:28:28.930093: step 180, loss = 13.10 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:28:37.565318: step 190, loss = 13.15 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 00:28:45.817162: step 200, loss = 13.04 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:28:55.242211: step 210, loss = 13.06 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 00:29:04.054659: step 220, loss = 13.06 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:29:12.391013: step 230, loss = 13.18 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 00:29:21.120884: step 240, loss = 13.22 (35.5 examples/sec; 0.900 sec/batch)
2017-02-04 00:29:29.856846: step 250, loss = 13.15 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:29:38.675411: step 260, loss = 13.05 (36.0 examples/sec; 0.888 sec/batch)
2017-02-04 00:29:47.656974: step 270, loss = 13.08 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:29:56.167917: step 280, loss = 13.09 (39.6 examples/sec; 0.809 sec/batch)
2017-02-04 00:30:04.694746: step 290, loss = 13.02 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:30:13.119613: step 300, loss = 13.11 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:30:22.963423: step 310, loss = 12.98 (35.5 examples/sec; 0.903 sec/batch)
2017-02-04 00:30:31.765307: step 320, loss = 13.03 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 00:30:40.410719: step 330, loss = 12.92 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 00:30:49.029228: step 340, loss = 13.04 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:30:57.664064: step 350, loss = 13.01 (35.3 examples/sec; 0.908 sec/batch)
2017-02-04 00:31:06.286923: step 360, loss = 13.19 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:31:14.911895: step 370, loss = 13.05 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 00:31:23.355001: step 380, loss = 13.03 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:31:31.824419: step 390, loss = 13.00 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:31:40.516336: step 400, loss = 13.04 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:31:50.144594: step 410, loss = 13.09 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:31:58.763464: step 420, loss = 13.11 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:32:07.418889: step 430, loss = 12.94 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 00:32:15.855700: step 440, loss = 12.99 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:32:24.656475: step 450, loss = 12.94 (39.6 examples/sec; 0.809 sec/batch)
2017-02-04 00:32:33.377454: step 460, loss = 13.04 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 00:32:42.126681: step 470, loss = 13.00 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:32:50.644382: step 480, loss = 13.05 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 00:32:59.285551: step 490, loss = 12.93 (39.5 examples/sec; 0.809 sec/batch)
2017-02-04 00:33:07.834925: step 500, loss = 12.93 (35.3 examples/sec; 0.905 sec/batch)
2017-02-04 00:33:17.851957: step 510, loss = 12.96 (36.0 examples/sec; 0.888 sec/batch)
2017-02-04 00:33:26.562413: step 520, loss = 13.00 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:33:35.096534: step 530, loss = 12.77 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:33:43.519755: step 540, loss = 13.04 (40.0 examples/sec; 0.799 sec/batch)
2017-02-04 00:33:51.771520: step 550, loss = 12.86 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:33:59.957014: step 560, loss = 12.95 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:34:08.409274: step 570, loss = 13.12 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:34:17.153257: step 580, loss = 12.93 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:34:25.790337: step 590, loss = 12.92 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:34:34.406525: step 600, loss = 13.04 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:34:43.989407: step 610, loss = 12.84 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:34:52.987144: step 620, loss = 13.00 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 00:35:01.523367: step 630, loss = 12.87 (39.3 examples/sec; 0.813 sec/batch)
2017-02-04 00:35:10.345003: step 640, loss = 12.98 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:35:19.072652: step 650, loss = 12.98 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:35:27.715747: step 660, loss = 12.97 (35.2 examples/sec; 0.909 sec/batch)
2017-02-04 00:35:36.138036: step 670, loss = 12.81 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 00:35:44.863116: step 680, loss = 12.94 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:35:53.084247: step 690, loss = 12.94 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 00:36:01.723724: step 700, loss = 12.81 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 00:36:11.242917: step 710, loss = 13.03 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:36:19.620583: step 720, loss = 12.87 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:36:28.405275: step 730, loss = 12.87 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 00:36:36.841079: step 740, loss = 12.87 (35.7 examples/sec; 0.896 sec/batch)
2017-02-04 00:36:45.355406: step 750, loss = 12.93 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:36:53.897597: step 760, loss = 12.90 (39.6 examples/sec; 0.807 sec/batch)
2017-02-04 00:37:02.338533: step 770, loss = 12.81 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 00:37:10.679978: step 780, loss = 12.93 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 00:37:19.270092: step 790, loss = 12.82 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 00:37:27.705977: step 800, loss = 12.86 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:37:37.482185: step 810, loss = 12.67 (35.9 examples/sec; 0.893 sec/batch)
2017-02-04 00:37:46.175610: step 820, loss = 12.85 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:37:54.885538: step 830, loss = 12.81 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 00:38:03.329227: step 840, loss = 12.85 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:38:11.856975: step 850, loss = 12.87 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:38:20.485562: step 860, loss = 12.90 (35.3 examples/sec; 0.905 sec/batch)
2017-02-04 00:38:29.011881: step 870, loss = 12.83 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:38:37.449212: step 880, loss = 12.87 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:38:46.095247: step 890, loss = 12.89 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:38:55.039512: step 900, loss = 12.73 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:39:04.798633: step 910, loss = 12.82 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 00:39:13.610021: step 920, loss = 12.94 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:39:22.300319: step 930, loss = 12.79 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:39:31.132295: step 940, loss = 12.88 (35.5 examples/sec; 0.903 sec/batch)
2017-02-04 00:39:39.658893: step 950, loss = 12.76 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:39:48.102645: step 960, loss = 12.86 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:39:56.620993: step 970, loss = 12.78 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 00:40:05.350911: step 980, loss = 12.76 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:40:13.887375: step 990, loss = 12.80 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:40:22.509225: step 1000, loss = 12.80 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 00:40:32.478131: step 1010, loss = 12.78 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:40:41.274767: step 1020, loss = 12.69 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:40:49.382488: step 1030, loss = 12.95 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 00:40:58.264803: step 1040, loss = 12.78 (35.7 examples/sec; 0.895 sec/batch)
2017-02-04 00:41:06.777408: step 1050, loss = 12.85 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:41:15.230282: step 1060, loss = 12.81 (35.5 examples/sec; 0.900 sec/batch)
2017-02-04 00:41:23.751163: step 1070, loss = 12.68 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:41:32.388326: step 1080, loss = 12.60 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:41:41.122272: step 1090, loss = 12.71 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:41:50.123324: step 1100, loss = 12.77 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:41:59.948370: step 1110, loss = 12.70 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:42:08.312319: step 1120, loss = 12.65 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 00:42:17.097833: step 1130, loss = 12.83 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:42:25.144198: step 1140, loss = 12.84 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 00:42:33.688013: step 1150, loss = 12.64 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 00:42:42.408281: step 1160, loss = 12.83 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:42:50.943050: step 1170, loss = 12.83 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:42:59.687410: step 1180, loss = 12.77 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:43:08.182653: step 1190, loss = 12.76 (35.6 examples/sec; 0.898 sec/batch)
2017-02-04 00:43:16.253200: step 1200, loss = 12.97 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:43:25.866594: step 1210, loss = 12.68 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 00:43:34.492366: step 1220, loss = 12.67 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 00:43:43.141196: step 1230, loss = 12.51 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 00:43:51.677243: step 1240, loss = 12.69 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 00:44:00.268666: step 1250, loss = 12.62 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:44:08.997328: step 1260, loss = 12.72 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 00:44:17.252815: step 1270, loss = 12.77 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:44:25.717451: step 1280, loss = 12.73 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:44:34.401500: step 1290, loss = 12.79 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:44:42.937551: step 1300, loss = 12.66 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 00:44:54.077520: step 1310, loss = 12.51 (39.3 examples/sec; 0.813 sec/batch)
2017-02-04 00:45:02.610156: step 1320, loss = 12.74 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:45:11.304514: step 1330, loss = 12.68 (35.2 examples/sec; 0.909 sec/batch)
2017-02-04 00:45:19.676876: step 1340, loss = 12.75 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:45:28.288503: step 1350, loss = 12.74 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:45:36.820211: step 1360, loss = 12.59 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:45:45.472860: step 1370, loss = 12.94 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:45:54.183538: step 1380, loss = 12.68 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 00:46:02.807493: step 1390, loss = 12.62 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:46:11.327811: step 1400, loss = 12.38 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:46:20.958010: step 1410, loss = 12.78 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 00:46:29.127780: step 1420, loss = 12.58 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:46:38.021790: step 1430, loss = 12.67 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:46:46.561700: step 1440, loss = 12.50 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:46:55.099447: step 1450, loss = 12.51 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:47:03.711516: step 1460, loss = 12.71 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:47:12.355231: step 1470, loss = 12.40 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:47:20.989419: step 1480, loss = 12.49 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:47:29.409153: step 1490, loss = 12.84 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:47:38.221296: step 1500, loss = 12.75 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:47:48.054711: step 1510, loss = 12.63 (39.8 examples/sec; 0.803 sec/batch)
2017-02-04 00:47:56.220351: step 1520, loss = 12.66 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:48:04.929136: step 1530, loss = 12.57 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 00:48:13.888005: step 1540, loss = 12.54 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 00:48:22.506407: step 1550, loss = 12.79 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 00:48:30.708528: step 1560, loss = 12.59 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 00:48:39.341618: step 1570, loss = 12.64 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:48:47.795198: step 1580, loss = 12.71 (40.1 examples/sec; 0.799 sec/batch)
2017-02-04 00:48:56.247661: step 1590, loss = 12.74 (38.8 examples/sec; 0.824 sec/batch)
2017-02-04 00:49:05.024100: step 1600, loss = 12.66 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 00:49:14.786486: step 1610, loss = 12.59 (39.2 examples/sec; 0.815 sec/batch)
2017-02-04 00:49:23.497104: step 1620, loss = 12.78 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:49:31.658534: step 1630, loss = 12.60 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 00:49:40.544749: step 1640, loss = 12.53 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:49:49.182892: step 1650, loss = 12.45 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 00:49:58.047368: step 1660, loss = 12.63 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 00:50:06.860169: step 1670, loss = 12.83 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:50:15.458721: step 1680, loss = 12.51 (36.1 examples/sec; 0.888 sec/batch)
2017-02-04 00:50:24.415861: step 1690, loss = 12.65 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:50:32.758892: step 1700, loss = 12.50 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:50:42.326630: step 1710, loss = 12.64 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:50:51.005000: step 1720, loss = 12.58 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:50:59.524898: step 1730, loss = 12.68 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:51:08.065177: step 1740, loss = 12.25 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:51:16.614408: step 1750, loss = 12.65 (35.3 examples/sec; 0.905 sec/batch)
2017-02-04 00:51:25.620306: step 1760, loss = 12.56 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 00:51:34.512247: step 1770, loss = 12.77 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 00:51:43.506414: step 1780, loss = 12.67 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:51:52.015414: step 1790, loss = 12.59 (39.8 examples/sec; 0.803 sec/batch)
2017-02-04 00:52:00.308222: step 1800, loss = 12.69 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 00:52:09.971220: step 1810, loss = 12.65 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 00:52:18.353826: step 1820, loss = 12.58 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 00:52:26.881516: step 1830, loss = 12.75 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:52:35.367949: step 1840, loss = 12.55 (36.1 examples/sec; 0.887 sec/batch)
2017-02-04 00:52:44.028648: step 1850, loss = 12.58 (35.8 examples/sec; 0.895 sec/batch)
2017-02-04 00:52:52.577610: step 1860, loss = 12.49 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 00:53:01.355257: step 1870, loss = 12.69 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:53:09.987376: step 1880, loss = 12.56 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:53:18.801997: step 1890, loss = 12.62 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 00:53:27.354757: step 1900, loss = 12.40 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:53:36.722510: step 1910, loss = 12.47 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 00:53:45.353002: step 1920, loss = 12.46 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:53:54.241693: step 1930, loss = 12.45 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:54:02.583589: step 1940, loss = 12.41 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:54:11.114955: step 1950, loss = 12.62 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 00:54:19.928086: step 1960, loss = 12.44 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 00:54:28.557497: step 1970, loss = 12.64 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:54:37.090775: step 1980, loss = 12.28 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:54:45.718643: step 1990, loss = 12.51 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 00:54:54.349041: step 2000, loss = 12.35 (36.0 examples/sec; 0.888 sec/batch)
2017-02-04 00:55:03.728468: step 2010, loss = 12.45 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 00:55:12.719612: step 2020, loss = 12.48 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:55:20.866096: step 2030, loss = 12.46 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 00:55:29.588453: step 2040, loss = 12.46 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 00:55:38.205421: step 2050, loss = 12.45 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 00:55:46.551831: step 2060, loss = 12.39 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 00:55:54.912109: step 2070, loss = 12.77 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:56:03.310743: step 2080, loss = 12.63 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 00:56:11.945699: step 2090, loss = 12.31 (39.2 examples/sec; 0.815 sec/batch)
2017-02-04 00:56:20.692425: step 2100, loss = 12.76 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 00:56:30.415787: step 2110, loss = 12.42 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 00:56:39.131612: step 2120, loss = 12.17 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 00:56:47.564139: step 2130, loss = 12.47 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 00:56:56.083118: step 2140, loss = 12.31 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 00:57:04.346063: step 2150, loss = 12.43 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 00:57:13.148561: step 2160, loss = 12.62 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:57:21.673567: step 2170, loss = 12.45 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 00:57:30.202833: step 2180, loss = 12.39 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 00:57:38.849845: step 2190, loss = 11.93 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 00:57:47.753796: step 2200, loss = 12.53 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 00:57:57.197404: step 2210, loss = 12.58 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 00:58:06.029808: step 2220, loss = 12.37 (39.5 examples/sec; 0.811 sec/batch)
2017-02-04 00:58:14.672756: step 2230, loss = 12.73 (35.5 examples/sec; 0.900 sec/batch)
2017-02-04 00:58:23.225295: step 2240, loss = 12.26 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 00:58:31.645971: step 2250, loss = 12.58 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 00:58:40.442350: step 2260, loss = 12.49 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 00:58:48.913649: step 2270, loss = 12.63 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 00:58:57.524590: step 2280, loss = 12.48 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 00:59:06.148396: step 2290, loss = 12.39 (39.4 examples/sec; 0.811 sec/batch)
2017-02-04 00:59:14.484585: step 2300, loss = 12.44 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 00:59:24.600473: step 2310, loss = 12.62 (35.6 examples/sec; 0.898 sec/batch)
2017-02-04 00:59:32.855332: step 2320, loss = 12.47 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 00:59:41.826297: step 2330, loss = 12.58 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 00:59:50.804213: step 2340, loss = 12.48 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 00:59:59.334061: step 2350, loss = 12.48 (35.2 examples/sec; 0.909 sec/batch)
2017-02-04 01:00:07.753364: step 2360, loss = 12.17 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:00:16.365548: step 2370, loss = 12.46 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 01:00:24.725037: step 2380, loss = 12.41 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:00:33.233423: step 2390, loss = 12.42 (40.0 examples/sec; 0.800 sec/batch)
2017-02-04 01:00:41.688170: step 2400, loss = 12.53 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:00:51.546232: step 2410, loss = 12.49 (35.6 examples/sec; 0.898 sec/batch)
2017-02-04 01:01:00.258046: step 2420, loss = 12.08 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:01:09.079635: step 2430, loss = 12.40 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:01:17.408455: step 2440, loss = 12.31 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 01:01:26.042730: step 2450, loss = 12.63 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 01:01:34.573421: step 2460, loss = 12.26 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 01:01:43.082895: step 2470, loss = 12.34 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 01:01:51.455930: step 2480, loss = 12.34 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:02:00.163515: step 2490, loss = 12.46 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 01:02:08.768380: step 2500, loss = 12.16 (39.6 examples/sec; 0.807 sec/batch)
2017-02-04 01:02:18.119589: step 2510, loss = 12.27 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:02:26.582923: step 2520, loss = 12.35 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 01:02:34.884891: step 2530, loss = 12.40 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:02:43.777802: step 2540, loss = 12.26 (40.0 examples/sec; 0.800 sec/batch)
2017-02-04 01:02:52.260608: step 2550, loss = 12.32 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 01:03:00.862888: step 2560, loss = 12.40 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:03:08.960567: step 2570, loss = 12.38 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:03:17.300653: step 2580, loss = 12.23 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:03:25.773437: step 2590, loss = 12.36 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:03:34.119214: step 2600, loss = 12.46 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:03:43.805582: step 2610, loss = 12.22 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:03:52.341375: step 2620, loss = 12.20 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:04:00.592840: step 2630, loss = 12.55 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:04:09.150585: step 2640, loss = 12.36 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:04:17.520261: step 2650, loss = 12.18 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:04:25.872012: step 2660, loss = 12.57 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:04:34.003863: step 2670, loss = 12.37 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:04:42.562318: step 2680, loss = 12.43 (39.6 examples/sec; 0.809 sec/batch)
2017-02-04 01:04:50.749269: step 2690, loss = 12.40 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:04:58.969005: step 2700, loss = 12.28 (39.5 examples/sec; 0.811 sec/batch)
2017-02-04 01:05:09.014190: step 2710, loss = 12.27 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:05:17.436567: step 2720, loss = 12.62 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 01:05:25.647001: step 2730, loss = 12.50 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:05:33.850474: step 2740, loss = 12.55 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 01:05:42.172954: step 2750, loss = 12.37 (39.0 examples/sec; 0.821 sec/batch)
2017-02-04 01:05:50.450329: step 2760, loss = 12.06 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:05:58.977829: step 2770, loss = 12.26 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 01:06:07.053450: step 2780, loss = 12.32 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:06:15.522981: step 2790, loss = 12.39 (35.5 examples/sec; 0.903 sec/batch)
2017-02-04 01:06:23.957974: step 2800, loss = 12.30 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:06:33.499895: step 2810, loss = 12.04 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:06:41.869581: step 2820, loss = 12.20 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:06:50.385762: step 2830, loss = 12.31 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 01:06:58.675945: step 2840, loss = 12.23 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:07:07.103662: step 2850, loss = 12.38 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:07:15.541826: step 2860, loss = 12.35 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:07:23.895839: step 2870, loss = 12.17 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:07:32.811075: step 2880, loss = 12.26 (35.3 examples/sec; 0.905 sec/batch)
2017-02-04 01:07:41.619906: step 2890, loss = 12.21 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:07:49.924983: step 2900, loss = 12.02 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:07:59.620611: step 2910, loss = 12.32 (35.6 examples/sec; 0.899 sec/batch)
2017-02-04 01:08:08.056415: step 2920, loss = 12.08 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:08:16.758565: step 2930, loss = 12.11 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:08:24.988546: step 2940, loss = 12.00 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:08:33.517219: step 2950, loss = 12.28 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:08:42.050035: step 2960, loss = 12.01 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 01:08:50.158502: step 2970, loss = 12.49 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 01:08:58.841609: step 2980, loss = 12.44 (35.7 examples/sec; 0.898 sec/batch)
2017-02-04 01:09:07.727967: step 2990, loss = 11.98 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 01:09:16.092032: step 3000, loss = 12.46 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:09:25.385395: step 3010, loss = 12.27 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:09:33.651576: step 3020, loss = 12.21 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:09:42.377040: step 3030, loss = 12.51 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:09:50.988744: step 3040, loss = 12.34 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:09:59.246043: step 3050, loss = 12.22 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:10:07.370365: step 3060, loss = 12.12 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:10:16.269369: step 3070, loss = 12.36 (35.8 examples/sec; 0.895 sec/batch)
2017-02-04 01:10:24.709074: step 3080, loss = 12.28 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:10:32.794082: step 3090, loss = 12.25 (39.6 examples/sec; 0.807 sec/batch)
2017-02-04 01:10:41.215403: step 3100, loss = 12.17 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:10:50.996148: step 3110, loss = 12.24 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 01:10:59.407350: step 3120, loss = 12.20 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:11:07.874219: step 3130, loss = 11.88 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:11:16.421231: step 3140, loss = 12.42 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:11:24.534233: step 3150, loss = 11.95 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:11:33.258536: step 3160, loss = 11.87 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:11:41.701091: step 3170, loss = 12.17 (35.3 examples/sec; 0.908 sec/batch)
2017-02-04 01:11:50.302411: step 3180, loss = 12.44 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:11:58.433252: step 3190, loss = 12.09 (39.5 examples/sec; 0.811 sec/batch)
2017-02-04 01:12:06.950522: step 3200, loss = 12.20 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:12:16.245152: step 3210, loss = 12.10 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:12:24.425582: step 3220, loss = 12.06 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:12:33.059271: step 3230, loss = 12.23 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:12:41.474663: step 3240, loss = 12.24 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:12:49.843551: step 3250, loss = 12.12 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:12:58.375721: step 3260, loss = 12.12 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:13:06.815556: step 3270, loss = 12.30 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 01:13:15.332380: step 3280, loss = 12.31 (35.3 examples/sec; 0.907 sec/batch)
2017-02-04 01:13:23.860257: step 3290, loss = 11.86 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:13:32.465869: step 3300, loss = 12.07 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:13:41.812697: step 3310, loss = 12.11 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:13:50.435606: step 3320, loss = 12.08 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:13:58.857951: step 3330, loss = 12.05 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:14:07.372016: step 3340, loss = 12.31 (36.1 examples/sec; 0.887 sec/batch)
2017-02-04 01:14:15.827102: step 3350, loss = 11.98 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:14:24.353784: step 3360, loss = 12.15 (39.3 examples/sec; 0.813 sec/batch)
2017-02-04 01:14:32.461803: step 3370, loss = 12.09 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:14:40.997218: step 3380, loss = 11.95 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:14:49.523181: step 3390, loss = 12.20 (39.5 examples/sec; 0.810 sec/batch)
2017-02-04 01:14:58.122514: step 3400, loss = 12.05 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:15:07.439392: step 3410, loss = 12.25 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:15:15.540139: step 3420, loss = 11.71 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:15:24.147918: step 3430, loss = 12.07 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:15:32.582986: step 3440, loss = 12.22 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:15:41.099478: step 3450, loss = 12.01 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:15:49.472111: step 3460, loss = 12.13 (39.1 examples/sec; 0.817 sec/batch)
2017-02-04 01:15:58.361295: step 3470, loss = 11.87 (40.0 examples/sec; 0.800 sec/batch)
2017-02-04 01:16:06.467682: step 3480, loss = 12.03 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:16:14.662035: step 3490, loss = 12.32 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:16:22.779295: step 3500, loss = 11.81 (39.0 examples/sec; 0.821 sec/batch)
2017-02-04 01:16:32.111223: step 3510, loss = 12.33 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 01:16:40.821364: step 3520, loss = 12.21 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:16:49.157127: step 3530, loss = 11.96 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:16:57.576408: step 3540, loss = 12.25 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:17:05.907762: step 3550, loss = 12.05 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:17:14.012646: step 3560, loss = 12.45 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 01:17:22.369081: step 3570, loss = 11.99 (35.7 examples/sec; 0.896 sec/batch)
2017-02-04 01:17:30.901573: step 3580, loss = 12.27 (39.6 examples/sec; 0.807 sec/batch)
2017-02-04 01:17:39.240110: step 3590, loss = 12.18 (35.7 examples/sec; 0.898 sec/batch)
2017-02-04 01:17:47.961566: step 3600, loss = 12.19 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 01:17:57.337583: step 3610, loss = 11.82 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:18:06.107475: step 3620, loss = 11.88 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:18:14.511910: step 3630, loss = 11.74 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 01:18:23.077977: step 3640, loss = 11.82 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:18:31.600719: step 3650, loss = 11.87 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:18:40.324240: step 3660, loss = 12.05 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:18:48.760107: step 3670, loss = 11.88 (39.5 examples/sec; 0.810 sec/batch)
2017-02-04 01:18:57.030241: step 3680, loss = 11.73 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:19:05.735551: step 3690, loss = 12.24 (39.8 examples/sec; 0.803 sec/batch)
2017-02-04 01:19:14.461803: step 3700, loss = 11.70 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:19:24.341305: step 3710, loss = 11.72 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:19:33.312339: step 3720, loss = 11.95 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:19:41.501356: step 3730, loss = 12.21 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:19:50.060670: step 3740, loss = 12.05 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:19:58.190068: step 3750, loss = 12.02 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 01:20:06.567646: step 3760, loss = 12.06 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:20:15.093601: step 3770, loss = 11.96 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:20:23.162499: step 3780, loss = 12.00 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 01:20:31.443472: step 3790, loss = 12.05 (38.9 examples/sec; 0.822 sec/batch)
2017-02-04 01:20:40.086379: step 3800, loss = 12.08 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:20:49.907438: step 3810, loss = 11.84 (38.8 examples/sec; 0.824 sec/batch)
2017-02-04 01:20:58.437466: step 3820, loss = 12.06 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 01:21:06.545086: step 3830, loss = 11.80 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:21:15.283183: step 3840, loss = 11.98 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:21:23.646874: step 3850, loss = 11.74 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:21:32.086776: step 3860, loss = 12.02 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:21:41.066783: step 3870, loss = 12.19 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:21:49.229570: step 3880, loss = 11.85 (39.5 examples/sec; 0.810 sec/batch)
2017-02-04 01:21:57.760908: step 3890, loss = 12.06 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:22:06.754918: step 3900, loss = 12.14 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 01:22:16.239737: step 3910, loss = 12.19 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 01:22:24.409592: step 3920, loss = 11.65 (39.6 examples/sec; 0.809 sec/batch)
2017-02-04 01:22:32.493759: step 3930, loss = 12.04 (39.3 examples/sec; 0.815 sec/batch)
2017-02-04 01:22:40.765974: step 3940, loss = 12.22 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:22:49.496832: step 3950, loss = 12.00 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:22:57.600306: step 3960, loss = 11.89 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:23:05.877055: step 3970, loss = 11.89 (35.7 examples/sec; 0.898 sec/batch)
2017-02-04 01:23:14.399365: step 3980, loss = 11.57 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:23:22.843727: step 3990, loss = 11.84 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:23:31.655328: step 4000, loss = 11.75 (39.8 examples/sec; 0.803 sec/batch)
2017-02-04 01:23:41.836794: step 4010, loss = 11.46 (35.5 examples/sec; 0.903 sec/batch)
2017-02-04 01:23:50.820507: step 4020, loss = 11.79 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:23:59.071080: step 4030, loss = 11.64 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:24:07.685660: step 4040, loss = 11.76 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:24:16.127330: step 4050, loss = 11.87 (38.9 examples/sec; 0.822 sec/batch)
2017-02-04 01:24:24.836627: step 4060, loss = 11.92 (35.4 examples/sec; 0.903 sec/batch)
2017-02-04 01:24:32.922000: step 4070, loss = 11.77 (39.0 examples/sec; 0.821 sec/batch)
2017-02-04 01:24:41.014974: step 4080, loss = 11.97 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 01:24:49.264289: step 4090, loss = 11.84 (39.5 examples/sec; 0.809 sec/batch)
2017-02-04 01:24:57.377386: step 4100, loss = 11.98 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:25:06.825032: step 4110, loss = 11.91 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:25:14.916908: step 4120, loss = 11.90 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:25:23.173049: step 4130, loss = 12.23 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:25:31.879826: step 4140, loss = 11.88 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 01:25:39.969891: step 4150, loss = 12.05 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:25:48.423529: step 4160, loss = 11.94 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 01:25:56.627053: step 4170, loss = 12.02 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 01:26:04.724311: step 4180, loss = 11.81 (40.0 examples/sec; 0.800 sec/batch)
2017-02-04 01:26:12.829493: step 4190, loss = 11.84 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:26:21.548125: step 4200, loss = 11.90 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:26:31.103893: step 4210, loss = 12.12 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:26:39.569436: step 4220, loss = 11.87 (39.1 examples/sec; 0.819 sec/batch)
2017-02-04 01:26:47.692452: step 4230, loss = 11.57 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:26:56.667027: step 4240, loss = 12.06 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:27:04.788942: step 4250, loss = 11.72 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 01:27:12.890861: step 4260, loss = 12.09 (39.1 examples/sec; 0.818 sec/batch)
2017-02-04 01:27:21.577587: step 4270, loss = 11.76 (39.5 examples/sec; 0.811 sec/batch)
2017-02-04 01:27:30.030515: step 4280, loss = 11.79 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 01:27:38.134872: step 4290, loss = 11.69 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 01:27:46.355718: step 4300, loss = 11.95 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:27:56.387670: step 4310, loss = 12.19 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:28:04.692895: step 4320, loss = 12.35 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:28:13.141148: step 4330, loss = 11.95 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:28:21.579619: step 4340, loss = 11.78 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:28:30.048448: step 4350, loss = 11.49 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:28:38.186602: step 4360, loss = 11.57 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:28:46.384723: step 4370, loss = 11.79 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 01:28:54.509035: step 4380, loss = 11.77 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:29:02.935712: step 4390, loss = 11.79 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:29:11.806460: step 4400, loss = 11.58 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:29:21.541049: step 4410, loss = 11.68 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:29:30.519506: step 4420, loss = 11.99 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:29:38.972328: step 4430, loss = 11.71 (36.0 examples/sec; 0.890 sec/batch)
2017-02-04 01:29:47.671545: step 4440, loss = 11.92 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:29:56.112749: step 4450, loss = 11.79 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 01:30:04.729184: step 4460, loss = 11.53 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:30:13.451537: step 4470, loss = 11.88 (36.0 examples/sec; 0.888 sec/batch)
2017-02-04 01:30:21.548005: step 4480, loss = 11.68 (40.0 examples/sec; 0.801 sec/batch)
2017-02-04 01:30:30.453134: step 4490, loss = 11.89 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:30:39.072597: step 4500, loss = 12.04 (35.3 examples/sec; 0.906 sec/batch)
2017-02-04 01:30:48.888259: step 4510, loss = 11.98 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:30:57.157349: step 4520, loss = 12.07 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:31:05.874444: step 4530, loss = 11.63 (39.0 examples/sec; 0.820 sec/batch)
2017-02-04 01:31:13.974098: step 4540, loss = 11.83 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:31:22.441124: step 4550, loss = 11.70 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:31:31.339226: step 4560, loss = 11.54 (39.4 examples/sec; 0.813 sec/batch)
2017-02-04 01:31:39.874147: step 4570, loss = 11.65 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:31:48.390315: step 4580, loss = 11.62 (35.9 examples/sec; 0.892 sec/batch)
2017-02-04 01:31:56.525644: step 4590, loss = 11.79 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:32:05.162965: step 4600, loss = 11.63 (35.4 examples/sec; 0.905 sec/batch)
2017-02-04 01:32:15.336158: step 4610, loss = 11.53 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:32:24.317949: step 4620, loss = 11.55 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:32:32.848336: step 4630, loss = 11.94 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:32:40.937811: step 4640, loss = 11.75 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:32:49.011695: step 4650, loss = 11.70 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:32:57.382317: step 4660, loss = 11.71 (39.2 examples/sec; 0.816 sec/batch)
2017-02-04 01:33:05.498472: step 4670, loss = 11.45 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:33:14.227747: step 4680, loss = 11.29 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:33:23.227060: step 4690, loss = 11.69 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:33:31.537432: step 4700, loss = 11.80 (39.2 examples/sec; 0.817 sec/batch)
2017-02-04 01:33:41.348587: step 4710, loss = 11.90 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:33:50.319624: step 4720, loss = 11.77 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:33:58.877778: step 4730, loss = 11.84 (35.5 examples/sec; 0.900 sec/batch)
2017-02-04 01:34:07.216144: step 4740, loss = 11.03 (39.7 examples/sec; 0.807 sec/batch)
2017-02-04 01:34:16.134293: step 4750, loss = 11.23 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:34:24.741146: step 4760, loss = 12.02 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:34:33.074370: step 4770, loss = 11.59 (39.5 examples/sec; 0.810 sec/batch)
2017-02-04 01:34:41.797718: step 4780, loss = 11.58 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:34:49.974468: step 4790, loss = 11.77 (35.6 examples/sec; 0.900 sec/batch)
2017-02-04 01:34:58.957946: step 4800, loss = 11.68 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:35:09.098883: step 4810, loss = 11.28 (35.3 examples/sec; 0.905 sec/batch)
2017-02-04 01:35:17.830741: step 4820, loss = 11.59 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:35:26.269461: step 4830, loss = 12.07 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:35:34.907423: step 4840, loss = 11.69 (39.4 examples/sec; 0.812 sec/batch)
2017-02-04 01:35:43.542820: step 4850, loss = 11.64 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:35:52.568991: step 4860, loss = 11.80 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:36:01.106092: step 4870, loss = 11.72 (39.9 examples/sec; 0.803 sec/batch)
2017-02-04 01:36:09.836049: step 4880, loss = 11.24 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 01:36:18.541070: step 4890, loss = 11.58 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:36:26.974150: step 4900, loss = 11.67 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:36:36.543247: step 4910, loss = 11.32 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:36:45.157750: step 4920, loss = 11.49 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:36:53.692798: step 4930, loss = 11.58 (35.7 examples/sec; 0.896 sec/batch)
2017-02-04 01:37:02.097153: step 4940, loss = 11.78 (35.4 examples/sec; 0.904 sec/batch)
2017-02-04 01:37:10.676497: step 4950, loss = 11.62 (35.5 examples/sec; 0.902 sec/batch)
2017-02-04 01:37:19.699332: step 4960, loss = 11.67 (35.3 examples/sec; 0.908 sec/batch)
2017-02-04 01:37:28.705291: step 4970, loss = 11.80 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 01:37:37.244971: step 4980, loss = 11.67 (39.6 examples/sec; 0.807 sec/batch)
2017-02-04 01:37:46.050092: step 4990, loss = 11.45 (35.8 examples/sec; 0.895 sec/batch)
2017-02-04 01:37:54.382861: step 5000, loss = 11.30 (35.9 examples/sec; 0.890 sec/batch)
2017-02-04 01:38:48.849911: step 5010, loss = 11.69 (39.2 examples/sec; 0.815 sec/batch)
2017-02-04 01:38:56.923789: step 5020, loss = 11.60 (39.8 examples/sec; 0.805 sec/batch)
2017-02-04 01:39:05.084285: step 5030, loss = 11.76 (39.9 examples/sec; 0.801 sec/batch)
2017-02-04 01:39:13.268107: step 5040, loss = 11.79 (36.1 examples/sec; 0.887 sec/batch)
2017-02-04 01:39:21.622496: step 5050, loss = 11.81 (39.8 examples/sec; 0.804 sec/batch)
2017-02-04 01:39:29.901554: step 5060, loss = 11.62 (36.0 examples/sec; 0.889 sec/batch)
2017-02-04 01:39:38.011230: step 5070, loss = 11.83 (39.7 examples/sec; 0.805 sec/batch)
2017-02-04 01:39:46.292691: step 5080, loss = 11.41 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:39:55.013527: step 5090, loss = 11.69 (39.3 examples/sec; 0.814 sec/batch)
2017-02-04 01:40:03.515889: step 5100, loss = 12.01 (35.8 examples/sec; 0.893 sec/batch)
2017-02-04 01:40:12.973415: step 5110, loss = 11.66 (39.8 examples/sec; 0.803 sec/batch)
2017-02-04 01:40:21.843931: step 5120, loss = 11.78 (35.9 examples/sec; 0.891 sec/batch)
2017-02-04 01:40:30.640944: step 5130, loss = 11.54 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:40:38.880631: step 5140, loss = 11.60 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:40:47.573805: step 5150, loss = 11.54 (35.5 examples/sec; 0.901 sec/batch)
2017-02-04 01:40:55.922953: step 5160, loss = 11.30 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:41:04.355598: step 5170, loss = 11.11 (39.6 examples/sec; 0.808 sec/batch)
2017-02-04 01:41:12.811012: step 5180, loss = 11.50 (35.2 examples/sec; 0.909 sec/batch)
2017-02-04 01:41:21.804128: step 5190, loss = 11.27 (35.8 examples/sec; 0.894 sec/batch)
2017-02-04 01:41:30.138893: step 5200, loss = 11.53 (35.7 examples/sec; 0.895 sec/batch)
2017-02-04 01:41:39.589414: step 5210, loss = 11.94 (39.9 examples/sec; 0.802 sec/batch)
2017-02-04 01:41:48.213521: step 5220, loss = 11.32 (35.7 examples/sec; 0.897 sec/batch)
2017-02-04 01:41:56.660974: step 5230, loss = 11.35 (39.7 examples/sec; 0.806 sec/batch)
2017-02-04 01:42:05.452788: step 5240, loss = 11.83 (35.6 examples/sec; 0.900 sec/batch)