2017-02-04 01:52:16.893972: step 0, loss = 13.05 (4.9 examples/sec; 6.583 sec/batch)
2017-02-04 01:53:07.758441: step 10, loss = 13.84 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 01:53:22.067827: step 20, loss = 14.55 (22.4 examples/sec; 1.432 sec/batch)
2017-02-04 01:53:36.405030: step 30, loss = 15.45 (22.6 examples/sec; 1.419 sec/batch)
2017-02-04 01:53:50.758083: step 40, loss = 14.04 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 01:54:05.113914: step 50, loss = 13.64 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 01:54:19.460905: step 60, loss = 13.37 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 01:54:33.804760: step 70, loss = 13.11 (22.2 examples/sec; 1.440 sec/batch)
2017-02-04 01:54:48.136621: step 80, loss = 13.16 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 01:55:02.442226: step 90, loss = 13.10 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 01:55:16.735241: step 100, loss = 13.09 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 01:55:32.871922: step 110, loss = 13.10 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 01:55:47.182481: step 120, loss = 13.23 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 01:56:01.501972: step 130, loss = 13.03 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 01:56:15.807855: step 140, loss = 13.17 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 01:56:30.112026: step 150, loss = 13.12 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 01:56:44.409197: step 160, loss = 13.10 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 01:56:58.703972: step 170, loss = 13.14 (22.2 examples/sec; 1.439 sec/batch)
2017-02-04 01:57:13.006908: step 180, loss = 13.04 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 01:57:27.327043: step 190, loss = 13.06 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 01:57:41.602334: step 200, loss = 13.11 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 01:57:57.705430: step 210, loss = 13.04 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 01:58:11.997484: step 220, loss = 13.15 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 01:58:26.281101: step 230, loss = 13.08 (22.3 examples/sec; 1.436 sec/batch)
2017-02-04 01:58:40.551990: step 240, loss = 13.08 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 01:58:54.857480: step 250, loss = 13.12 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 01:59:09.151274: step 260, loss = 13.08 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 01:59:23.422598: step 270, loss = 13.06 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 01:59:37.747236: step 280, loss = 13.03 (22.2 examples/sec; 1.440 sec/batch)
2017-02-04 01:59:52.049685: step 290, loss = 13.05 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:00:06.300977: step 300, loss = 13.04 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:00:22.417121: step 310, loss = 13.08 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:00:36.699189: step 320, loss = 13.00 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:00:50.990634: step 330, loss = 13.03 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:01:05.292576: step 340, loss = 13.03 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:01:19.612532: step 350, loss = 13.09 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:01:33.907365: step 360, loss = 13.02 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:01:48.193777: step 370, loss = 13.05 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:02:02.493997: step 380, loss = 13.01 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:02:16.778176: step 390, loss = 12.97 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:02:31.052744: step 400, loss = 13.03 (22.4 examples/sec; 1.432 sec/batch)
2017-02-04 02:02:47.143543: step 410, loss = 13.01 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:03:01.428562: step 420, loss = 13.01 (22.3 examples/sec; 1.435 sec/batch)
2017-02-04 02:03:15.709594: step 430, loss = 13.00 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:03:30.027451: step 440, loss = 13.06 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:03:44.292081: step 450, loss = 13.07 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:03:58.549150: step 460, loss = 13.01 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:04:12.823832: step 470, loss = 12.94 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:04:27.127650: step 480, loss = 13.01 (22.3 examples/sec; 1.436 sec/batch)
2017-02-04 02:04:41.404858: step 490, loss = 13.05 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:04:55.694832: step 500, loss = 13.00 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:05:11.776490: step 510, loss = 12.97 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:05:26.075699: step 520, loss = 12.96 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:05:40.383966: step 530, loss = 13.03 (22.2 examples/sec; 1.439 sec/batch)
2017-02-04 02:05:54.639329: step 540, loss = 13.00 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:06:08.930584: step 550, loss = 12.93 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:06:23.204674: step 560, loss = 13.01 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:06:37.503542: step 570, loss = 12.94 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:06:51.773401: step 580, loss = 12.96 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:07:06.038594: step 590, loss = 12.97 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:07:20.314031: step 600, loss = 12.93 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:07:36.393904: step 610, loss = 12.99 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:07:50.700464: step 620, loss = 12.99 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:08:04.968994: step 630, loss = 12.96 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:08:19.236996: step 640, loss = 12.83 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:08:33.509062: step 650, loss = 12.97 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:08:47.807087: step 660, loss = 12.97 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:09:02.087555: step 670, loss = 12.89 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:09:16.350519: step 680, loss = 12.98 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:09:30.633238: step 690, loss = 12.93 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:09:44.935792: step 700, loss = 12.95 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:10:01.038558: step 710, loss = 12.94 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:10:15.314085: step 720, loss = 12.96 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:10:29.593109: step 730, loss = 12.94 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:10:43.855515: step 740, loss = 12.92 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:10:58.114471: step 750, loss = 12.85 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:11:12.386673: step 760, loss = 12.91 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:11:26.662306: step 770, loss = 12.89 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:11:40.936815: step 780, loss = 12.96 (22.4 examples/sec; 1.425 sec/batch)
2017-02-04 02:11:55.222804: step 790, loss = 12.92 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:12:09.505279: step 800, loss = 12.90 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:12:25.585556: step 810, loss = 12.88 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 02:12:39.889783: step 820, loss = 12.90 (22.2 examples/sec; 1.443 sec/batch)
2017-02-04 02:12:54.155524: step 830, loss = 12.94 (22.4 examples/sec; 1.432 sec/batch)
2017-02-04 02:13:08.435123: step 840, loss = 12.93 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:13:22.718229: step 850, loss = 12.85 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:13:36.998458: step 860, loss = 12.89 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:13:51.304465: step 870, loss = 12.91 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:14:05.563050: step 880, loss = 12.86 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 02:14:19.853076: step 890, loss = 12.84 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:14:34.125551: step 900, loss = 12.89 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:14:50.218364: step 910, loss = 12.89 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:15:04.502044: step 920, loss = 12.91 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:15:18.766342: step 930, loss = 12.87 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:15:33.057156: step 940, loss = 12.85 (22.3 examples/sec; 1.436 sec/batch)
2017-02-04 02:15:47.344554: step 950, loss = 12.86 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:16:01.632465: step 960, loss = 12.87 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:16:15.926032: step 970, loss = 12.91 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:16:30.205343: step 980, loss = 12.86 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:16:44.476820: step 990, loss = 12.85 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:16:58.764882: step 1000, loss = 12.81 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:17:15.013111: step 1010, loss = 12.88 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:17:29.324654: step 1020, loss = 12.90 (22.2 examples/sec; 1.439 sec/batch)
2017-02-04 02:17:43.586664: step 1030, loss = 12.75 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:17:57.854958: step 1040, loss = 12.85 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:18:12.099033: step 1050, loss = 12.86 (22.6 examples/sec; 1.418 sec/batch)
2017-02-04 02:18:26.352636: step 1060, loss = 12.79 (22.6 examples/sec; 1.419 sec/batch)
2017-02-04 02:18:40.609074: step 1070, loss = 12.81 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:18:54.892067: step 1080, loss = 12.88 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:19:09.167015: step 1090, loss = 12.81 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:19:23.433268: step 1100, loss = 12.78 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:19:39.616255: step 1110, loss = 12.81 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:19:53.910617: step 1120, loss = 12.89 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:20:08.183386: step 1130, loss = 12.83 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:20:22.454343: step 1140, loss = 12.84 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:20:36.711694: step 1150, loss = 12.75 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:20:50.998248: step 1160, loss = 12.73 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:21:05.285719: step 1170, loss = 12.83 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:21:19.568621: step 1180, loss = 12.80 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:21:33.839191: step 1190, loss = 12.75 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:21:48.094212: step 1200, loss = 12.81 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:22:04.211902: step 1210, loss = 12.82 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:22:18.474430: step 1220, loss = 12.85 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:22:32.755142: step 1230, loss = 12.81 (22.3 examples/sec; 1.436 sec/batch)
2017-02-04 02:22:47.037466: step 1240, loss = 12.79 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:23:01.296468: step 1250, loss = 12.77 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:23:15.547936: step 1260, loss = 12.81 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:23:29.831870: step 1270, loss = 12.77 (22.2 examples/sec; 1.438 sec/batch)
2017-02-04 02:23:44.109576: step 1280, loss = 12.74 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:23:58.404538: step 1290, loss = 12.83 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:24:12.705125: step 1300, loss = 12.79 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:24:28.823259: step 1310, loss = 12.83 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:24:43.114891: step 1320, loss = 12.65 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:24:57.389285: step 1330, loss = 12.80 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:25:11.656777: step 1340, loss = 12.78 (22.2 examples/sec; 1.439 sec/batch)
2017-02-04 02:25:25.955577: step 1350, loss = 12.77 (22.3 examples/sec; 1.437 sec/batch)
2017-02-04 02:25:40.229347: step 1360, loss = 12.78 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:25:54.507618: step 1370, loss = 12.78 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:26:08.760496: step 1380, loss = 12.76 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:26:23.056789: step 1390, loss = 12.82 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:26:37.325849: step 1400, loss = 12.66 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:26:53.454637: step 1410, loss = 12.71 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:27:07.744153: step 1420, loss = 12.78 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:27:22.004860: step 1430, loss = 12.75 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:27:36.276472: step 1440, loss = 12.75 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:27:50.555987: step 1450, loss = 12.75 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:28:04.866162: step 1460, loss = 12.76 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:28:19.147090: step 1470, loss = 12.78 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:28:33.403198: step 1480, loss = 12.78 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:28:47.681688: step 1490, loss = 12.76 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:29:01.934617: step 1500, loss = 12.74 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:29:18.135034: step 1510, loss = 12.75 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:29:32.373999: step 1520, loss = 12.66 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:29:46.633976: step 1530, loss = 12.76 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:30:00.898155: step 1540, loss = 12.78 (22.6 examples/sec; 1.419 sec/batch)
2017-02-04 02:30:15.154670: step 1550, loss = 12.69 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:30:29.419947: step 1560, loss = 12.80 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:30:43.715925: step 1570, loss = 12.73 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:30:57.982307: step 1580, loss = 12.69 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:31:12.287465: step 1590, loss = 12.64 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:31:26.563279: step 1600, loss = 12.62 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:31:42.653378: step 1610, loss = 12.72 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:31:56.920312: step 1620, loss = 12.68 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:32:11.242386: step 1630, loss = 12.80 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:32:25.510114: step 1640, loss = 12.67 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:32:39.798416: step 1650, loss = 12.76 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:32:54.070239: step 1660, loss = 12.65 (22.3 examples/sec; 1.435 sec/batch)
2017-02-04 02:33:08.346555: step 1670, loss = 12.59 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:33:22.644754: step 1680, loss = 12.64 (22.3 examples/sec; 1.438 sec/batch)
2017-02-04 02:33:36.946739: step 1690, loss = 12.70 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:33:51.245058: step 1700, loss = 12.56 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:34:07.457445: step 1710, loss = 12.69 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:34:21.753559: step 1720, loss = 12.67 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:34:36.029783: step 1730, loss = 12.65 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:34:50.305674: step 1740, loss = 12.70 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:35:04.609419: step 1750, loss = 12.62 (22.3 examples/sec; 1.437 sec/batch)
2017-02-04 02:35:18.900083: step 1760, loss = 12.69 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:35:33.184385: step 1770, loss = 12.63 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:35:47.477376: step 1780, loss = 12.59 (22.2 examples/sec; 1.442 sec/batch)
2017-02-04 02:36:01.728355: step 1790, loss = 12.59 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:36:15.996279: step 1800, loss = 12.57 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:36:32.105922: step 1810, loss = 12.62 (22.3 examples/sec; 1.436 sec/batch)
2017-02-04 02:36:46.425060: step 1820, loss = 12.60 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:37:00.701325: step 1830, loss = 12.60 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:37:14.958240: step 1840, loss = 12.68 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:37:29.230998: step 1850, loss = 12.65 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:37:43.482835: step 1860, loss = 12.57 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:37:57.744645: step 1870, loss = 12.56 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:38:12.013093: step 1880, loss = 12.67 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:38:26.295233: step 1890, loss = 12.64 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:38:40.602912: step 1900, loss = 12.70 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:38:56.763157: step 1910, loss = 12.66 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:39:11.016823: step 1920, loss = 12.67 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 02:39:25.285887: step 1930, loss = 12.59 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:39:39.551798: step 1940, loss = 12.59 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:39:53.813172: step 1950, loss = 12.55 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:40:08.097558: step 1960, loss = 12.57 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:40:22.356220: step 1970, loss = 12.65 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:40:36.626793: step 1980, loss = 12.67 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:40:50.880890: step 1990, loss = 12.61 (22.6 examples/sec; 1.417 sec/batch)
2017-02-04 02:41:05.125880: step 2000, loss = 12.66 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:41:21.219189: step 2010, loss = 12.61 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:41:35.504462: step 2020, loss = 12.69 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:41:49.783525: step 2030, loss = 12.56 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:42:04.065110: step 2040, loss = 12.71 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:42:18.355066: step 2050, loss = 12.65 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:42:32.629881: step 2060, loss = 12.51 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:42:46.883916: step 2070, loss = 12.55 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 02:43:01.153586: step 2080, loss = 12.65 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:43:15.417113: step 2090, loss = 12.61 (22.6 examples/sec; 1.417 sec/batch)
2017-02-04 02:43:29.717769: step 2100, loss = 12.54 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:43:45.817450: step 2110, loss = 12.51 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:44:00.083664: step 2120, loss = 12.52 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:44:14.359554: step 2130, loss = 12.59 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:44:28.633943: step 2140, loss = 12.46 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:44:42.948075: step 2150, loss = 12.55 (22.2 examples/sec; 1.441 sec/batch)
2017-02-04 02:44:57.209512: step 2160, loss = 12.59 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:45:11.493493: step 2170, loss = 12.52 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:45:25.782603: step 2180, loss = 12.57 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:45:40.043662: step 2190, loss = 12.53 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:45:54.326292: step 2200, loss = 12.47 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:46:10.576646: step 2210, loss = 12.58 (22.2 examples/sec; 1.440 sec/batch)
2017-02-04 02:46:24.843703: step 2220, loss = 12.50 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:46:39.119783: step 2230, loss = 12.65 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 02:46:53.384111: step 2240, loss = 12.52 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:47:07.668541: step 2250, loss = 12.43 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:47:21.939622: step 2260, loss = 12.62 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:47:36.218014: step 2270, loss = 12.56 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:47:50.483169: step 2280, loss = 12.55 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:48:04.761511: step 2290, loss = 12.32 (22.4 examples/sec; 1.425 sec/batch)
2017-02-04 02:48:19.019944: step 2300, loss = 12.51 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:48:35.104523: step 2310, loss = 12.46 (22.6 examples/sec; 1.418 sec/batch)
2017-02-04 02:48:49.388289: step 2320, loss = 12.44 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:49:03.672053: step 2330, loss = 12.29 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:49:17.972809: step 2340, loss = 12.44 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:49:32.264644: step 2350, loss = 12.39 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:49:46.528053: step 2360, loss = 12.50 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:50:00.818796: step 2370, loss = 12.42 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:50:15.107469: step 2380, loss = 12.51 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:50:29.370730: step 2390, loss = 12.51 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:50:43.648026: step 2400, loss = 12.39 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:50:59.727064: step 2410, loss = 12.47 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:51:14.015604: step 2420, loss = 12.59 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:51:28.277037: step 2430, loss = 12.45 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:51:42.551347: step 2440, loss = 12.30 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:51:56.813306: step 2450, loss = 12.48 (22.6 examples/sec; 1.419 sec/batch)
2017-02-04 02:52:11.071016: step 2460, loss = 12.31 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:52:25.352483: step 2470, loss = 12.49 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 02:52:39.601737: step 2480, loss = 12.50 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:52:53.875110: step 2490, loss = 12.43 (22.3 examples/sec; 1.435 sec/batch)
2017-02-04 02:53:08.130848: step 2500, loss = 12.51 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:53:24.245382: step 2510, loss = 12.43 (22.3 examples/sec; 1.438 sec/batch)
2017-02-04 02:53:38.525818: step 2520, loss = 12.53 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:53:52.824713: step 2530, loss = 12.53 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:54:07.100950: step 2540, loss = 12.34 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 02:54:21.387819: step 2550, loss = 12.35 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 02:54:35.659564: step 2560, loss = 12.31 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:54:49.910164: step 2570, loss = 12.35 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 02:55:04.190031: step 2580, loss = 12.35 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:55:18.475363: step 2590, loss = 12.34 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:55:32.759626: step 2600, loss = 12.60 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:55:48.845987: step 2610, loss = 12.45 (22.5 examples/sec; 1.423 sec/batch)
2017-02-04 02:56:03.141744: step 2620, loss = 12.26 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:56:17.415219: step 2630, loss = 12.33 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:56:31.699881: step 2640, loss = 12.52 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:56:45.985570: step 2650, loss = 12.46 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:57:00.241513: step 2660, loss = 12.45 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:57:14.539578: step 2670, loss = 12.40 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 02:57:28.805412: step 2680, loss = 12.43 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:57:43.099448: step 2690, loss = 12.32 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 02:57:57.382635: step 2700, loss = 12.44 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 02:58:13.651181: step 2710, loss = 12.54 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:58:27.926933: step 2720, loss = 12.35 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:58:42.215489: step 2730, loss = 12.38 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 02:58:56.506319: step 2740, loss = 12.26 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 02:59:10.810090: step 2750, loss = 12.39 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 02:59:25.106362: step 2760, loss = 12.33 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 02:59:39.367083: step 2770, loss = 12.22 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 02:59:53.656174: step 2780, loss = 12.49 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 03:00:07.951398: step 2790, loss = 12.41 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:00:22.224296: step 2800, loss = 12.27 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 03:00:38.375964: step 2810, loss = 12.33 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:00:52.640672: step 2820, loss = 12.31 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 03:01:06.931848: step 2830, loss = 12.37 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:01:21.202290: step 2840, loss = 12.32 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 03:01:35.522869: step 2850, loss = 12.47 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:01:49.814305: step 2860, loss = 12.43 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:02:04.112189: step 2870, loss = 12.31 (22.3 examples/sec; 1.437 sec/batch)
2017-02-04 03:02:18.400182: step 2880, loss = 12.29 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 03:02:32.668508: step 2890, loss = 12.42 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:02:46.964442: step 2900, loss = 12.32 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 03:03:03.052700: step 2910, loss = 12.31 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:03:17.340105: step 2920, loss = 12.32 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:03:31.613117: step 2930, loss = 12.26 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 03:03:45.889589: step 2940, loss = 12.17 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:04:00.159782: step 2950, loss = 12.18 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:04:14.447746: step 2960, loss = 12.29 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:04:28.754504: step 2970, loss = 12.35 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 03:04:43.032509: step 2980, loss = 12.20 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 03:04:57.317057: step 2990, loss = 12.31 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 03:05:11.591852: step 3000, loss = 12.28 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:05:27.755639: step 3010, loss = 12.25 (22.4 examples/sec; 1.425 sec/batch)
2017-02-04 03:05:42.061995: step 3020, loss = 12.15 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 03:05:56.349067: step 3030, loss = 12.20 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:06:10.645624: step 3040, loss = 12.18 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 03:06:24.954235: step 3050, loss = 12.45 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 03:06:39.246533: step 3060, loss = 12.31 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:06:53.545244: step 3070, loss = 12.30 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 03:07:07.803100: step 3080, loss = 12.33 (22.5 examples/sec; 1.421 sec/batch)
2017-02-04 03:07:22.099756: step 3090, loss = 12.32 (22.3 examples/sec; 1.434 sec/batch)
2017-02-04 03:07:36.392533: step 3100, loss = 12.56 (22.2 examples/sec; 1.440 sec/batch)
2017-02-04 03:07:52.524554: step 3110, loss = 12.29 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:08:06.807059: step 3120, loss = 12.25 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:08:21.102655: step 3130, loss = 12.19 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 03:08:35.384430: step 3140, loss = 12.06 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 03:08:49.656558: step 3150, loss = 12.24 (22.4 examples/sec; 1.429 sec/batch)
2017-02-04 03:09:03.959401: step 3160, loss = 12.23 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 03:09:18.232221: step 3170, loss = 12.39 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:09:32.528220: step 3180, loss = 11.99 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:09:46.794198: step 3190, loss = 12.42 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:10:01.077918: step 3200, loss = 12.26 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 03:10:17.344152: step 3210, loss = 12.05 (22.5 examples/sec; 1.422 sec/batch)
2017-02-04 03:10:31.672422: step 3220, loss = 12.16 (22.2 examples/sec; 1.443 sec/batch)
2017-02-04 03:10:45.966884: step 3230, loss = 12.12 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:11:00.244013: step 3240, loss = 12.30 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 03:11:14.513749: step 3250, loss = 12.26 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:11:28.815529: step 3260, loss = 12.20 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:11:43.082627: step 3270, loss = 12.18 (22.5 examples/sec; 1.419 sec/batch)
2017-02-04 03:11:57.361622: step 3280, loss = 12.09 (22.3 examples/sec; 1.433 sec/batch)
2017-02-04 03:12:11.663589: step 3290, loss = 12.29 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:12:25.941016: step 3300, loss = 12.03 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 03:12:42.176533: step 3310, loss = 12.11 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:12:56.452817: step 3320, loss = 12.21 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:13:10.800073: step 3330, loss = 12.27 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:13:25.089796: step 3340, loss = 12.05 (22.5 examples/sec; 1.424 sec/batch)
2017-02-04 03:13:39.377969: step 3350, loss = 12.17 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 03:13:53.643965: step 3360, loss = 12.09 (22.5 examples/sec; 1.420 sec/batch)
2017-02-04 03:14:07.954285: step 3370, loss = 12.30 (22.2 examples/sec; 1.440 sec/batch)
2017-02-04 03:14:22.256259: step 3380, loss = 12.12 (22.4 examples/sec; 1.430 sec/batch)
2017-02-04 03:14:36.529187: step 3390, loss = 12.10 (22.4 examples/sec; 1.432 sec/batch)
2017-02-04 03:14:50.803027: step 3400, loss = 12.17 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 03:15:07.120164: step 3410, loss = 12.05 (22.3 examples/sec; 1.432 sec/batch)
2017-02-04 03:15:21.410439: step 3420, loss = 12.10 (22.4 examples/sec; 1.427 sec/batch)
2017-02-04 03:15:35.728036: step 3430, loss = 12.10 (22.4 examples/sec; 1.428 sec/batch)
2017-02-04 03:15:49.997775: step 3440, loss = 12.20 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:16:04.274541: step 3450, loss = 12.05 (22.5 examples/sec; 1.425 sec/batch)
2017-02-04 03:16:18.584827: step 3460, loss = 12.17 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:16:32.879535: step 3470, loss = 12.10 (22.4 examples/sec; 1.425 sec/batch)
2017-02-04 03:16:47.161173: step 3480, loss = 12.10 (22.4 examples/sec; 1.426 sec/batch)
2017-02-04 03:17:01.429993: step 3490, loss = 12.12 (22.4 examples/sec; 1.431 sec/batch)
2017-02-04 03:17:15.732289: step 3500, loss = 11.97 (22.3 examples/sec; 1.437 sec/batch)
2017-02-04 03:17:31.890059: step 3510, loss = 12.33 (22.4 examples/sec; 1.426 sec/batch)