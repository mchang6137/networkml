2017-02-03 19:48:28.650695: step 0, loss = 13.17 (2.1 examples/sec; 15.086 sec/batch)
2017-02-03 19:51:10.553847: step 10, loss = 13.52 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 19:51:15.017381: step 20, loss = 15.37 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 19:51:19.510958: step 30, loss = 13.65 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 19:51:23.947214: step 40, loss = 14.44 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:51:28.425753: step 50, loss = 14.06 (69.6 examples/sec; 0.460 sec/batch)
2017-02-03 19:51:32.944720: step 60, loss = 12.74 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 19:51:37.411895: step 70, loss = 14.45 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 19:51:41.861810: step 80, loss = 13.56 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 19:51:46.304035: step 90, loss = 13.41 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 19:51:50.784798: step 100, loss = 13.10 (69.1 examples/sec; 0.463 sec/batch)
2017-02-03 19:51:56.078369: step 110, loss = 13.47 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 19:52:00.544574: step 120, loss = 13.28 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:52:04.982598: step 130, loss = 13.15 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 19:52:09.366109: step 140, loss = 13.44 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 19:52:13.837267: step 150, loss = 13.14 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:52:18.318600: step 160, loss = 13.39 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:52:22.773916: step 170, loss = 12.90 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 19:52:27.216846: step 180, loss = 13.24 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:52:31.647972: step 190, loss = 13.29 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 19:52:36.150254: step 200, loss = 12.89 (70.0 examples/sec; 0.457 sec/batch)
2017-02-03 19:52:41.486195: step 210, loss = 13.11 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 19:52:45.961294: step 220, loss = 13.43 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 19:52:50.376968: step 230, loss = 13.14 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 19:52:54.874190: step 240, loss = 12.89 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:52:59.354493: step 250, loss = 13.00 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:53:03.868647: step 260, loss = 13.22 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 19:53:08.283518: step 270, loss = 13.34 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 19:53:12.746848: step 280, loss = 12.93 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 19:53:17.129365: step 290, loss = 12.97 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 19:53:21.608768: step 300, loss = 12.86 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 19:53:26.883369: step 310, loss = 13.06 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 19:53:31.298013: step 320, loss = 13.08 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 19:53:35.671450: step 330, loss = 13.16 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 19:53:40.125108: step 340, loss = 13.07 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 19:53:44.562291: step 350, loss = 13.17 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 19:53:49.011144: step 360, loss = 12.71 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 19:53:53.485871: step 370, loss = 13.30 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:53:57.955156: step 380, loss = 12.97 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 19:54:02.376217: step 390, loss = 13.31 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 19:54:06.812263: step 400, loss = 13.31 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:54:12.088569: step 410, loss = 13.03 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 19:54:16.533698: step 420, loss = 13.01 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 19:54:21.038157: step 430, loss = 13.29 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 19:54:25.496522: step 440, loss = 12.97 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 19:54:29.938249: step 450, loss = 13.09 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 19:54:34.447003: step 460, loss = 13.01 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 19:54:38.857960: step 470, loss = 12.91 (71.7 examples/sec; 0.447 sec/batch)
2017-02-03 19:54:43.317529: step 480, loss = 12.88 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 19:54:47.764174: step 490, loss = 13.02 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 19:54:52.226400: step 500, loss = 13.22 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 19:54:57.499145: step 510, loss = 13.29 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 19:55:01.987537: step 520, loss = 13.05 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 19:55:06.399578: step 530, loss = 13.07 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 19:55:10.819337: step 540, loss = 12.81 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 19:55:15.258266: step 550, loss = 12.98 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 19:55:19.679831: step 560, loss = 13.06 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 19:55:24.103538: step 570, loss = 12.83 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 19:55:28.521172: step 580, loss = 12.97 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 19:55:33.006116: step 590, loss = 12.99 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:55:37.450628: step 600, loss = 13.27 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 19:55:42.725651: step 610, loss = 12.80 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 19:55:47.161683: step 620, loss = 13.11 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 19:55:51.607225: step 630, loss = 13.17 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 19:55:56.060570: step 640, loss = 13.02 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 19:56:00.502362: step 650, loss = 12.89 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 19:56:04.978816: step 660, loss = 12.98 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 19:56:09.433460: step 670, loss = 12.91 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 19:56:13.859299: step 680, loss = 12.89 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 19:56:18.254658: step 690, loss = 13.01 (75.7 examples/sec; 0.423 sec/batch)
2017-02-03 19:56:22.704522: step 700, loss = 12.81 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 19:56:28.006243: step 710, loss = 13.00 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 19:56:32.478482: step 720, loss = 13.25 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 19:56:36.872465: step 730, loss = 12.97 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 19:56:41.328676: step 740, loss = 13.02 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 19:56:45.710552: step 750, loss = 13.10 (74.7 examples/sec; 0.429 sec/batch)
2017-02-03 19:56:50.151310: step 760, loss = 13.03 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 19:56:54.629237: step 770, loss = 13.06 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 19:56:59.086573: step 780, loss = 12.90 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 19:57:03.556517: step 790, loss = 12.80 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 19:57:07.991051: step 800, loss = 13.10 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 19:57:13.277729: step 810, loss = 12.96 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 19:57:17.718386: step 820, loss = 12.95 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 19:57:22.136359: step 830, loss = 12.94 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 19:57:26.589794: step 840, loss = 13.06 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 19:57:30.995499: step 850, loss = 12.86 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 19:57:35.399884: step 860, loss = 12.90 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 19:57:39.841397: step 870, loss = 12.99 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 19:57:44.246201: step 880, loss = 12.73 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 19:57:48.662566: step 890, loss = 13.06 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 19:57:53.053030: step 900, loss = 12.95 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 19:57:58.367460: step 910, loss = 12.89 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 19:58:02.823135: step 920, loss = 12.76 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 19:58:07.271524: step 930, loss = 13.05 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 19:58:11.708935: step 940, loss = 12.63 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 19:58:16.140312: step 950, loss = 12.81 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 19:58:20.566888: step 960, loss = 12.73 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 19:58:25.056286: step 970, loss = 12.94 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 19:58:29.525699: step 980, loss = 12.86 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 19:58:33.966260: step 990, loss = 12.80 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 19:58:38.403097: step 1000, loss = 12.86 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 19:58:43.643139: step 1010, loss = 12.86 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 19:58:48.079359: step 1020, loss = 12.93 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 19:58:52.501664: step 1030, loss = 12.89 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 19:58:56.939988: step 1040, loss = 13.03 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 19:59:01.359441: step 1050, loss = 12.79 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 19:59:05.811408: step 1060, loss = 12.90 (70.4 examples/sec; 0.455 sec/batch)
2017-02-03 19:59:10.255008: step 1070, loss = 12.94 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 19:59:14.710359: step 1080, loss = 13.02 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 19:59:19.096789: step 1090, loss = 12.91 (75.3 examples/sec; 0.425 sec/batch)
2017-02-03 19:59:23.502190: step 1100, loss = 12.82 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 19:59:28.718164: step 1110, loss = 12.61 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 19:59:33.189802: step 1120, loss = 12.89 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 19:59:37.557035: step 1130, loss = 12.84 (74.7 examples/sec; 0.428 sec/batch)
2017-02-03 19:59:41.978585: step 1140, loss = 12.83 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 19:59:46.391140: step 1150, loss = 12.71 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 19:59:50.845625: step 1160, loss = 12.71 (70.4 examples/sec; 0.455 sec/batch)
2017-02-03 19:59:55.249892: step 1170, loss = 12.93 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 19:59:59.697653: step 1180, loss = 12.80 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:00:04.188863: step 1190, loss = 12.82 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:00:08.705450: step 1200, loss = 12.65 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:00:13.947813: step 1210, loss = 12.95 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:00:18.425377: step 1220, loss = 12.87 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:00:22.875485: step 1230, loss = 12.91 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:00:27.323132: step 1240, loss = 12.77 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:00:31.778666: step 1250, loss = 12.85 (72.5 examples/sec; 0.442 sec/batch)
2017-02-03 20:00:36.208789: step 1260, loss = 12.88 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:00:40.670833: step 1270, loss = 13.17 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:00:45.112759: step 1280, loss = 12.78 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 20:00:49.629829: step 1290, loss = 12.79 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:00:54.036488: step 1300, loss = 12.81 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:00:59.328001: step 1310, loss = 12.76 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:01:03.797402: step 1320, loss = 12.71 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:01:08.273142: step 1330, loss = 12.64 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:01:12.781663: step 1340, loss = 12.64 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:01:17.198960: step 1350, loss = 12.65 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:01:21.658736: step 1360, loss = 12.62 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:01:26.126011: step 1370, loss = 12.69 (69.8 examples/sec; 0.459 sec/batch)
2017-02-03 20:01:30.550332: step 1380, loss = 12.59 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:01:34.986504: step 1390, loss = 12.88 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:01:39.398109: step 1400, loss = 12.65 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:01:44.708764: step 1410, loss = 12.90 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:01:49.141243: step 1420, loss = 12.67 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:01:53.606742: step 1430, loss = 12.86 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:01:57.969372: step 1440, loss = 12.80 (76.1 examples/sec; 0.421 sec/batch)
2017-02-03 20:02:02.427593: step 1450, loss = 12.72 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:02:06.848222: step 1460, loss = 12.80 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:02:11.297014: step 1470, loss = 12.69 (68.7 examples/sec; 0.466 sec/batch)
2017-02-03 20:02:15.703562: step 1480, loss = 12.54 (74.7 examples/sec; 0.429 sec/batch)
2017-02-03 20:02:20.118337: step 1490, loss = 12.50 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:02:24.578847: step 1500, loss = 12.56 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:02:29.868856: step 1510, loss = 12.63 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:02:34.297175: step 1520, loss = 12.66 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:02:38.711080: step 1530, loss = 12.59 (74.9 examples/sec; 0.427 sec/batch)
2017-02-03 20:02:43.111684: step 1540, loss = 12.62 (71.8 examples/sec; 0.445 sec/batch)
2017-02-03 20:02:47.526248: step 1550, loss = 12.57 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 20:02:51.937595: step 1560, loss = 12.86 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:02:56.409232: step 1570, loss = 12.76 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:03:00.773513: step 1580, loss = 12.98 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:03:05.211132: step 1590, loss = 12.85 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:03:09.629768: step 1600, loss = 12.67 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:03:15.478836: step 1610, loss = 12.91 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:03:19.939678: step 1620, loss = 12.84 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:03:24.376542: step 1630, loss = 12.34 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:03:28.838670: step 1640, loss = 12.75 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:03:33.258528: step 1650, loss = 12.62 (74.1 examples/sec; 0.432 sec/batch)
2017-02-03 20:03:37.740598: step 1660, loss = 12.52 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:03:42.115172: step 1670, loss = 12.54 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:03:46.568309: step 1680, loss = 12.74 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:03:51.011364: step 1690, loss = 12.83 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:03:55.513190: step 1700, loss = 12.72 (64.4 examples/sec; 0.497 sec/batch)
2017-02-03 20:04:00.788883: step 1710, loss = 12.69 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:04:05.183924: step 1720, loss = 12.61 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:04:09.636403: step 1730, loss = 12.56 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:04:14.071514: step 1740, loss = 12.79 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:04:18.468890: step 1750, loss = 12.62 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:04:22.905187: step 1760, loss = 12.72 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:04:27.370824: step 1770, loss = 12.59 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:04:31.771281: step 1780, loss = 12.53 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:04:36.210901: step 1790, loss = 12.74 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:04:40.650344: step 1800, loss = 12.72 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:04:45.928841: step 1810, loss = 12.50 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:04:50.403341: step 1820, loss = 12.58 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:04:54.840310: step 1830, loss = 12.68 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:04:59.238031: step 1840, loss = 12.70 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:05:03.651824: step 1850, loss = 12.54 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:05:08.075223: step 1860, loss = 12.86 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:05:12.588288: step 1870, loss = 12.83 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:05:16.992138: step 1880, loss = 12.64 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:05:21.398496: step 1890, loss = 12.41 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:05:25.827137: step 1900, loss = 12.53 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:05:31.053853: step 1910, loss = 12.71 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:05:35.479888: step 1920, loss = 12.55 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:05:39.909290: step 1930, loss = 12.59 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:05:44.345474: step 1940, loss = 12.49 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:05:48.764912: step 1950, loss = 12.43 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:05:53.206600: step 1960, loss = 12.46 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:05:57.621624: step 1970, loss = 12.65 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:06:02.056075: step 1980, loss = 12.65 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:06:06.455211: step 1990, loss = 12.60 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:06:10.867580: step 2000, loss = 12.60 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:06:16.146780: step 2010, loss = 12.68 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:06:20.580521: step 2020, loss = 12.54 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:06:24.970010: step 2030, loss = 12.75 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:06:29.412438: step 2040, loss = 12.58 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:06:33.843271: step 2050, loss = 12.53 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:06:38.278940: step 2060, loss = 12.77 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:06:42.700101: step 2070, loss = 12.47 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:06:47.119390: step 2080, loss = 12.58 (74.2 examples/sec; 0.432 sec/batch)
2017-02-03 20:06:51.518969: step 2090, loss = 12.71 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:06:55.949502: step 2100, loss = 12.51 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:07:01.223935: step 2110, loss = 12.60 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 20:07:05.623304: step 2120, loss = 12.70 (70.6 examples/sec; 0.454 sec/batch)
2017-02-03 20:07:10.061480: step 2130, loss = 12.62 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:07:14.535783: step 2140, loss = 12.54 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:07:19.013387: step 2150, loss = 12.74 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:07:23.419211: step 2160, loss = 12.70 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:07:27.817735: step 2170, loss = 12.68 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:07:32.268039: step 2180, loss = 12.49 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:07:36.728713: step 2190, loss = 12.55 (71.7 examples/sec; 0.447 sec/batch)
2017-02-03 20:07:41.206201: step 2200, loss = 12.58 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:07:46.459942: step 2210, loss = 12.50 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:07:50.882343: step 2220, loss = 12.30 (75.6 examples/sec; 0.423 sec/batch)
2017-02-03 20:07:55.351606: step 2230, loss = 12.41 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:07:59.747090: step 2240, loss = 12.63 (75.9 examples/sec; 0.422 sec/batch)
2017-02-03 20:08:04.164411: step 2250, loss = 12.38 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:08:08.581496: step 2260, loss = 12.54 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:08:13.041878: step 2270, loss = 12.49 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:08:17.490934: step 2280, loss = 12.49 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:08:21.907784: step 2290, loss = 12.51 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:08:26.376270: step 2300, loss = 12.64 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:08:31.654992: step 2310, loss = 12.55 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:08:36.115867: step 2320, loss = 12.56 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:08:40.517089: step 2330, loss = 12.46 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:08:44.926958: step 2340, loss = 12.43 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:08:49.386345: step 2350, loss = 12.35 (72.5 examples/sec; 0.442 sec/batch)
2017-02-03 20:08:53.822724: step 2360, loss = 12.35 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:08:58.247975: step 2370, loss = 12.55 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 20:09:02.658489: step 2380, loss = 12.67 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:09:07.146540: step 2390, loss = 12.51 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:09:11.565102: step 2400, loss = 12.57 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:09:16.785751: step 2410, loss = 12.36 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:09:21.286539: step 2420, loss = 12.34 (71.7 examples/sec; 0.447 sec/batch)
2017-02-03 20:09:25.738949: step 2430, loss = 12.56 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:09:30.162142: step 2440, loss = 12.56 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:09:34.542338: step 2450, loss = 12.56 (75.6 examples/sec; 0.424 sec/batch)
2017-02-03 20:09:38.994412: step 2460, loss = 12.49 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:09:43.403158: step 2470, loss = 12.50 (74.5 examples/sec; 0.430 sec/batch)
2017-02-03 20:09:47.801524: step 2480, loss = 12.33 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:09:52.241385: step 2490, loss = 12.67 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:09:56.619454: step 2500, loss = 12.52 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:10:01.817269: step 2510, loss = 12.50 (74.9 examples/sec; 0.427 sec/batch)
2017-02-03 20:10:06.261271: step 2520, loss = 12.41 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:10:10.655088: step 2530, loss = 12.48 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:10:15.078257: step 2540, loss = 12.44 (71.8 examples/sec; 0.445 sec/batch)
2017-02-03 20:10:19.533290: step 2550, loss = 12.41 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:10:23.915118: step 2560, loss = 12.56 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:10:28.330941: step 2570, loss = 12.56 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:10:32.734738: step 2580, loss = 12.58 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:10:37.163580: step 2590, loss = 12.36 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:10:41.608516: step 2600, loss = 12.37 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:10:46.836307: step 2610, loss = 12.37 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:10:51.237861: step 2620, loss = 12.69 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:10:55.644190: step 2630, loss = 12.53 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:11:00.054789: step 2640, loss = 12.67 (73.5 examples/sec; 0.436 sec/batch)
2017-02-03 20:11:04.521222: step 2650, loss = 12.37 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 20:11:08.970133: step 2660, loss = 12.56 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:11:13.411501: step 2670, loss = 12.52 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:11:17.820441: step 2680, loss = 12.38 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:11:22.263000: step 2690, loss = 12.59 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:11:26.705918: step 2700, loss = 12.45 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:11:31.867820: step 2710, loss = 12.48 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:11:36.317448: step 2720, loss = 12.54 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:11:40.676461: step 2730, loss = 12.55 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:11:45.099068: step 2740, loss = 12.21 (73.1 examples/sec; 0.437 sec/batch)
2017-02-03 20:11:49.546954: step 2750, loss = 12.55 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:11:53.986083: step 2760, loss = 12.46 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:11:58.438407: step 2770, loss = 12.37 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:12:02.806352: step 2780, loss = 12.60 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:12:07.240109: step 2790, loss = 12.47 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:12:11.629921: step 2800, loss = 12.40 (70.4 examples/sec; 0.455 sec/batch)
2017-02-03 20:12:16.889957: step 2810, loss = 12.38 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:12:21.318857: step 2820, loss = 12.68 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:12:25.676921: step 2830, loss = 12.47 (74.5 examples/sec; 0.430 sec/batch)
2017-02-03 20:12:30.122867: step 2840, loss = 12.49 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:12:34.557651: step 2850, loss = 12.45 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:12:38.980775: step 2860, loss = 12.28 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:12:43.430203: step 2870, loss = 12.32 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:12:47.896660: step 2880, loss = 12.38 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 20:12:52.290699: step 2890, loss = 12.35 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:12:56.707055: step 2900, loss = 12.43 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:13:01.955750: step 2910, loss = 11.90 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:13:06.331511: step 2920, loss = 12.70 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:13:10.733253: step 2930, loss = 12.38 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:13:15.174094: step 2940, loss = 12.50 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:13:19.641035: step 2950, loss = 12.09 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:13:24.062200: step 2960, loss = 12.53 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:13:28.500428: step 2970, loss = 12.30 (70.1 examples/sec; 0.457 sec/batch)
2017-02-03 20:13:32.984948: step 2980, loss = 12.44 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:13:37.440342: step 2990, loss = 12.25 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:13:41.816715: step 3000, loss = 11.99 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:13:47.084229: step 3010, loss = 12.12 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:13:51.551413: step 3020, loss = 12.23 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:13:56.029615: step 3030, loss = 12.38 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:14:00.448377: step 3040, loss = 12.56 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:14:04.870126: step 3050, loss = 12.46 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:14:09.288505: step 3060, loss = 12.01 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:14:13.708031: step 3070, loss = 12.60 (70.6 examples/sec; 0.454 sec/batch)
2017-02-03 20:14:18.139366: step 3080, loss = 12.10 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:14:22.554807: step 3090, loss = 12.51 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:14:26.989412: step 3100, loss = 12.41 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:14:32.252968: step 3110, loss = 12.08 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:14:36.728188: step 3120, loss = 12.22 (69.7 examples/sec; 0.459 sec/batch)
2017-02-03 20:14:41.174630: step 3130, loss = 12.34 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:14:45.619924: step 3140, loss = 12.19 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 20:14:50.060076: step 3150, loss = 12.35 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:14:54.519493: step 3160, loss = 12.26 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:14:58.964607: step 3170, loss = 12.31 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:15:03.370156: step 3180, loss = 12.12 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:15:07.823033: step 3190, loss = 12.33 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:15:12.232333: step 3200, loss = 12.31 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:15:17.494241: step 3210, loss = 12.28 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:15:21.961055: step 3220, loss = 12.39 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:15:26.432754: step 3230, loss = 12.20 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:15:30.872984: step 3240, loss = 12.26 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:15:35.269634: step 3250, loss = 12.26 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:15:39.696024: step 3260, loss = 12.22 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:15:44.143989: step 3270, loss = 12.27 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:15:48.633962: step 3280, loss = 12.63 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:15:53.049040: step 3290, loss = 12.31 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:15:57.474012: step 3300, loss = 12.29 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:16:02.716602: step 3310, loss = 12.23 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:16:07.123129: step 3320, loss = 12.33 (75.5 examples/sec; 0.424 sec/batch)
2017-02-03 20:16:11.562469: step 3330, loss = 11.99 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:16:16.013751: step 3340, loss = 12.26 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:16:20.376022: step 3350, loss = 12.62 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:16:24.785215: step 3360, loss = 12.18 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:16:29.233265: step 3370, loss = 12.40 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:16:33.661865: step 3380, loss = 12.18 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:16:38.077249: step 3390, loss = 12.30 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:16:42.529968: step 3400, loss = 12.25 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:16:47.738920: step 3410, loss = 12.53 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:16:52.125863: step 3420, loss = 12.29 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:16:56.520145: step 3430, loss = 12.32 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:17:00.949281: step 3440, loss = 12.35 (70.0 examples/sec; 0.457 sec/batch)
2017-02-03 20:17:05.370930: step 3450, loss = 12.20 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:17:09.755055: step 3460, loss = 12.42 (74.9 examples/sec; 0.427 sec/batch)
2017-02-03 20:17:14.159885: step 3470, loss = 12.27 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:17:18.606614: step 3480, loss = 12.17 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:17:23.034120: step 3490, loss = 12.47 (75.3 examples/sec; 0.425 sec/batch)
2017-02-03 20:17:27.520073: step 3500, loss = 12.22 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:17:32.798847: step 3510, loss = 12.31 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:17:37.213786: step 3520, loss = 12.05 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:17:41.604772: step 3530, loss = 12.03 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:17:46.048806: step 3540, loss = 12.39 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:17:50.474589: step 3550, loss = 12.31 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:17:54.880663: step 3560, loss = 12.36 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:17:59.287039: step 3570, loss = 12.04 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:18:03.746330: step 3580, loss = 12.13 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:18:08.182322: step 3590, loss = 12.38 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:18:12.652116: step 3600, loss = 12.20 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:18:18.008367: step 3610, loss = 12.35 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:18:22.452922: step 3620, loss = 12.31 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:18:26.874940: step 3630, loss = 12.33 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:18:31.323829: step 3640, loss = 12.26 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:18:35.735149: step 3650, loss = 12.22 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:18:40.182605: step 3660, loss = 12.05 (74.5 examples/sec; 0.430 sec/batch)
2017-02-03 20:18:44.607701: step 3670, loss = 12.42 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:18:49.043839: step 3680, loss = 12.12 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:18:53.510816: step 3690, loss = 12.09 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:18:57.935162: step 3700, loss = 12.26 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:19:03.211107: step 3710, loss = 12.28 (70.4 examples/sec; 0.455 sec/batch)
2017-02-03 20:19:07.661858: step 3720, loss = 12.19 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:19:12.045151: step 3730, loss = 12.57 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:19:16.468778: step 3740, loss = 12.30 (74.9 examples/sec; 0.427 sec/batch)
2017-02-03 20:19:20.918403: step 3750, loss = 12.21 (70.1 examples/sec; 0.457 sec/batch)
2017-02-03 20:19:25.317936: step 3760, loss = 11.92 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:19:29.768599: step 3770, loss = 12.21 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:19:34.184126: step 3780, loss = 12.38 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:19:38.602981: step 3790, loss = 12.21 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:19:43.075036: step 3800, loss = 12.02 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:19:48.280541: step 3810, loss = 11.88 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:19:52.722013: step 3820, loss = 12.32 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:19:57.154685: step 3830, loss = 12.27 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:20:01.550299: step 3840, loss = 12.06 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:20:05.994828: step 3850, loss = 11.86 (69.8 examples/sec; 0.458 sec/batch)
2017-02-03 20:20:10.383973: step 3860, loss = 12.23 (72.3 examples/sec; 0.442 sec/batch)
2017-02-03 20:20:14.815243: step 3870, loss = 12.13 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:20:19.219855: step 3880, loss = 12.44 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:20:23.594680: step 3890, loss = 12.31 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 20:20:28.018475: step 3900, loss = 12.24 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:20:33.247539: step 3910, loss = 11.96 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:20:37.673629: step 3920, loss = 12.27 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:20:42.072790: step 3930, loss = 12.10 (75.6 examples/sec; 0.423 sec/batch)
2017-02-03 20:20:46.513980: step 3940, loss = 12.13 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:20:50.933811: step 3950, loss = 12.13 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 20:20:55.399241: step 3960, loss = 12.20 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:20:59.818438: step 3970, loss = 12.31 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 20:21:04.295244: step 3980, loss = 12.08 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:21:08.695144: step 3990, loss = 12.24 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:21:13.123972: step 4000, loss = 11.84 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:21:18.397526: step 4010, loss = 12.20 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:21:22.820163: step 4020, loss = 11.99 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:21:27.239675: step 4030, loss = 11.99 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:21:31.669253: step 4040, loss = 12.06 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 20:21:36.057677: step 4050, loss = 12.19 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:21:40.485017: step 4060, loss = 11.55 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:21:44.886442: step 4070, loss = 12.29 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:21:49.311351: step 4080, loss = 12.07 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:21:53.776325: step 4090, loss = 12.08 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:21:58.234900: step 4100, loss = 12.05 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:22:03.465295: step 4110, loss = 12.39 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:22:07.925216: step 4120, loss = 12.16 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:22:12.347423: step 4130, loss = 12.16 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:22:16.771636: step 4140, loss = 12.15 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:22:21.250034: step 4150, loss = 12.32 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:22:25.725133: step 4160, loss = 12.34 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:22:30.152412: step 4170, loss = 12.02 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:22:34.588738: step 4180, loss = 12.03 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:22:39.010926: step 4190, loss = 12.13 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:22:43.439696: step 4200, loss = 11.77 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 20:22:48.692632: step 4210, loss = 11.91 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:22:53.132948: step 4220, loss = 12.24 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:22:57.518736: step 4230, loss = 12.19 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:23:01.883045: step 4240, loss = 11.86 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:23:06.325767: step 4250, loss = 12.25 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:23:10.741793: step 4260, loss = 12.09 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:23:15.180435: step 4270, loss = 11.98 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:23:19.623985: step 4280, loss = 12.19 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:23:24.095112: step 4290, loss = 12.19 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:23:28.543903: step 4300, loss = 12.30 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:23:33.869884: step 4310, loss = 12.27 (70.1 examples/sec; 0.457 sec/batch)
2017-02-03 20:23:38.307234: step 4320, loss = 11.60 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:23:42.772580: step 4330, loss = 11.90 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:23:47.240664: step 4340, loss = 11.91 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:23:51.715782: step 4350, loss = 12.12 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:23:56.169591: step 4360, loss = 11.85 (74.7 examples/sec; 0.428 sec/batch)
2017-02-03 20:24:00.600044: step 4370, loss = 11.87 (70.3 examples/sec; 0.456 sec/batch)
2017-02-03 20:24:04.998819: step 4380, loss = 11.85 (75.8 examples/sec; 0.422 sec/batch)
2017-02-03 20:24:09.431965: step 4390, loss = 12.45 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 20:24:13.856902: step 4400, loss = 11.88 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:24:19.176846: step 4410, loss = 12.16 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:24:23.608325: step 4420, loss = 11.96 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:24:28.058387: step 4430, loss = 11.92 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:24:32.534466: step 4440, loss = 11.88 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:24:36.980864: step 4450, loss = 12.12 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:24:41.388671: step 4460, loss = 12.20 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:24:45.800392: step 4470, loss = 12.11 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:24:50.235162: step 4480, loss = 12.49 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:24:54.697190: step 4490, loss = 12.47 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:24:59.138238: step 4500, loss = 11.88 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:25:04.407891: step 4510, loss = 11.91 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:25:08.887921: step 4520, loss = 12.10 (69.9 examples/sec; 0.458 sec/batch)
2017-02-03 20:25:13.328716: step 4530, loss = 12.03 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:25:17.713231: step 4540, loss = 12.16 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:25:22.167729: step 4550, loss = 11.68 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:25:26.619607: step 4560, loss = 11.90 (71.2 examples/sec; 0.450 sec/batch)
2017-02-03 20:25:31.156144: step 4570, loss = 12.25 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:25:35.610098: step 4580, loss = 12.04 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:25:40.037627: step 4590, loss = 12.00 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:25:44.494993: step 4600, loss = 11.59 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:25:49.765214: step 4610, loss = 12.12 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:25:54.163510: step 4620, loss = 11.97 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:25:58.601410: step 4630, loss = 11.99 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:26:03.057822: step 4640, loss = 12.22 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:26:07.474298: step 4650, loss = 12.14 (69.8 examples/sec; 0.459 sec/batch)
2017-02-03 20:26:11.911695: step 4660, loss = 11.59 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:26:16.353114: step 4670, loss = 12.09 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:26:20.774047: step 4680, loss = 12.06 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:26:25.197963: step 4690, loss = 12.32 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:26:29.624917: step 4700, loss = 12.04 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:26:34.849995: step 4710, loss = 11.58 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:26:39.294921: step 4720, loss = 11.92 (74.7 examples/sec; 0.428 sec/batch)
2017-02-03 20:26:43.726625: step 4730, loss = 12.12 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:26:48.165290: step 4740, loss = 11.84 (76.6 examples/sec; 0.418 sec/batch)
2017-02-03 20:26:52.595567: step 4750, loss = 12.03 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:26:57.008895: step 4760, loss = 12.17 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:27:01.405954: step 4770, loss = 11.95 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:27:05.832743: step 4780, loss = 12.15 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:27:10.223381: step 4790, loss = 11.91 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:27:14.685725: step 4800, loss = 11.72 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:27:19.932258: step 4810, loss = 12.32 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:27:24.377097: step 4820, loss = 12.20 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:27:28.811096: step 4830, loss = 11.65 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:27:33.273831: step 4840, loss = 11.95 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:27:37.750895: step 4850, loss = 11.90 (72.3 examples/sec; 0.442 sec/batch)
2017-02-03 20:27:42.195977: step 4860, loss = 11.87 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:27:46.589315: step 4870, loss = 11.98 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:27:51.010716: step 4880, loss = 11.57 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:27:55.470224: step 4890, loss = 11.64 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:27:59.968095: step 4900, loss = 11.90 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:28:05.206337: step 4910, loss = 11.71 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:28:09.626306: step 4920, loss = 12.26 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:28:14.029370: step 4930, loss = 11.20 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:28:18.475456: step 4940, loss = 12.16 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:28:22.931834: step 4950, loss = 11.93 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:28:27.381068: step 4960, loss = 11.70 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:28:31.813763: step 4970, loss = 11.82 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:28:36.193245: step 4980, loss = 12.32 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:28:40.602074: step 4990, loss = 12.16 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:28:45.047017: step 5000, loss = 12.11 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:30:59.668492: step 5010, loss = 11.98 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:31:04.110351: step 5020, loss = 11.81 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:31:08.567893: step 5030, loss = 11.67 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:31:13.018642: step 5040, loss = 12.05 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:31:17.462821: step 5050, loss = 12.08 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:31:21.883000: step 5060, loss = 12.07 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:31:26.336684: step 5070, loss = 12.06 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:31:30.775550: step 5080, loss = 12.16 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:31:35.193564: step 5090, loss = 12.10 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:31:39.606965: step 5100, loss = 12.12 (75.1 examples/sec; 0.426 sec/batch)
2017-02-03 20:31:44.877649: step 5110, loss = 11.99 (70.9 examples/sec; 0.452 sec/batch)
2017-02-03 20:31:49.289593: step 5120, loss = 11.75 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:31:53.746586: step 5130, loss = 11.83 (70.0 examples/sec; 0.457 sec/batch)
2017-02-03 20:31:58.169364: step 5140, loss = 11.96 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:32:02.586644: step 5150, loss = 12.18 (74.0 examples/sec; 0.433 sec/batch)
2017-02-03 20:32:07.025473: step 5160, loss = 11.97 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:32:11.438148: step 5170, loss = 12.06 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:32:15.869385: step 5180, loss = 12.08 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:32:20.271368: step 5190, loss = 11.85 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:32:24.671308: step 5200, loss = 12.07 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:32:29.906745: step 5210, loss = 11.91 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:32:34.303662: step 5220, loss = 11.50 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:32:38.761879: step 5230, loss = 12.37 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:32:43.206011: step 5240, loss = 11.88 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:32:47.619605: step 5250, loss = 12.04 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:32:52.066959: step 5260, loss = 11.94 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:32:56.531231: step 5270, loss = 11.77 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:33:00.959147: step 5280, loss = 12.13 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:33:05.393514: step 5290, loss = 11.64 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:33:09.855618: step 5300, loss = 11.60 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:33:15.088783: step 5310, loss = 12.21 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:33:19.446684: step 5320, loss = 11.90 (75.2 examples/sec; 0.425 sec/batch)
2017-02-03 20:33:23.871167: step 5330, loss = 11.68 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:33:28.280870: step 5340, loss = 11.11 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:33:32.680952: step 5350, loss = 11.97 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:33:37.078954: step 5360, loss = 11.75 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:33:41.537106: step 5370, loss = 11.26 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:33:45.970460: step 5380, loss = 11.56 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:33:50.443913: step 5390, loss = 11.86 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:33:54.837793: step 5400, loss = 11.87 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 20:34:00.128573: step 5410, loss = 11.75 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:34:04.562814: step 5420, loss = 11.98 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:34:09.040317: step 5430, loss = 12.22 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:34:13.518707: step 5440, loss = 12.00 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:34:17.970723: step 5450, loss = 11.35 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:34:22.453841: step 5460, loss = 11.96 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:34:26.867265: step 5470, loss = 11.99 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:34:31.265772: step 5480, loss = 11.97 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:34:35.748052: step 5490, loss = 12.01 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:34:40.197372: step 5500, loss = 11.95 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:34:45.515436: step 5510, loss = 12.18 (69.7 examples/sec; 0.459 sec/batch)
2017-02-03 20:34:50.002307: step 5520, loss = 12.08 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:34:54.399040: step 5530, loss = 11.82 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:34:58.824473: step 5540, loss = 11.90 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 20:35:03.242235: step 5550, loss = 11.69 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:35:07.674924: step 5560, loss = 11.67 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:35:12.126340: step 5570, loss = 11.86 (70.1 examples/sec; 0.456 sec/batch)
2017-02-03 20:35:16.567294: step 5580, loss = 11.64 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:35:20.973312: step 5590, loss = 11.82 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:35:25.381825: step 5600, loss = 11.37 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:35:30.633560: step 5610, loss = 11.73 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:35:35.045297: step 5620, loss = 11.55 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:35:39.490732: step 5630, loss = 11.68 (75.0 examples/sec; 0.427 sec/batch)
2017-02-03 20:35:43.952272: step 5640, loss = 11.47 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:35:48.372786: step 5650, loss = 11.88 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:35:52.841479: step 5660, loss = 11.82 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:35:57.285840: step 5670, loss = 11.92 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:36:01.768235: step 5680, loss = 12.41 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:36:06.211801: step 5690, loss = 11.68 (75.5 examples/sec; 0.424 sec/batch)
2017-02-03 20:36:10.629263: step 5700, loss = 12.02 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:36:15.848024: step 5710, loss = 11.78 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:36:20.304517: step 5720, loss = 11.86 (69.5 examples/sec; 0.461 sec/batch)
2017-02-03 20:36:24.754421: step 5730, loss = 11.88 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:36:29.187603: step 5740, loss = 11.76 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:36:33.621346: step 5750, loss = 11.49 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:36:38.114266: step 5760, loss = 11.81 (69.7 examples/sec; 0.459 sec/batch)
2017-02-03 20:36:42.530258: step 5770, loss = 11.71 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:36:46.939342: step 5780, loss = 11.69 (69.7 examples/sec; 0.459 sec/batch)
2017-02-03 20:36:51.336908: step 5790, loss = 11.77 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:36:55.787942: step 5800, loss = 11.49 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:37:01.058879: step 5810, loss = 11.67 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:37:05.523242: step 5820, loss = 11.80 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:37:09.953983: step 5830, loss = 12.06 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:37:14.417276: step 5840, loss = 11.65 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:37:18.913356: step 5850, loss = 11.80 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:37:23.380785: step 5860, loss = 11.70 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:37:27.841503: step 5870, loss = 12.03 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:37:32.249477: step 5880, loss = 11.91 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:37:36.737991: step 5890, loss = 11.58 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:37:41.195950: step 5900, loss = 11.92 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:37:46.488921: step 5910, loss = 11.78 (69.5 examples/sec; 0.460 sec/batch)
2017-02-03 20:37:50.901553: step 5920, loss = 11.64 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:37:55.317662: step 5930, loss = 11.87 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:37:59.730559: step 5940, loss = 11.95 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:38:04.122767: step 5950, loss = 11.89 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:38:08.577528: step 5960, loss = 11.97 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:38:13.041467: step 5970, loss = 11.80 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:38:17.494560: step 5980, loss = 11.86 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:38:21.888265: step 5990, loss = 11.88 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:38:26.253912: step 6000, loss = 11.53 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:38:32.148806: step 6010, loss = 11.57 (72.5 examples/sec; 0.442 sec/batch)
2017-02-03 20:38:36.545812: step 6020, loss = 11.94 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:38:40.964075: step 6030, loss = 11.88 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:38:45.349350: step 6040, loss = 11.93 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:38:49.830530: step 6050, loss = 11.70 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:38:54.280632: step 6060, loss = 11.85 (74.0 examples/sec; 0.433 sec/batch)
2017-02-03 20:38:58.683527: step 6070, loss = 11.85 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:39:03.050004: step 6080, loss = 11.71 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:39:07.483115: step 6090, loss = 11.44 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:39:11.877479: step 6100, loss = 11.51 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:39:17.141031: step 6110, loss = 11.70 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:39:21.550269: step 6120, loss = 11.57 (73.8 examples/sec; 0.433 sec/batch)
2017-02-03 20:39:25.972589: step 6130, loss = 11.66 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:39:30.350209: step 6140, loss = 11.39 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:39:34.769987: step 6150, loss = 11.89 (74.0 examples/sec; 0.432 sec/batch)
2017-02-03 20:39:39.248717: step 6160, loss = 11.15 (71.0 examples/sec; 0.450 sec/batch)
2017-02-03 20:39:43.747756: step 6170, loss = 11.88 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:39:48.199519: step 6180, loss = 11.69 (70.4 examples/sec; 0.455 sec/batch)
2017-02-03 20:39:52.618391: step 6190, loss = 11.37 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:39:57.039487: step 6200, loss = 11.63 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:40:02.328531: step 6210, loss = 11.85 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:40:06.716279: step 6220, loss = 11.86 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:40:11.113441: step 6230, loss = 11.64 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:40:15.543891: step 6240, loss = 11.60 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:40:20.005235: step 6250, loss = 11.60 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:40:24.442019: step 6260, loss = 11.65 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:40:28.909742: step 6270, loss = 11.67 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:40:33.316965: step 6280, loss = 12.02 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:40:37.756922: step 6290, loss = 11.45 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:40:42.202142: step 6300, loss = 11.79 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:40:47.466970: step 6310, loss = 11.65 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:40:51.900572: step 6320, loss = 11.89 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:40:56.305370: step 6330, loss = 11.74 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:41:00.717903: step 6340, loss = 11.75 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:41:05.221880: step 6350, loss = 11.92 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:41:09.700932: step 6360, loss = 11.71 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:41:14.113817: step 6370, loss = 11.56 (75.1 examples/sec; 0.426 sec/batch)
2017-02-03 20:41:18.545402: step 6380, loss = 11.48 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:41:22.926726: step 6390, loss = 11.43 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:41:27.380413: step 6400, loss = 11.48 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:41:32.676805: step 6410, loss = 11.65 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:41:37.120022: step 6420, loss = 11.92 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:41:41.486328: step 6430, loss = 11.54 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:41:45.964030: step 6440, loss = 12.05 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:41:50.385573: step 6450, loss = 11.43 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:41:54.786518: step 6460, loss = 11.84 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:41:59.168157: step 6470, loss = 11.75 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:42:03.605433: step 6480, loss = 11.54 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:42:07.994952: step 6490, loss = 11.68 (74.7 examples/sec; 0.428 sec/batch)
2017-02-03 20:42:12.449454: step 6500, loss = 11.77 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:42:17.722420: step 6510, loss = 11.41 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:42:22.184257: step 6520, loss = 11.62 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:42:26.618678: step 6530, loss = 11.51 (74.7 examples/sec; 0.428 sec/batch)
2017-02-03 20:42:31.048404: step 6540, loss = 11.47 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:42:35.450465: step 6550, loss = 11.46 (75.9 examples/sec; 0.422 sec/batch)
2017-02-03 20:42:39.865143: step 6560, loss = 11.40 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:42:44.263786: step 6570, loss = 11.58 (76.3 examples/sec; 0.419 sec/batch)
2017-02-03 20:42:48.724007: step 6580, loss = 11.13 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:42:53.134711: step 6590, loss = 11.55 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:42:57.569886: step 6600, loss = 11.34 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:43:02.826483: step 6610, loss = 11.54 (74.3 examples/sec; 0.431 sec/batch)
2017-02-03 20:43:07.252409: step 6620, loss = 11.81 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:43:11.711897: step 6630, loss = 11.74 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:43:16.162522: step 6640, loss = 11.59 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:43:20.590843: step 6650, loss = 11.31 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:43:24.977833: step 6660, loss = 11.67 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:43:29.435812: step 6670, loss = 11.76 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:43:33.846933: step 6680, loss = 12.30 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:43:38.323262: step 6690, loss = 11.22 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:43:42.770503: step 6700, loss = 11.98 (70.1 examples/sec; 0.457 sec/batch)
2017-02-03 20:43:48.076710: step 6710, loss = 11.45 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:43:52.546006: step 6720, loss = 12.00 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:43:56.962290: step 6730, loss = 11.64 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:44:01.385562: step 6740, loss = 11.58 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 20:44:05.791729: step 6750, loss = 11.68 (73.5 examples/sec; 0.436 sec/batch)
2017-02-03 20:44:10.233005: step 6760, loss = 11.52 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:44:14.654991: step 6770, loss = 11.28 (70.7 examples/sec; 0.453 sec/batch)
2017-02-03 20:44:19.136454: step 6780, loss = 12.00 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:44:23.603584: step 6790, loss = 11.34 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:44:28.036818: step 6800, loss = 11.39 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:44:33.334528: step 6810, loss = 11.68 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:44:37.790233: step 6820, loss = 12.02 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:44:42.194800: step 6830, loss = 11.41 (71.5 examples/sec; 0.447 sec/batch)
2017-02-03 20:44:46.642343: step 6840, loss = 11.28 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:44:51.109142: step 6850, loss = 11.88 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:44:55.518468: step 6860, loss = 11.55 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:44:59.956249: step 6870, loss = 11.84 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:45:04.420395: step 6880, loss = 11.60 (73.1 examples/sec; 0.438 sec/batch)
2017-02-03 20:45:08.814749: step 6890, loss = 11.77 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:45:13.269570: step 6900, loss = 11.72 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:45:18.522439: step 6910, loss = 11.35 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:45:22.969970: step 6920, loss = 11.49 (71.8 examples/sec; 0.445 sec/batch)
2017-02-03 20:45:27.393580: step 6930, loss = 11.55 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:45:31.810448: step 6940, loss = 11.32 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:45:36.263528: step 6950, loss = 11.63 (70.2 examples/sec; 0.456 sec/batch)
2017-02-03 20:45:40.721996: step 6960, loss = 11.54 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:45:45.188710: step 6970, loss = 11.57 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:45:49.625973: step 6980, loss = 11.47 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:45:54.059649: step 6990, loss = 11.48 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:45:58.464912: step 7000, loss = 11.61 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:46:03.726120: step 7010, loss = 11.53 (74.4 examples/sec; 0.430 sec/batch)
2017-02-03 20:46:08.215912: step 7020, loss = 11.57 (72.3 examples/sec; 0.442 sec/batch)
2017-02-03 20:46:12.621064: step 7030, loss = 11.55 (72.3 examples/sec; 0.442 sec/batch)
2017-02-03 20:46:17.059130: step 7040, loss = 11.81 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:46:21.492916: step 7050, loss = 10.99 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 20:46:25.902200: step 7060, loss = 11.79 (74.1 examples/sec; 0.432 sec/batch)
2017-02-03 20:46:30.369334: step 7070, loss = 11.45 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:46:34.785017: step 7080, loss = 11.71 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:46:39.237056: step 7090, loss = 11.40 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:46:43.692890: step 7100, loss = 11.45 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:46:48.953592: step 7110, loss = 11.39 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:46:53.398684: step 7120, loss = 11.51 (70.9 examples/sec; 0.451 sec/batch)
2017-02-03 20:46:57.851619: step 7130, loss = 11.69 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:47:02.306464: step 7140, loss = 11.35 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:47:06.761654: step 7150, loss = 11.65 (75.4 examples/sec; 0.424 sec/batch)
2017-02-03 20:47:11.210404: step 7160, loss = 11.69 (74.2 examples/sec; 0.431 sec/batch)
2017-02-03 20:47:15.688738: step 7170, loss = 11.70 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:47:20.075282: step 7180, loss = 11.56 (72.8 examples/sec; 0.440 sec/batch)
2017-02-03 20:47:24.549694: step 7190, loss = 11.53 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:47:29.020429: step 7200, loss = 11.40 (72.6 examples/sec; 0.440 sec/batch)
2017-02-03 20:47:34.319707: step 7210, loss = 12.16 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:47:38.709436: step 7220, loss = 11.38 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:47:43.236166: step 7230, loss = 11.67 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:47:47.691068: step 7240, loss = 11.49 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:47:52.141890: step 7250, loss = 11.45 (70.4 examples/sec; 0.454 sec/batch)
2017-02-03 20:47:56.547945: step 7260, loss = 11.13 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:48:01.015691: step 7270, loss = 11.53 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:48:05.454331: step 7280, loss = 11.48 (70.7 examples/sec; 0.452 sec/batch)
2017-02-03 20:48:09.883356: step 7290, loss = 11.57 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:48:14.316016: step 7300, loss = 11.59 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:48:19.638886: step 7310, loss = 11.22 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:48:24.078477: step 7320, loss = 11.42 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:48:28.538031: step 7330, loss = 11.71 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:48:32.977448: step 7340, loss = 10.96 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:48:37.403644: step 7350, loss = 11.11 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:48:41.885186: step 7360, loss = 12.05 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:48:46.312943: step 7370, loss = 11.44 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:48:50.778971: step 7380, loss = 11.78 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:48:55.286633: step 7390, loss = 11.30 (71.7 examples/sec; 0.447 sec/batch)
2017-02-03 20:48:59.714509: step 7400, loss = 11.04 (74.5 examples/sec; 0.430 sec/batch)
2017-02-03 20:49:04.948556: step 7410, loss = 11.62 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:49:09.390028: step 7420, loss = 11.54 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:49:13.836995: step 7430, loss = 11.40 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:49:18.274235: step 7440, loss = 11.67 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:49:22.750254: step 7450, loss = 11.49 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:49:27.180416: step 7460, loss = 10.92 (71.3 examples/sec; 0.449 sec/batch)
2017-02-03 20:49:31.574894: step 7470, loss = 11.05 (72.4 examples/sec; 0.442 sec/batch)
2017-02-03 20:49:35.972671: step 7480, loss = 11.58 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:49:40.378889: step 7490, loss = 11.60 (75.6 examples/sec; 0.423 sec/batch)
2017-02-03 20:49:44.867483: step 7500, loss = 11.04 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:49:50.207009: step 7510, loss = 11.09 (71.6 examples/sec; 0.447 sec/batch)
2017-02-03 20:49:54.626841: step 7520, loss = 11.53 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:49:59.030217: step 7530, loss = 11.39 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:50:03.474150: step 7540, loss = 11.19 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:50:07.929254: step 7550, loss = 11.02 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:50:12.372263: step 7560, loss = 11.34 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:50:16.743838: step 7570, loss = 10.80 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:50:21.176199: step 7580, loss = 11.64 (73.3 examples/sec; 0.437 sec/batch)
2017-02-03 20:50:25.596953: step 7590, loss = 11.73 (74.1 examples/sec; 0.432 sec/batch)
2017-02-03 20:50:29.998793: step 7600, loss = 11.38 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:50:35.277300: step 7610, loss = 11.38 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:50:39.702765: step 7620, loss = 10.91 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:50:44.076728: step 7630, loss = 11.86 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:50:48.526025: step 7640, loss = 11.19 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:50:52.964816: step 7650, loss = 11.68 (71.1 examples/sec; 0.450 sec/batch)
2017-02-03 20:50:57.417647: step 7660, loss = 10.67 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:51:01.851594: step 7670, loss = 11.52 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:51:06.326610: step 7680, loss = 11.53 (72.3 examples/sec; 0.443 sec/batch)
2017-02-03 20:51:10.779030: step 7690, loss = 11.49 (71.7 examples/sec; 0.446 sec/batch)
2017-02-03 20:51:15.208258: step 7700, loss = 10.78 (72.8 examples/sec; 0.439 sec/batch)
2017-02-03 20:51:20.511187: step 7710, loss = 11.76 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:51:24.989936: step 7720, loss = 11.00 (69.8 examples/sec; 0.459 sec/batch)
2017-02-03 20:51:29.420919: step 7730, loss = 11.35 (72.0 examples/sec; 0.445 sec/batch)
2017-02-03 20:51:33.822735: step 7740, loss = 12.27 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:51:38.239140: step 7750, loss = 11.31 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:51:42.678726: step 7760, loss = 11.08 (73.2 examples/sec; 0.437 sec/batch)
2017-02-03 20:51:47.109845: step 7770, loss = 11.20 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:51:51.511823: step 7780, loss = 11.36 (73.0 examples/sec; 0.439 sec/batch)
2017-02-03 20:51:55.995894: step 7790, loss = 10.41 (73.6 examples/sec; 0.435 sec/batch)
2017-02-03 20:52:00.439300: step 7800, loss = 11.45 (73.7 examples/sec; 0.434 sec/batch)
2017-02-03 20:52:05.625797: step 7810, loss = 11.26 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:52:10.046650: step 7820, loss = 12.25 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 20:52:14.474141: step 7830, loss = 11.18 (72.6 examples/sec; 0.441 sec/batch)
2017-02-03 20:52:18.917576: step 7840, loss = 11.34 (72.9 examples/sec; 0.439 sec/batch)
2017-02-03 20:52:23.347162: step 7850, loss = 11.49 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:52:27.783790: step 7860, loss = 10.98 (71.8 examples/sec; 0.446 sec/batch)
2017-02-03 20:52:32.227679: step 7870, loss = 11.08 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:52:36.648647: step 7880, loss = 11.53 (72.7 examples/sec; 0.440 sec/batch)
2017-02-03 20:52:41.065382: step 7890, loss = 10.84 (70.5 examples/sec; 0.454 sec/batch)
2017-02-03 20:52:45.474188: step 7900, loss = 11.24 (73.0 examples/sec; 0.438 sec/batch)
2017-02-03 20:52:50.749227: step 7910, loss = 11.77 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:52:55.153097: step 7920, loss = 11.51 (75.1 examples/sec; 0.426 sec/batch)
2017-02-03 20:52:59.597480: step 7930, loss = 11.18 (74.5 examples/sec; 0.429 sec/batch)
2017-02-03 20:53:04.021881: step 7940, loss = 11.70 (73.3 examples/sec; 0.436 sec/batch)
2017-02-03 20:53:08.428598: step 7950, loss = 11.02 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:53:12.895628: step 7960, loss = 10.43 (71.5 examples/sec; 0.448 sec/batch)
2017-02-03 20:53:17.348233: step 7970, loss = 10.82 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:53:21.754526: step 7980, loss = 11.19 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:53:26.198814: step 7990, loss = 11.38 (72.1 examples/sec; 0.444 sec/batch)
2017-02-03 20:53:30.608755: step 8000, loss = 11.31 (73.4 examples/sec; 0.436 sec/batch)
2017-02-03 20:53:35.927909: step 8010, loss = 11.53 (70.6 examples/sec; 0.453 sec/batch)
2017-02-03 20:53:40.380173: step 8020, loss = 11.41 (71.0 examples/sec; 0.451 sec/batch)
2017-02-03 20:53:44.813330: step 8030, loss = 11.68 (72.5 examples/sec; 0.441 sec/batch)
2017-02-03 20:53:49.251203: step 8040, loss = 11.06 (72.2 examples/sec; 0.443 sec/batch)
2017-02-03 20:53:53.649075: step 8050, loss = 11.41 (74.8 examples/sec; 0.428 sec/batch)
2017-02-03 20:53:58.045798: step 8060, loss = 11.12 (74.6 examples/sec; 0.429 sec/batch)
2017-02-03 20:54:02.499430: step 8070, loss = 11.42 (69.8 examples/sec; 0.459 sec/batch)
2017-02-03 20:54:06.953994: step 8080, loss = 11.31 (69.8 examples/sec; 0.459 sec/batch)
2017-02-03 20:54:11.411500: step 8090, loss = 11.91 (71.9 examples/sec; 0.445 sec/batch)
2017-02-03 20:54:15.825701: step 8100, loss = 11.63 (71.2 examples/sec; 0.449 sec/batch)
2017-02-03 20:54:21.095500: step 8110, loss = 11.70 (71.4 examples/sec; 0.448 sec/batch)
2017-02-03 20:54:25.541786: step 8120, loss = 10.92 (70.8 examples/sec; 0.452 sec/batch)
2017-02-03 20:54:29.985562: step 8130, loss = 10.86 (70.3 examples/sec; 0.455 sec/batch)
2017-02-03 20:54:34.395666: step 8140, loss = 11.46 (72.8 examples/sec; 0.439 sec/batch)
2017-02-03 20:54:38.875694: step 8150, loss = 11.43 (72.0 examples/sec; 0.444 sec/batch)
2017-02-03 20:54:43.291338: step 8160, loss = 11.28 (73.9 examples/sec; 0.433 sec/batch)
2017-02-03 20:54:47.689983: step 8170, loss = 11.66 (73.5 examples/sec; 0.435 sec/batch)
2017-02-03 20:54:52.103258: step 8180, loss = 11.30 (73.8 examples/sec; 0.434 sec/batch)
2017-02-03 20:54:56.538641: step 8190, loss = 11.72 (73.0 examples/sec; 0.438 sec/batch)